---
layout: post
title:  深度强化学习（二）——概述, Deep Q-learning Network
category: DRL 
---

* toc
{:toc}

# 工具/框架（续）

https://mp.weixin.qq.com/s/ApP0zNuG5OP_-HzJC1v95Q

谷歌发布开源Dopamine 2.0，让强化学习变得更灵活

https://mp.weixin.qq.com/s/Hod37LQ-eEe0EVtLfyXLGQ

DeepMind重磅开源强化学习框架！覆盖28款游戏，24多个算法(OpenSpiel)

https://mp.weixin.qq.com/s/OIxgL4tLXNnapwHk4lDU4g

DeepMind悄咪咪开源三大新框架，深度强化学习落地希望再现（OpenSpiel、SpriteWorld、bsuite）

https://zhuanlan.zhihu.com/p/68462431

谷歌开源RL足球环境

https://mp.weixin.qq.com/s/BhTX4KQnLxUQLvPUfY3q6Q

物理实验成本为零！南大LAMDA开源虚拟RL训练环境

https://mp.weixin.qq.com/s/MYm17ungMfMhu_7Cm66gqg

Deepmind官方实现的DQN库

https://mp.weixin.qq.com/s/LymwoIpIu9xCBZlNM-_CIw

DeepMind首发并开源Alchemy，一种元强化学习(meta-RL)基准环境

https://mp.weixin.qq.com/s/La43IWrxuDLoppnfRAYtbA

ReinforceJS库（动态展示DP、TD、DQN算法运行过程）

https://mp.weixin.qq.com/s/HUf41-OBAwZm7zlISkIx0w

DRL在Unity自行车环境中配置与实践

https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650771231&idx=3&sn=09410e8fd9f34cacfac7df6abf4d9973

集合三大类无模型强化学习算法，BAIR开源RL代码库rlpyt

https://mp.weixin.qq.com/s/PYrE6vUCivusWuFvdiUbNQ

几行代码轻松实现，Tensorlayer 2.0推出深度强化学习基准库

https://mp.weixin.qq.com/s/AUKMBRFMifcgZmdXBjDl3Q

从“小”培养AI安全意识：OpenAI开源最新强化学习训练工具，安全约束自由定制，开箱即用

https://mp.weixin.qq.com/s/oPK9eNgIPMP6ICwBB-4kQA

Facebook推基于NetHack的深度强化学习利器，超轻量级架构性价比远超GPT-2和BERT

https://mp.weixin.qq.com/s/PfyGOK2Ov6Qi-L5aqMfNjg

DeepMind发布强化学习库RLax

https://github.com/thu-ml/tianshou

清华开源“天授”强化学习平台

https://mp.weixin.qq.com/s/-5KzyKeNFOYBtFQ8ZVZNag

用Python写出Gameboy模拟器，还能训练AI模型：丹麦小哥的大学项目火了

https://mp.weixin.qq.com/s/Ewiz8YB0Tel9127cHcdRqw

用TF-AGENTS进行强化学习

https://mp.weixin.qq.com/s/dkaW4WNLmfjjbRYR9Za3bQ

DeepMind最新力作：分布式强化学习框架Acme，智能体并行性加强

https://mp.weixin.qq.com/s/LEqm8Ix67Q-tM-ufwK6a6w

强化学习开源模拟环境大盘点

# 概述

![](/images/img2/RL_intro.png)

上图是深度强化学习的脉络图，参考文献中还有该领域的关键论文列表。

![](/images/img3/RL_methods.png)

![](/images/img3/RL_challenges.png)

原图地址：

http://louiskirsch.com/maps/reinforcement-learning

参考：

https://mp.weixin.qq.com/s/781fVvKr4yuq4q2GI1Y1gA

从Zero到Hero，OpenAI重磅发布深度强化学习资源

https://spinningup.openai.com/en/latest/spinningup/keypapers.html

深度强化学习关键论文列表

## DeepMind vs OpenAI

DeepMind和OpenAI算是DRL的两大重镇。

DeepMind主要是David Silver和他导师Richard Sutton一脉，最初来源于control theory，代表作是Deep Q-learning, DDPG。

OpenAI背后的派别是Berkeley帮，主要工作是围绕Pieter Abbeel以及他的两位superstar博士生Sergey Levine和John Schulman，最初来源于Robotics，代表作是TRPO以及后来的PPO。

某牛的点评：

>现在的研究发现policy gradient的方法效果比Q-learning这种单纯基于value的方法好，所以选择policy gradient，事实上是把两者结合起来的actor-critic效果最好！只是说actor-critic的关键在于policy gradient。   
>然后呢，说说目前DeepMind和OpenAI的研究情况，DeepMind在Q-Learning上造诣太深，OpenAI的Pieter Abbeel派很难从中弄成什么新的东西，因此从改进policy gradient的方法入手，得到TRPO+GAE等方法。（然后DeepMind基本不鸟，论文根本不比较他们的方法，而OpenAI这边到现在也不敢和A3C比较，估计必败）   
>现在OpenAI又因为有一些Deep Generative Model的大神，又把这方面也结合进来了。OpenAI这边的优势在于机器人控制上之前做的多，把传统控制和深度学习结合的方法也能搞出一些东西比如GPS，其他的团队比如Satinder Singh, Honglak Lee那就更只能从边边上去做了，比如做了Atari视频预测，给网络加上memory network。   
>最后还是从方法上说一下，Policy Gradient显然比基于Value的方法更直接，输入感知，输出控制。按道理来说是更符合人类行为的方法。特别在控制上。但是在一些离散的决策上，人类也是评估各方好坏value来做选择的，所以这一块Q-Learning应该会做的更好。事实上，DeepMind也将DQN的Gorila分布式系统用在了Google的推荐系统上。还有，就是接下来机器人控制将会是深度学习要占领的一块地盘，在连续控制上，基于value的方法本身就不好做，连续DQN比如那个NAF方法使用了很多小技巧。而基于Policy的方法则很直接。

参考：

https://www.zhihu.com/question/316626294

请问DeepMind和OpenAI身后的两大RL流派有什么具体的区别？

https://www.zhihu.com/question/49787932

RL两大类算法的本质区别？（Policy Gradient和Q-Learning)

## 参考

https://www.nervanasys.com/demystifying-deep-reinforcement-learning/

深度强化学习揭秘

https://zhuanlan.zhihu.com/p/24446336

深度强化学习Deep Reinforcement Learning学习整理

https://mp.weixin.qq.com/s/K82PlSZ5TDWHJzlEJrjGlg

深度学习与强化学习

http://mp.weixin.qq.com/s/lLPRwInF5qaw7ewYHOpPyw

深度强化学习资料

http://lamda.nju.edu.cn/yangjw/project/drlintro.html

深度强化学习初探

https://zhuanlan.zhihu.com/p/21498750

深度强化学习导引

https://mp.weixin.qq.com/s/RnUWHa6QzgJbE_XqLeAQmg

深度强化学习，决策与控制

https://mp.weixin.qq.com/s/SckTPgEw7KWmfKXWriNIRg

浅谈强化学习的方法及学习路线

https://mp.weixin.qq.com/s/-JHHOQPB6pKVuge64NkMuQ

DeepMind主攻的深度强化学习3大核心算法及7大挑战

https://mp.weixin.qq.com/s/2SOHQElaYbplse3QqG9tYw

强化学习介绍

https://mp.weixin.qq.com/s/R30quVGK0TgjerLpiIK9eg

从算法到训练，综述强化学习实现技巧与调试经验

https://www.zhihu.com/question/49230922

强化学习（reinforcement learning)有什么好的开源项目、网站、文章推荐一下？

https://zhuanlan.zhihu.com/p/75913329

深度强化学习简介

# Deep Q-learning Network

## 基本思想

Deep Q-learning Network是DL在RL领域的开山之作。它的思想主要来自于Deepmind的两篇论文：

《Playing Atari with Deep Reinforcement Learning》

《Human-level control through deep reinforcement learning》

Deepmind是当今DL领域最前沿的科研机构，尤其在RL领域更是领先同行一大截，是当之无愧的RL王者。

![](/images/img2/DQN.png)

上图是DQN的网络结构图。由于这里的任务是训练Atari游戏的AI，因此网络的输入实际上就是游戏的画面。而理解游戏画面，就需要一定的CNN结构。所以DQN的结构实际上和一般的CNN是一致的，其关键要害在于loss函数的设定。

由《强化学习（六）》中的“[价值函数的近似表示](/rl/2019/06/22/RL_6.html#Value)”可知：$$Q(s,a)\approx f(s,a,w)$$

老套路，我们可以用一个神经网络来拟合f函数，这也就是所谓的Q网络（Q-Network）。

对于Atari游戏而言，这是一个高维状态输入（原始图像），低维动作输出（只有几个离散的动作，比如上下左右）。那么怎么来表示这个函数f呢？

Deepmind采取的方法是：只把状态s作为输入，但是输出的时候，会输出每一个动作的Q值，也就是输出一个向量$$[Q(s,a_1),Q(s,a_2),\dots,Q(s,a_n)]$$。这样我们只要输入状态s，就可以得到所有的动作Q值。

在[《强化学习（二）》](/rl/2018/11/23/RL_2.html#transition_rule)中，我们指出Q-Learning算法的**transition rule**为：

$$Q(s,a)=R(s,a)+\gamma \max(Q(\tilde s,\tilde a))$$

这时就存在两个Q值了：

- 根据当前的<s,a>，由Q网络计算得到的Q值，被称为**current Q-value**。

- 使用transition rule，也由Q网络计算得到的Q值，被称为**target Q-value**。

显然从Q函数的定义来看，这两者是等价的。因此，我们的目标就是使这两者尽可能的一致，即：

$$L=E[({\color{blue}{r+\gamma \max_{a'}Q(s',a',w)}}-Q(s,a,w))^2]$$

上式中蓝色的部分即为target Q-value。

DQN虽然是model-free的，但是也有难点，那就是Reward的定义。好在在Atari游戏的例子里，这个问题比较简单，直接采用游戏得分即可。但得分奖励过于肤浅，在动作类游戏中或许没啥问题，但对于《Montezuma's Revenge》这样的解谜游戏，就不太好使了。

此外，Deepmind在Atari游戏中，采用了连续4帧图像作为DQN的输入。这样的行为也被称作状态的表示。状态的表示会严重影响学习的效果，然而目前来说，它还是一门艺术，而非科学。例如，AlphaGo采用了过去八手棋局作为状态的表示。

## Nature DQN

DQN最早发表于NIPS 2013，该版本的DQN，也被称为NIPS DQN。NIPS DQN除了提出DQN的基本概念之外，还使用了《强化学习（六）》中提到的Experience Replay技术。

2015年初，Deepmind在Nature上提出了改进版本，是为Nature DQN。它改进了Loss函数：

$$L=({\color{blue}{r+\gamma \max_{a'}Q(s',a',w^-)}}-Q(s,a,w))^2$$

上式中，计算目标Q值的网络使用的参数是$$w^-$$，而不是w。也就是说，原来NIPS DQN的target Q网络是动态变化的，跟着Q网络的更新而变化，这样不利于计算目标Q值，导致目标Q值和当前的Q值相关性较大。

因此，Deepmind提出可以单独使用一个目标Q网络。那么目标Q网络的参数如何来呢？还是从Q网络中来，只不过是延迟更新。也就是每次等训练了一段时间再将当前Q网络的参数值复制给目标Q网络。

个人理解这里的机制类似于进化算法。**变异既可以是进化，也可以是退化，只有进化的Q网络才值得保留下来。**

这在下文介绍的Leela Zero项目的训练过程中体现的很明显：一开始我们只训练一个简单的AI，然后逐渐加深加大网络，淘汰废物AI，得到更优秀的AI。

为了使进化生效，在一段时间内保持目标Q网络不变就是很显然的了——你过不了我这关，就没资格继续下去了。
