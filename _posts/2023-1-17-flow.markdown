---
layout: post
title:  Flow-based Model, Autoregressive Model
category: Generative Model
---

* toc
{:toc}

# Flow-based Model

![](/images/img4/flow.png)

Flow-based Model是GAN和VAE之外的另一大类生成模型方法。

从表面来看，Flow-based Model和VAE非常类似，无非把Encoder和Decoder换成了Flow和它的Inverse，但是实际上两者不仅数学原理不同，具体的训练方法也有极大差异。上图说是照骗也不为过。

以下内容主要参考了李宏毅老师的课件：

http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/FLOW%20(v7).pdf

还有以下笔记：

http://www.seeprettyface.com/pdf/Note_Flow.pdf

![](/images/img4/flow_2.png)

Flow-based Model的训练过程是用图片x通过网络$$f(x)$$生成随机数z。由于这个经过巧妙构造的$$f(x)$$具有可直接得到的可逆函数$$f^{-1}(z)$$，所以在推理阶段，无需任何额外处理，即可直接由$$f^{-1}(z)$$，从随机数z得到图片x。

随机数z可以是任意分布，但通常为了理论推导的简单，而使用正态分布，即所谓的Normalizing Flow。

Flow-based Model的理论不算复杂，难点主要在于如何构造可逆的函数G。目前已经有了一些成熟的构造方法，但相对于网络结构的千变万化，构造方法的种类就少的太多了。主流的大概也就是NICE、RealNVP和GLOW。

最初的NICE实现了从A分布到高斯分布的可逆求解；后来RealNVP实现了从A分布到条件非高斯分布的可逆求解；而最新的GLOW，实现了从A分布到B分布的可逆求解，其中B分布可以是与A分布同样复杂的分布，这意味着给定两堆图片，GLOW能够实现这两堆图片间的任意转换。

我们以NICE为例，介绍一下Flow-based Model的基本套路。

首先，$$G^{-1}$$必须是存在的且能被算出，这意味着G的输入和输出的维度必须是一致的并且G的行列式不能为0。因此，z和x的形状必须完全一致。

作为一个生成模型自然希望自己产生的数据的概率越高越好。因此这里的优化目标就是：

$$G^*=\arg \max_{G} \sum_{i=1}^m\log p_{G}(x^i)$$

这里不加证明的给出结论：

$$\log p_{G}(x^i)=\log \pi(G^{-1}(x^i))+\log |det(J_{G^{-1}})|$$

这里的第一项实际上就是$$\log \pi(z^i)$$。显然，当z为0时，正态分布的概率最大。

然而这会导致$$det(J_{G^{-1}})=0$$，也就是第二项为$$-\infty$$。

所以z必须在两项之间平衡，才能得到最大值。这个平衡点就是我们的优化目标。

NICE采用了一种称为耦合层（Coupling Layer）的设计，如下图所示：

![](/images/img5/flow.png)

z和x都会被拆分成两个部分，分别是前1~d维和后d+1~D维。

z的1~d维直接复制（copy）给x的1~d维；z的d+1~D维分别通过F和H两个神经网络，通过仿射计算（affine）传递给x。

由于F和H的结果仅仅是系数，求偏导数的时候，会被当成常数，所以对于从$$G^{-1}$$构建G没有什么影响。

![](/images/img5/flow_2.png)

如果一层变换的表现力不足的话，我们也可以多做几层变换。需要注意的是，z的1~d维直接复制（copy）给x的1~d维的方式中的copy操作，对于生成模型的表现力会有影响。（总不可能生成图片的一部分还是随机噪声吧。）所以多层变换的话，需要使用交错的方式，组合copy操作和affine操作。

![](/images/img4/MAF-vs-IAF.png)

Masked Autoregressive Flow

Inverse Autoregressive Flow

参考：

https://zhuanlan.zhihu.com/p/44304684

Normalizing Flow小结

https://www.jianshu.com/p/66393cebe8ba

标准化流(Normalizing Flow)教程（一）

https://www.jianshu.com/p/db72c38233f3

标准化流(Normalizing Flow)（二）：现代标准化流技术

https://mp.weixin.qq.com/s/oUQuHvy0lYco4HsocqvH3Q

Normalizing Flows入门(上)

https://mp.weixin.qq.com/s/XtlK3m-EHgFRKrtcwJHZCw

Normalizing Flows入门(中)

https://mp.weixin.qq.com/s/TRgTFBz_NmBJygQjOYwdqw

GAN/VAE地位难保？Flow在零样本识别任务上大显身手

https://mp.weixin.qq.com/s/xMO9jhzQH6P5NEA_D-uyIA

这个模型的脑补能力比GAN更强，ETH提出新型超分辨率模型SRFlow

https://zhuanlan.zhihu.com/p/351479696

基于流的生成模型-Flow based generative models

https://mp.weixin.qq.com/s/KrvW16GAxSGAPOCYYKbGUg

生成模型：标准化流（Normalized Flow）

https://lilianweng.github.io/posts/2018-10-13-flow-models/

Flow-based Deep Generative Models

# Autoregressive Model

AR vs AE的相关内容在《Attention（七）》中已有描述，这里不再赘述。

Autoregressive Model实际上是最古老的生成模型，比GAN/VAE/Flow/Diffusion都要古老。

![](/images/img4/gen.jpg)

这里以PixelCNN为例介绍一下AR Model的原理。

![](/images/img5/PixelCNN.png)

PixelCNN简单来说就是按照从左到右，从上到下的顺序依次生成图片。表面上和Diffusion Model差不多，也是若干step的生成过程。

但是有个问题就是中间步骤不能省略。比如一张28x28的图片，如果一次出1个点的话，一共就要28x28个step。少了其中的某一步，图片就不完整了。而且显然一次出1个点的模型一定和一次出2个点的模型是不一样的。

但Diffusion Model就无所谓了，少了某一步，无非少了一种Noise Predictor而已，不会对结果有什么根本性的影响。

PixelCNN训练和过程和Autoencoder差不多，也是最终生成的图片和训练图片做比较。但是为了表示没有偷看后面的数据，训练时需要用MASK遮住还未生成的那部分。

此外，PixelCNN还要给图片打分，用以表明生成的图片是那一类的。比如MNIST数据集的手写数字的10分类。

这样最终训练好之后，只要给出分类信息和seed，就可以生成图片了。

参考：

https://zhuanlan.zhihu.com/p/591881660

通俗形象地分析比较生成模型（GAN/VAE/Flow/Diffusion/AR）

https://mp.weixin.qq.com/s/YZcw5pnHzuACSvEmGZHnEQ

自回归模型:PixelCNN

https://www.microsoft.com/en-us/research/uploads/prod/2022/12/Generative-Models-for-TTS.pdf

Generative Models for TTS

# 苏俄笑话+

一个苏联人在大街上骂道：“勃列日涅夫是蠢猪！”

法院判这个人有期徒刑十年零一个月：其中一个月刑期是因为辱骂领袖，十年刑期是因为泄露国家机密。

---

美国人：我敢在白宫外骂总统，你能吗？

苏联人：我也敢在白宫外骂美国总统。

---

一个文化宣传部的局长要退休了，退休之前想提拔两个人，以后有什么事好有关系，但思考了几天不知道提拔谁。

这时妻子看到老公这几天愁眉苦脸的，问他怎么了？局长就把自己的问题，给妻子讲了一遍。

妻子出主意就说：你就说你被双规，我通知他们来商量对策，看看谁在困难的时候选择过来给你分忧，他就是自己人。

妻子分别发短信给四名心腹，两天过后，别说登门的，连一个电话都没有。

局长正在痛心，就在这时终于有人来敲门了，开门后没想到竟然是两位纪检人员，他们说:“你被双规了”。

局长两眼一瞪:“开什么玩笑，我老婆那短信是假的。”

纪检人员说:“就因为那条假短信，你的四名部下把你的材料交到我们这里了。”

---

普京这一仗打得让世界吃惊：打废了乌克兰，打傻了日本，打残了北约，打蒙了美国，打醒了欧盟，把台独分子打怂了，打出了病毒的来源，把中俄关系给打牢了，把联合国打得不敢说话了。他打黄了股市，打长了物价，打出了真理，打出了威风，打出了水平，打出了一个真正的硬汉。

---

一天晚上，埃里希·昂纳克（东德领导人）和情人在豪华寝室里说着枕边悄悄话。

他心情舒畅，慷慨地许诺她一个礼物，要什么都行。

她想了一会，回答说：“哦，埃里希，如果让你一定要为我做一件事的话，我希望是：开放柏林墙，一天就够了。”

昂纳克说：“当然没问题，亲爱的。”

不过他对她这样的请求有些不解，就问，“为什么要我做这样一件事呢？”

情人回答说：“我只想和你单独享受二人世界。”

---

美术馆里有一幅描写亚当和夏娃的画。

一个英国人看了，说：“他们一定是英国人，男士有好吃的东西就和女士分享。”

一个法国人看了，说：“他们一定是法国人，情侣裸体散步，追求浪漫。”

一个苏联人看了，说：“他们一定是苏联人，他们没有衣服穿，吃得东西少得可怜，却还以为自己生活在天堂里！”

---

普京在地狱碰见傻大木。

傻大木：老弟，你咋也下来了，也是荡秋千来的啊。

普京：是啊，那玩意真疼，扑腾了老半天。

傻：你也跟美国人干仗了？

普：我跟乌克兰打仗，对抗整个北约！

傻：乌克兰加入北约了？

普：……没有……

傻：北约派兵去乌克兰了？

普：……没有……

傻：那你是怎么对抗北约的？

普：就是北约给了乌克兰点钱，和一些军火。

傻：确实，欧美军火确实厉害，当年F15、F16、F18、F117打的我找不着北。

普：这些都没给……

傻：那A10，阿帕奇，狂风呢？

普：也没有。

傻：那都给了点啥？

普：海马斯，M1A1，豹2，挑战者2这些……

傻：这些也都挺厉害，给了好几千台吧？

普：每种三四十台，挑战者十几台……

傻：那你打了多久?

普：两三年。

傻：打死多少北约士兵?

普：一个也没有……

傻：老弟，你这不行呀，我这当年扛的可是英法几十万联军，我也不敢说我扛北约，你连个乌克兰都打不过，这么吹牛逼，也不怕风大闪了舌头。

普：本来我也是不相信的，只是有人天天说俄罗斯扛住了北约，说的我都信了。

傻：你说的那帮人我知道，当年我本来慌得一批准备跑路，结果听有人说我根本不慌，踏马我只能装作不慌的样子，到最后跑都没处跑了。

普：不怕神一样的对手，就怕猪一样的队友，拿着三鹿奶粉，奶谁死谁！
