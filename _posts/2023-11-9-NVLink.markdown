---
layout: post
title: NVLink, AMD, GPU通信技术
category: Chip 
---

* toc
{:toc}

# NVLink

## 算力 vs 带宽

![](/images/img5/bandwidth_compute_bound.png)

在过去十年的竞争中，核心竞争指标是算力，现在变成了内存带宽和容量。

https://blog.csdn.net/tugouxp/article/details/126479276

利用roofline模型分析异构系统算力VS带宽

https://mp.weixin.qq.com/s/y9dVg9YtfWxu6NcW-fxi6Q

内存带宽与计算能力，谁才是决定深度学习执行性能的关键？

## NVLink

NVLink技术提供比PCIe 3更高的带宽与更多的链路，并可提升多GPU和多GPU/CPU系统配置的可扩展性。

SXM是Nvidia的双插槽卡设计，与PCIe卡不同，它不需要连接电源。

---

![](/images/img4/PCIE.jpg)

官网：

https://www.nvidia.cn/data-center/nvlink/

![](/images/img3/nvlink.png)

Tesla V100中以NVLink连接的GPU至GPU和GPU至CPU通信。

![](/images/img3/nvlink_2.png)

在DGX-1V服务器中，混合立体网络拓扑使用NVLink连接8个Tesla V100加速器。每个GPU有6条nvlink通道，总带宽高达300GB/s。

从上图可以看到，即使每个GPU拥有6条nvlink通道，仍然无法做到“全连接”（即任意两个GPU之间存在双向通道）。这就引出了下一个更加疯狂的技术：nvswitch。

![](/images/img3/nvswitch.png)

NV Switch是首款节点交换架构，可支持单个服务器节点中16个全互联的GPU，并可使全部8个GPU对分别以300GB/s的惊人速度进行同时通信。这16个全互联的GPU还可作为单个大型加速器，拥有0.5 TB统一显存空间和2 PetaFLOPS计算性能。

https://zhuanlan.zhihu.com/p/35470532

全球最大GPU背后的秘密：NVSwitch如何实现NVIDIA DGX-2的超强功力？

---

NVLink和NV Switch的关系：

NVLink是一种协议。

NV Switch是NVLink协议的芯片化实现。

GPU core和NV Switch如果都实现了NVLink协议的话，则它们之间可以通过NVLink协议进行通信。

未来解决带宽问题的两大法宝，一个靠内存厂给提供的牙膏继续叠单GPU芯片的带宽，另一个就是目前这些形态更高密度的通过nvlink组合起来，从而创造一个大的“一个GPU”。

就这个搞一个大的“一个GPU”而言，又有很多种可能的扩展形态

- 双GPU卡
- 主板堆8卡
- 垂直堆8个刀片
- 网络组更多机柜

其实也就对应了nvlink的多种形态

- nvlink c2c
- nvlink switch
- nvlink bridge
- nvlink network

---

![](/images/img5/Nvidia_Grace_Hopper.jpg)

https://zhuanlan.zhihu.com/p/600638633

Nvidia Grace Hopper超级芯片架构解读（一）

https://zhuanlan.zhihu.com/p/601072219

Nvidia Grace Hopper超级芯片架构解读（二）

---

单单从信号线路数量来说，x16的PCIe和x2的NVLink是相同的，都是32对差分线。x2的NVLink 3.0双向带宽是100 GB/s，比PCIe 5.0 x16的126 GB/s要低。

这么高的带宽，数据传输功耗是一个不可忽略的重要因素，NV能做到x18的NVLink，不代表其它厂家可以轻松实现x144的PCIe。

大概在2017年的hotchips上，Intel给出的一份PPT上，PCB上每传输1bit数据的参考功耗是20 pJ（1e-12焦耳），按这个功耗计算，NVLink 3.0 x18的7200 Gbit/s意味着单单是信号传输就要消耗144W功耗，这显然是难以接受的。2019年NV发表过一份1.17 pJ/bit的论文，虽然因为应用环境不同，不能直接对比说NV的技术使得传输功耗不到6%，但还是可以从侧面猜测一下NV的技术水平。

https://www.zhihu.com/question/546809864

为什么NVlink能够实现比PCIe更高的传输带宽？

---

https://zhuanlan.zhihu.com/p/623060064

什么是NVLink？

https://zhuanlan.zhihu.com/p/672749098

Nvidia GPU互联技术全景图

## 存储体系

![](/images/img5/GPU_2.png)

Registers -> Caches -> Shared Memory -> Gloabl Memory（Local Memory）

https://blog.csdn.net/qq_41554005/article/details/119765334

GPU存储资源：寄存器，本地内存，共享内存，缓存，显存等存储器细节

---

通信的问题，其本质是数据的共享。因此，质不够量来凑，也是一个不错的方法。

由于没有NVLink这样的杀手锏，AMD的AI方案更倾向于增加单卡内存，来减少卡/机的数量。

比如AMD发布的Instinct MI300X GPU，单卡内存192GB，8卡服务器就是1.5TB HBM3内存。

当然，单纯堆料这种没有技术含量的活，对于老黄没有任何难度。老黄反手推出H200，单卡141GB HBM3e。

## NCCL

NCCL是Nvidia Collective multi-GPU Communication Library的简称，它是一个实现多GPU的collective communication通信库，可以在PCIe、Nvlink、InfiniBand上实现较高的通信速度。

官方文档：

https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html

nccl v1：支持单机多卡通信，不支持多机通信。

nccl v2：支持多机通信。

参考：

https://www.zhihu.com/question/63219175

如何理解Nvidia英伟达的Multi-GPU多卡通信框架NCCL？

https://zhuanlan.zhihu.com/p/364816069

NCCL--GPU的collective communication通信技术

https://blog.csdn.net/TH_NUM/article/details/81479317

nvidia-nccl学习笔记

https://developer.nvidia.com/blog/fast-multi-gpu-collectives-nccl/

Fast Multi-GPU collectives with NCCL

## 参考

https://www.infoq.cn/article/3D4MsRVS8ZOtGCj7*krT

GPU通信技术初探

https://zhuanlan.zhihu.com/p/67785062

不止显卡！这些硬件因素也影响着你的深度学习模型性能

# AMD

## Matrix Core

对标英伟达Tensor Core，AMD推出Matrix Core。

## 高速互联

对标NVLink，AMD推出了：

GMI：Global Memory Interconnect

AMD Infinity Fabric Link

其实Intel也有一个叫做Xelink的东西。

IBM BlueLink

---

HCCS，即Huawei Cache Coherence System，华为自研，叫“华为Cache一致性总线”。

英伟达的NVLink可以实现8颗芯片互联，而HCCS目前仅支持4路互联。

## HIP

![](/images/img4/latestGPU.png)

HIP：Heterogeneous Interface for Portability。

HIP是AMD提出的C++接口，号称能兼容CUDA和自家的ROCm。

https://zhuanlan.zhihu.com/p/545296023

写给CUDA开发者AMD ROCm & Intel oneAPI开发贴士

https://streamhpc.com/blog/2016-04-05/comparing-syntax-cuda-opencl-hip/

Comparing Syntax for CUDA, OpenCL and HiP

## Composable Kernel

Composable Kernel（CK）库旨在提供一套在AMD GPU上算子融合的后端方案。

https://www.sohu.com/a/603796560_129720

AMD Composable Kernel: 定制化算子融合，大幅提升AI端到端性能

## AITemplate

AITemplate首先在Python层寻找最优的kernel配置，生成Jinja2 template，再生成C++ template：

- NVIDIA GPU：基于CUTLASS的GPU Tensor Core C++ template；

- AMD GPU：基于CK（Composable Kernel）的Matrix Core C++ Template。

官网：

https://github.com/facebookincubator/AITemplate

参考：

https://www.zhihu.com/question/557608132

如何看待Meta发布的全新推理引擎AITemplate？

## AI服务器

AMD Instinct系列，大致对标NVIDIA DGX。

官网：

https://www.amd.com/zh-hans/graphics/instinct-server-accelerators

![](/images/img5/AMD.jpg)

---

https://zhuanlan.zhihu.com/p/434686566

AMD CDNA2架构（MI200）

https://www.zhihu.com/question/606505567

如何看待AMD发布Instinct MI300X GPU芯片？是否在大模型时代威胁Nvidia地位?

## 其他对标术语

| AMD | NVIDIA |
|:--:|:--:|
| GCN wavefront(64 threads wide) | CUDA warp(32 threads wide) |

## 参考

![](/images/img5/AMD.png)

罗家是台南水仙宫一带的世家。

---

以前的算法比较简单，数据吞吐量小，AMD的短流水线渲染单元数量多所以效率高。到了Ethash这类重IO算法主流的年代，其实A卡效率还略有优势，但是没以前那么夸张，所以N卡也被拉出来挖。

---

A卡的新驱动对于老游戏的支持有些差，解决办法：删除游戏目录下的dbghelp.dll文件。

---

https://www.zhihu.com/question/593343983

截至2023年4月，用AMD显卡做机器学习怎么样？

https://zhuanlan.zhihu.com/p/651797296

通过“最差实践”实验探索AMD GPU调度细节

# GPU通信技术

## 共享数据

共享数据很简单，OpenGL时代的persistent map，让驱动去做同步、做数据搬运，从而给开发者营造“数据随时可得”的假象（当然也可能的确就是随时可得），这就已经达到共享数据的要求了。

共享地址的难度则增加了一些，其目标是为了让你在CPU上跑的链表能直接在GPU等地方使用。x64时代到来使得内存地址扩充到48位，而且CPU上已经全面虚拟地址，GPU也要跟着加上地址转换的能力。

共享物理内存则明确表明，实现了上两者的独显（OpenCL的SVM就是前两个要求）也要被排除。那就基本是核显Only（主机的架构也可以看作是核显）。

https://www.zhihu.com/question/430936274

苹果的统一内存和集成显卡与CPU共用内存有什么区别？

## RDMA

RDMA网卡（Remote Direct Memory Access，这是一种硬件的网络技术，它使得计算机访问远程的内存时无需远程机器上CPU的干预）已经可以提供50~100Gbps的网络带宽和微秒级的传输延迟。

目前许多以深度学习为目标应用的GPU机群都部署了这样的网络。

UCX是一个建立在RDMA等技术之上的用于数据处理和高性能计算的通信框架，RDMA是其底层核心之一。

OpenUCX是UCX的一个开源实现。

![](/images/img5/RDMA.jpg)

RDMA包含Infiniband（IB），RDMA over Converged Ethernet（RoCE）和internet Wide Area RDMA Protocol（iWARP）。三种协议都符合RDMA标准，使用相同的上层接口，在不同层次上有一些差别。

InfiniBand（直译为“无限带宽”技术，缩写为IB）是一个用于高性能计算的计算机网络通信标准，它具有极高的吞吐量和极低的延迟，用于计算机与计算机之间的数据互连。相当于是面向HPC的以太网的替代品。

Mellanox Technologies是一家InfiniBand制造商，于2020年被Nvidia收购。

官网：

https://www.openucx.org/

参考：

https://mp.weixin.qq.com/s/_xcE8RUs0m4gwk3kxpe9jA

基于HTM/RDMA的可扩展内存事务处理系统

https://www.zhihu.com/column/c_1231181516811390976

一个RDMA方面的专栏

https://www.zhihu.com/question/422501188

什么是InfiniBand，它和以太网的区别在于什么？

https://www.cnblogs.com/allcloud/p/8945544.html

InfiniBand 与Intel Omni-Path Architecture

https://zhuanlan.zhihu.com/p/580035183

RoCE vs Infiniband vs TCP/IP
