---
layout: post
title: NVLink, GPU通信技术
category: Chip 
---

* toc
{:toc}

# NVLink

## 算力 vs 带宽

![](/images/img5/bandwidth_compute_bound.png)

在过去十年的竞争中，核心竞争指标是算力，现在变成了内存带宽和容量。

https://blog.csdn.net/tugouxp/article/details/126479276

利用roofline模型分析异构系统算力VS带宽

https://mp.weixin.qq.com/s/y9dVg9YtfWxu6NcW-fxi6Q

内存带宽与计算能力，谁才是决定深度学习执行性能的关键？

## NVLink

NVLink技术提供比PCIe 3更高的带宽与更多的链路，并可提升多GPU和多GPU/CPU系统配置的可扩展性。

SXM是Nvidia的双插槽卡设计，与PCIe卡不同，它不需要连接电源。

---

![](/images/img4/PCIE.jpg)

官网：

https://www.nvidia.cn/data-center/nvlink/

![](/images/img3/nvlink.png)

Tesla V100中以NVLink连接的GPU至GPU和GPU至CPU通信。

![](/images/img3/nvlink_2.png)

在DGX-1V服务器中，混合立体网络拓扑使用NVLink连接8个Tesla V100加速器。每个GPU有6条nvlink通道，总带宽高达300GB/s。

从上图可以看到，即使每个GPU拥有6条nvlink通道，仍然无法做到“全连接”（即任意两个GPU之间存在双向通道）。这就引出了下一个更加疯狂的技术：nvswitch。

![](/images/img3/nvswitch.png)

NV Switch是首款节点交换架构，可支持单个服务器节点中16个全互联的GPU，并可使全部8个GPU对分别以300GB/s的惊人速度进行同时通信。这16个全互联的GPU还可作为单个大型加速器，拥有0.5 TB统一显存空间和2 PetaFLOPS计算性能。

https://zhuanlan.zhihu.com/p/35470532

全球最大GPU背后的秘密：NVSwitch如何实现NVIDIA DGX-2的超强功力？

---

NVLink和NV Switch的关系：

NVLink是一种协议。

NV Switch是NVLink协议的芯片化实现。

GPU core和NV Switch如果都实现了NVLink协议的话，则它们之间可以通过NVLink协议进行通信。

未来解决带宽问题的两大法宝，一个靠内存厂给提供的牙膏继续叠单GPU芯片的带宽，另一个就是目前这些形态更高密度的通过nvlink组合起来，从而创造一个大的“一个GPU”。

就这个搞一个大的“一个GPU”而言，又有很多种可能的扩展形态

- 双GPU卡
- 主板堆8卡
- 垂直堆8个刀片
- 网络组更多机柜

其实也就对应了nvlink的多种形态

- nvlink c2c
- nvlink switch
- nvlink bridge
- nvlink network

---

![](/images/img5/Nvidia_Grace_Hopper.jpg)

https://zhuanlan.zhihu.com/p/600638633

Nvidia Grace Hopper超级芯片架构解读（一）

https://zhuanlan.zhihu.com/p/601072219

Nvidia Grace Hopper超级芯片架构解读（二）

---

单单从信号线路数量来说，x16的PCIe和x2的NVLink是相同的，都是32对差分线。x2的NVLink 3.0双向带宽是100 GB/s，比PCIe 5.0 x16的126 GB/s要低。

这么高的带宽，数据传输功耗是一个不可忽略的重要因素，NV能做到x18的NVLink，不代表其它厂家可以轻松实现x144的PCIe。

大概在2017年的hotchips上，Intel给出的一份PPT上，PCB上每传输1bit数据的参考功耗是20 pJ（1e-12焦耳），按这个功耗计算，NVLink 3.0 x18的7200 Gbit/s意味着单单是信号传输就要消耗144W功耗，这显然是难以接受的。2019年NV发表过一份1.17 pJ/bit的论文，虽然因为应用环境不同，不能直接对比说NV的技术使得传输功耗不到6%，但还是可以从侧面猜测一下NV的技术水平。

https://www.zhihu.com/question/546809864

为什么NVlink能够实现比PCIe更高的传输带宽？

---

https://zhuanlan.zhihu.com/p/623060064

什么是NVLink？

https://zhuanlan.zhihu.com/p/672749098

Nvidia GPU互联技术全景图

## 存储体系

![](/images/img5/GPU_2.png)

Registers -> Caches -> Shared Memory -> Gloabl Memory（Local Memory）

https://blog.csdn.net/qq_41554005/article/details/119765334

GPU存储资源：寄存器，本地内存，共享内存，缓存，显存等存储器细节

---

通信的问题，其本质是数据的共享。因此，质不够量来凑，也是一个不错的方法。

由于没有NVLink这样的杀手锏，AMD的AI方案更倾向于增加单卡内存，来减少卡/机的数量。

比如AMD发布的Instinct MI300X GPU，单卡内存192GB，8卡服务器就是1.5TB HBM3内存。

当然，单纯堆料这种没有技术含量的活，对于老黄没有任何难度。老黄反手推出H200，单卡141GB HBM3e。

| 板卡 | 内存 |
|:--:|:--:|
| H100 | 80GB HBM2 |
| 升腾910B | 94GB HBM2 |
| MI300X | 192GB HBM3 |
| H200 | 144GB（24GBx6） HBM3e |
| GB200 | 192GB（24GBx8） HBM3e |

## NCCL

NCCL是Nvidia Collective multi-GPU Communication Library的简称，它是一个实现多GPU的collective communication通信库，可以在PCIe、Nvlink、InfiniBand上实现较高的通信速度。

官方文档：

https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html

nccl v1：支持单机多卡通信，不支持多机通信。

nccl v2：支持多机通信。

参考：

https://www.zhihu.com/question/63219175

如何理解Nvidia英伟达的Multi-GPU多卡通信框架NCCL？

https://zhuanlan.zhihu.com/p/364816069

NCCL--GPU的collective communication通信技术

https://blog.csdn.net/TH_NUM/article/details/81479317

nvidia-nccl学习笔记

https://developer.nvidia.com/blog/fast-multi-gpu-collectives-nccl/

Fast Multi-GPU collectives with NCCL

## 参考

https://www.infoq.cn/article/3D4MsRVS8ZOtGCj7*krT

GPU通信技术初探

https://zhuanlan.zhihu.com/p/67785062

不止显卡！这些硬件因素也影响着你的深度学习模型性能

https://zhuanlan.zhihu.com/p/680702927

NVLink发展概述

# GPU通信技术

## 共享数据

共享数据很简单，OpenGL时代的persistent map，让驱动去做同步、做数据搬运，从而给开发者营造“数据随时可得”的假象（当然也可能的确就是随时可得），这就已经达到共享数据的要求了。

共享地址的难度则增加了一些，其目标是为了让你在CPU上跑的链表能直接在GPU等地方使用。x64时代到来使得内存地址扩充到48位，而且CPU上已经全面虚拟地址，GPU也要跟着加上地址转换的能力。

共享物理内存则明确表明，实现了上两者的独显（OpenCL的SVM就是前两个要求）也要被排除。那就基本是核显Only（主机的架构也可以看作是核显）。

https://www.zhihu.com/question/430936274

苹果的统一内存和集成显卡与CPU共用内存有什么区别？

https://www.zhihu.com/question/22233341

CPU与GPU之间是如何通信的？

## RDMA

RDMA网卡（Remote Direct Memory Access，这是一种硬件的网络技术，它使得计算机访问远程的内存时无需远程机器上CPU的干预）已经可以提供50~100Gbps的网络带宽和微秒级的传输延迟。

目前许多以深度学习为目标应用的GPU机群都部署了这样的网络。

UCX是一个建立在RDMA等技术之上的用于数据处理和高性能计算的通信框架，RDMA是其底层核心之一。

OpenUCX是UCX的一个开源实现。

![](/images/img5/RDMA.jpg)

RDMA包含Infiniband（IB），RDMA over Converged Ethernet（RoCE）和internet Wide Area RDMA Protocol（iWARP）。三种协议都符合RDMA标准，使用相同的上层接口，在不同层次上有一些差别。

InfiniBand（直译为“无限带宽”技术，缩写为IB）是一个用于高性能计算的计算机网络通信标准，它具有极高的吞吐量和极低的延迟，用于计算机与计算机之间的数据互连。相当于是面向HPC的以太网的替代品。

Mellanox Technologies是一家InfiniBand制造商，于2020年被Nvidia收购。

官网：

https://www.openucx.org/

参考：

https://mp.weixin.qq.com/s/_xcE8RUs0m4gwk3kxpe9jA

基于HTM/RDMA的可扩展内存事务处理系统

https://www.zhihu.com/column/c_1231181516811390976

一个RDMA方面的专栏

https://www.zhihu.com/question/422501188

什么是InfiniBand，它和以太网的区别在于什么？

https://www.cnblogs.com/allcloud/p/8945544.html

InfiniBand 与Intel Omni-Path Architecture

https://zhuanlan.zhihu.com/p/580035183

RoCE vs Infiniband vs TCP/IP

# 国产半导体+

飞腾找台湾世芯代理设计，但因飞腾参与超算建设上了美国黑名单，台湾世芯宁可承受损失也不再继续提供设计服务。

飞腾的通用CPU单核性能变化像是过山车，因为它的FT-1000和FT-1500是基于SUN开源的UltraSPARC T2核心，2007年时的UltraSPARC T2可与Intel至强一较高下。飞腾在投靠ARM之后，CPU性能反而降低了。

飞腾多核互联和多路互联的技术较强，只是在ARM推出原生的互联方案之后，飞腾的研究成果也成了鸡肋。

## Russia CPU

俄罗斯设计通用CPU的企业主要有两家，一家是莫斯科SPARC技术中心(MCST)，另一家是贝加尔电子公司(Baikal Electronics)。

莫斯科SPARC技术中心是俄罗斯自主研发CPU的代表，研发了Elbrus系列产品。

贝加尔电子公司推出的Baikal系列CPU都是外购的CPU核心设计，再自行与其它的外购IP进行集成，与我们的国产手机CPU是相同的路线。

https://mp.weixin.qq.com/s/Ud4f5L2tYNbP1oURVBszag

俄罗斯自研Elbrus CPU参数曝光，CEO年近九旬仍未退休

>Boris Babayan，1933年生，俄罗斯科学院院士，Intel院士。俄罗斯CPU之父。Elbrus系列超算和通用CPU的主要研发者，1978年他率领团队研发出世界第一台超标量计算机Elbrus-1，整整领先西方世界十三年。

https://zhuanlan.zhihu.com/p/549196030

甭怂，奣烎，国产通用CPU比俄罗斯勥
