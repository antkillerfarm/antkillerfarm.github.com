---
layout: post
title:  AIGC, CLIP, SORA
category: Generative Model
---

* toc
{:toc}

# AIGC

![](/images/img5/AIGC.png)

![](/images/img5/Web3.png)

---

《A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT》

《ChatGPT is not all you need. A State of the Art Review of large Generative AI models》

---

ChatGPT（文本到文本的对话模型）

DALL-E-2（文本到图像的生成模型）

Codex（文本到代码的生成模型）

Dreamfusion （文本到3D图像）

Flamingo（图像到文本）

Phenaki（文本到视频）

AudioLM（文本到音频）

Galactica（文本到科学文本）

AlphaTensor（自动搜索高性能的矩阵运算逻辑）

---

https://civitai.com/

一个AI艺术的网站

https://space.bilibili.com/12566101

一个AI绘图方面的博主（秋葉aaaki）

https://microsoft.github.io/generative-ai-for-beginners

Generative AI for Beginners

---

在C站，一个南京艺术学院毕业的学美术小姐姐，网名叫娜乌斯嘉，她用自己的照片训练了lora，并分享给全网AI绘画爱好者们使用。

她的B站主页：

https://space.bilibili.com/8095370

---

https://mp.weixin.qq.com/s/H2nqQi2EVQ_EyeDNCRD3Cg

一文回顾AI绘画的成长之路：从简笔画到真实人脸生成

https://www.zhihu.com/question/583294094

Ai绘画半年了，到目前为止，AI绘画让多少画师失业了？未来又会有多少?

https://www.zhihu.com/question/584139316

AI绘画引入ControlNet，将会带来哪些影响？

https://www.zhihu.com/question/584053473

如何评价2023年2月AI绘画的最新水平？

https://mp.weixin.qq.com/s/HpziNAqHY9Oetsgk2AVxEg

ControlNet组合拳效果惊人，颠覆AI绘画游戏规则

https://zhuanlan.zhihu.com/p/615522634

AIGC的一些记录

https://www.zhihu.com/question/593770520

首批因AI失业的人来了，有公司已裁减原画师，导演陆川力赞AI海报高效优质，哪些职位容易被取代？

https://zhuanlan.zhihu.com/p/619730103

AI绘画教程：如何用Stable Diffusion始终画同一个人？

https://zhuanlan.zhihu.com/p/626335914

AI绘图StableDiffusion最强大模型盘点 - 诸神乱战

https://zhuanlan.zhihu.com/p/622914660

《Stable Diffusion 倚天剑术》第1卷：在各种设备上把Stable Diffusion玩起来

https://zhuanlan.zhihu.com/p/629348322

StableDiffusion LoRA自训练教程

https://mp.weixin.qq.com/s/DBLMAEbVw6v4xH94-5Zl3w

GAN逆袭归来！清华校友论文引爆AI绘图圈，一秒把大象P转身，Diffusion黯然失色

https://zhuanlan.zhihu.com/p/643872569

AI这样把NB写在脸上，它在玩一种很新的艺术

https://zhuanlan.zhihu.com/p/626004957

利用AI在独立游戏项目中大干快上

https://zhuanlan.zhihu.com/p/664461927

一天时间，我用AI做了一个恐龙网站

# CLIP

CLIP由OpenAI在2021年1月发布。

CLIP模型在不使用ImageNet数据集的任何一张图片（zero-shot）进行训练的的情况下，最终模型精度能跟一个有监督的训练好的ResNet-50打成平手（在ImageNet上zero-shot精度为76.2%，这在之前一度被认为是不可能的）。

为了训练CLIP，OpenAI从互联网收集了共4个亿的文本-图像对，论文称之为WIT(Web Image Text，WIT质量很高，而且清理的非常好，其规模相当于JFT-300M，这也是CLIP如此强大的原因之一，后续在WIT上还孕育出了DALL-E模型)

![](/images/img5/CLIP.png)

Text Encoder可以采用NLP中常用的text transformer模型。而Image Encoder可以采用常用CNN模型或者vision transformer等模型，相似度是计算文本特征和图像特征的余弦相似性cosine similarity。

CLIP建立了“文字潜在空间”到“图片潜在空间”的对应关系。上面提到的在ImageNet上的测试，属于“图生文”，配合Diffusion Model即可“文生图”。这类模型也被称为VLP（Vision-Language Pre-training）模型。

https://zhuanlan.zhihu.com/p/493489688

神器CLIP：连接文本和图像，打造可迁移的视觉模型

https://zhuanlan.zhihu.com/p/589170132

FLIP：通过图像掩码加速CLIP训练

https://www.zhihu.com/question/593888697

如何评价Meta/FAIR 最新工作Segment Anything？

https://zhuanlan.zhihu.com/p/620271321

最强Zero-Shot视觉应用：Grounding DINO + Segment Anything + Stable Diffusion

https://www.zhihu.com/question/593914819

Meta发布图像分割论文Segment Anything，将给CV研究带来什么影响？

https://blog.csdn.net/v_JULY_v/article/details/131205615

AI绘画原理解析：从CLIP、BLIP到DALLE、DALLE 2、DALLE 3、Stable Diffusion

## BLIP

![](/images/img5/BLIP.png)

Image-Text Contrastive Loss目标是对齐视觉和文本的特征空间。

Image-Text Matching Loss调整视觉和语言之间的细粒度对齐。

Language Modeling Loss使模型能够将视觉信息转换为连贯的字幕。

![](/images/img5/BLIP_2.png)

BLIP-2的视觉侧和文本侧分别使用预训练的CLIP ViT-G/14模型和FLAN-T5模型，仅中间的起桥接作用的Q-Former参与训练，训练需要的成本和数据量进一步降低，BLIP-2的训练数据量仅129M，16卡A100训练9天。

此后这类桥接预训练的大模型（无论是CV还是NLP）的工作逐渐成为了主流，从头开始训练的，渐趋式微。

## DALLE 2

![](/images/img5/DALLE.png)

DALLE由于是前CLIP时代的东西，已经有些过时了。这里直接介绍DALLE 2。

prior的训练：根据文本特征(即CLIP text encoder编码后得到的文本特征)，预测图像特征(CLIP image encoder编码后得到的图片特征)

decoder生成图：常规的扩散模型解码器，解码生成图像。这里的decoder就是升级版的GLIDE(GLIDE基于扩散模型)。

## DALLE 3

对于稍微复杂的文本，目前的文生图模型生成的图像往往会容易忽略部分文本描述，甚至无法生成文本所描述的图像。这个问题主要还是由于训练数据集本身所造成的，更具体的是说是图像caption不够准确。

图像常规的文本描述往往过于简单(比如COCO数据集)，它们大部分只描述图像中的主体而忽略图像中其它的很多信息，比如背景，物体的位置和数量，图像中的文字等。

DALLE 3 = 原始caption和合成长caption混合训练 + T5 + latent decoder

# SORA

https://openai.com/research/video-generation-models-as-world-simulators

官方技术报告

MS解读SORA的论文：

《Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models》

---

SORA的使用方法：

![](/images/img5/SORA.png)

SORA的基本框架：

![](/images/img5/SORA_2.png)

---

![](/images/img5/SORA_3.png)

训练时可以采用Masked Diffusion Transformer (MDT)的方法增强学习效果。

---

![](/images/img5/NaViT.png)

NaViT（Native Resolution ViT）没有采用传统的将图像调整至固定大小的做法，而通过特定的架构来实现对任意分辨率和宽高比图像的灵活处理。打包支持保持宽高比的可变分辨率图像，减少了训练时间，提高了性能，并增加了灵活性。

---

参考：

https://zhuanlan.zhihu.com/p/683231546

Sora大模型技术精要——原理、关键技术、模型架构与未来趋势

https://fisherdaddy.com/posts/sora-reading-list/

"Road to Sora" 论文阅读清单

https://blog.csdn.net/v_JULY_v/article/details/134655535

Sora之前的视频生成发展史：从Gen2、Emu Video到PixelDance、SVD、Pika 1.0

https://blog.csdn.net/v_JULY_v/article/details/136845242

视频生成Sora的从零复现：从Latte、Open-Sora(含1.0及其升级版)到StreamingT2V

https://zhouyifan.net/2024/06/05/20240405-SVD/

Stable Video Diffusion结构浅析与论文速览
