---
layout: post
title:  Large Language Model（五）——AI agent, LLM实战
category: Attention 
---

* toc
{:toc}

# LLM应用

![](/images/img6/LLM_IC.png)

大语言模型参与的芯片前端设计迭代流程

---

Vibe Coding是一种2025年才出现、以AI为核心的全新软件开发方式：开发者不再逐行手写代码，而是用自然语言把“想要什么”告诉大模型，由AI生成、修改并反复迭代，直到“感觉对了”就上线——整个过程更像“对话＋氛围感知”，而不是传统意义上的编码。

https://www.zhihu.com/question/1944542162130833916

什么是Vibe Coding？

# AI agent

AI agent本质上是一个构建在LLM(大模型)之上的智能应用，也就是说AI agent是大模型的上层应用。如果说把AI比做一个人，那么大模型就是这个人的大脑，虽然它拥有了智能，但其却没有能够真正做事的实体。而AI agent就相当于人的手脚眼睛和嘴巴，以及各种人类能够利用的工具。

![](/images/img5/AI_agent.png)

https://blog.csdn.net/qq_33453797/article/details/138324548

爆火的AI Agent到底是什么？有了大模型为什么还需要AI Agent？

https://blog.csdn.net/v_JULY_v/article/details/135868163

智能体AI Agent的极速入门：从ReAct、AutoGPT到AutoGen、QwenAgent、XAgent、MetaGPT

https://blog.csdn.net/v_JULY_v/article/details/142796658

OmniH2O——通用灵巧且可全身远程操作并学习的人形机器人

https://blog.csdn.net/v_JULY_v/article/details/142769965

VLM驱动机器狗——从UMI on Legs到Helpful DoggyBot：分别把机械臂装到机器狗背上、夹爪装到机器狗嘴里

https://zhuanlan.zhihu.com/p/1917883331829273687

Devin炮轰Claude：别再搞Multi-Agent了

---

MCP（Model Context Protocol）由Anthropic于2024年11月推出，是一个开放标准协议，用于统一大模型与外部工具、数据源之间的交互方式。

https://zhuanlan.zhihu.com/p/1895177200665350365

大白话聊一聊Tool、MCP和Agent来龙去脉

---

https://github.com/OthmanAdi/planning-with-files

Manus原理

https://zhuanlan.zhihu.com/p/1993358891099111451

Manus核心Context技术被人做成了Skills

# LLM实战

书籍：

https://jax-ml.github.io/scaling-book/

How to Scale Your Model

---

实战心得：

https://pytorch.org/blog/high-performance-llama-2/

https://huggingface.co/blog/zh/bloom-inference-optimization

https://huggingface.co/blog/zh/bloom-megatron-deepspeed

---

![](/images/img5/Spike.webp)

训练过程中，损失偶尔会出现毛刺的情况。针对这种情况，Falcon作者会恢复到上一个最新的Checkpoint，并跳过1B Token数据继续训练。作者训练Falcon-180B时出现了9次毛刺。

Google训练PaLM模型遇到了同样的问题。针对此种情况，作者会重启训练，并从毛刺之前的100个step开始，跳过200-500个Batch的数据。作者也做了消融实验，发现并不是单个数据的问题，而可能是这连续的一系列Batch数据引起的。

https://mp.weixin.qq.com/s/rLJlaqI2RL7TGUEQyx-QaA

万卡GPU集群实战：探索LLM预训练的挑战

---

MFU（Model FLOPs Utilization）

$$\text{MFU} = \frac{\text{model FLOPs per iteration}}{\text{GPU单卡算力} \times \text{卡数} \times \text{一次迭代时间}}$$

Transformer模型的MFU计算公式：

$$\text{MFU} = \frac{72blsh^2 \left(1 + \frac{s}{6h} + \frac{V}{12hl}\right)}{F \times N \times T}$$

- b：批次大小（micro batch size）。
- l：Transformer 层数。
- s：序列长度。
- h：隐藏层维度。
- V：词表大小。
- F：GPU 单卡的峰值算力（FLOPS）。
- N：使用的 GPU 卡数量。
- T：一次迭代时间（秒）。

---

https://docs.swanlab.cn/zh/examples/pretrain_llm.html

从零预训练一个自己的大模型

PS: swanlab的文档库本身也是一个宝库，而且还是中文的。

本人的魔改版本，wiki_zh dataset + Qwen tokenizer + llama 2 model：

https://github.com/antkillerfarm/antkillerfarm_crazy/tree/master/python/ml/huggingface/llm_train.py

---

在大模型训练过程中，为确保训练的稳定性与有效性，需密切关注多项关键指标以评估训练状态，其中包括但不限于perplexity (PPL)、gradient norm (GNorm)、activation norm、内存占用情况以及Loss scale等参数。

梯度裁剪（Gradient Clipping）：作为一种常用的稳定训练手段，通常设定裁剪阈值为1.0，防止梯度过大引发训练不稳定。

Weight Decay（L2正则化）：设置合理的权重衰减率，如0.1，有助于防止过拟合，增强模型泛化能力。

特殊层的调整：GLM研究发现，embedding层往往存在较大的梯度异常情况，故需根据实际情况适度调整相关参数。

---

https://blog.csdn.net/v_JULY_v/article/details/132178447

七月论文审稿GPT第1版：通过3万多篇paper和10多万的review数据微调RWKV

https://blog.csdn.net/v_JULY_v/article/details/134183799

七月论文审稿GPT第2版：用一万多条paper-review数据微调LLaMA2 7B最终反超GPT4

https://blog.csdn.net/v_JULY_v/article/details/131552592

基于LangChain+LLM的本地知识库问答：从企业单文档问答到批量文档问答

https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5

这个网页收录了作者训练LLM时，各项指标的变化曲线。

# VLM

https://zhuanlan.zhihu.com/p/702811733

Vision-Language Models (VLMs)多模态大模型一年多的进展与思考-2406

# C/C++参考资源=

## std::tuple

```cpp
auto tuple = std::make_tuple(1, 'A', "test");
std::cout << std::get<0>(tuple) << std::endl;
std::cout << std::get<1>(tuple) << std::endl;
std::cout << std::get<2>(tuple) << std::endl;
```

## std::tie

可用tuple或pair返回多个返回值，然后用tie进行解包。这里的语法非常类似python。

```cpp
std::pair<int, std::string> fun_tie(int a, std::string str)
{
  return std::make_pair(a, str);
}

int a;
std::string str;
std::tie(a , str) = fun_tie(12, std::string("Pony Ma"));
std::cout << a << "," << str << std::endl;
```

## inl文件

inl文件是内联函数的源文件。内联函数通常在C++头文件中实现，但是当C++头文件中内联函数过多的情况下，我们想使头文件看起来简洁点，能不能像普通函数那样将内联函数声明和函数定义放在头文件和实现文件中呢？

当然答案是肯定的，具体做法将是：将内联函数的具体实现放在inl文件中，然后在该头文件末尾使用#include引入该inl文件。

参考：

https://www.cnblogs.com/findumars/p/4340936.html

C++中的INL

## 常用libc实现

libc是C语言标准库的简称，它有多种实现。除了最常用的gcc自带的glibc之外，还有musl、uClibc、dietlibc等。

http://www.etalabs.net/compare_libcs.html

这个网址是以上4种libc实现的比较结果。从结果来看，musl比较有投资价值。实际上，最近（2015.5）的OpenWrt项目就已经将libc由uClibc改为musl。我也是因为这个原因，才知道musl的。

当然这个表并不完整，其他的libc可以参见:

https://en.wikipedia.org/wiki/C_standard_library

## 有价值的C/C++库

https://github.com/fffaraz/awesome-cpp

一个专门收集各种C/C++库的网页

https://github.com/gabime/spdlog

一个C++的log库

https://zhuanlan.zhihu.com/p/674073158

超详细！spdlog源码解析

## 模板元编程

这段代码用来在编译期根据“一堆长度”生成一个N维静态数组类型。

```cpp
template<typename A, size_t... dims>
struct variadic_array;
// 告诉编译器：后面会有一个模板variadic_array，它接收一个类型A和任意数量的size_t类型的参数。此时没有定义，只做了声明。
template<typename A, size_t first_dim, size_t... rest_dims>
struct variadic_array<A, first_dim, rest_dims...> {
    using type = typename variadic_array<A, rest_dims...>::type[first_dim];
};
// 当模板实参列表里至少有一个size_t时，这个版本被选中。它把第一个维度first_dim拿出来，然后递归到剩下的维度rest_dims...。
template<typename A>
struct variadic_array<A> {
    using type = A;
};
// 当把所有维度都剥光后，只剩下类型A，递归停止。
template<typename A, size_t... dims> 
using variadic_array_t = typename variadic_array<A, dims...>::type;
// 给整个元函数起一个别名，variadic_array_t<float, 3, 4, 5>等价于float[3][4][5]。
```

`...`只能出现在模板参数列表或函数参数列表里，声明一般为`typename... Ts`的形式，而使用则是`Ts...`。前者也被称为`pack`，后者则是`unpack`。

使用`sizeof...(Ts)`运算符可以得到pack的数量。

## C/C++参考资源

`vector<bool>`并不是一个通常意义上的vector容器，这个源自于历史遗留问题。 早在C++98的时候，就有`vector< bool>`这个类型了，但是因为当时为了考虑到节省空间的想法，所以`vector<bool>`里面不是一个Byte一个Byte储存的，它是一个bit一个bit储存的！

https://www.zhihu.com/question/23367698

c++中为什么不提倡使用`vector<bool>`？

---

```c
struct Test {
  int a;
  int b;
  char c[0];
};
```

Zero-length array不占用结构体的空间，但可以由编译器计算offset，例如上例中指向c的指针，实际上指向了整个结构体的尾部（不含结构体本身）。

---

C++已经有不少解释器：clang-repl，cling等。

---

https://zhuanlan.zhihu.com/p/23016264

这么多款STL，总有一款适合你

https://mp.weixin.qq.com/s/Hpn7KqYlBKz0JdryiozqyQ

每个开发者都应该了解的一些C++特性

https://www.cnblogs.com/wuchanming/p/3913492.html

emplace_back与push_back的区别

https://zhuanlan.zhihu.com/p/82895086

当我们谈论C++时，我们在谈论什么？

https://mp.weixin.qq.com/s/pxyTlQn4wx-N_MaWZc0oAQ

漫谈C++的各种检查

https://mp.weixin.qq.com/s/LchYGGcSbIMVGxO0Uea0RA

深入C++回调
