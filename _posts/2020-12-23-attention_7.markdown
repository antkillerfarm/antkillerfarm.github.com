---
layout: post
title:  Attention（七）——Large Language Model
category: Attention 
---

* toc
{:toc}

# ChatGPT

## 微软小冰（续）

绝大多数的NLP工程师们所做的工程项目，主要是针对某些特定任务提出一个具体的模型，进行有针对性的数据标注，然后再制作模型。简而言之，就是以NLP子任务独立进行研究开发。比如分词、实体识别、文本分类、相似度判别、机器翻译、文摘系统、事件抽取，等等，不一而足。

也就是说，NLP产业界实际上处于一种手工业模式，你干你的，我干我的，针对不同的企业、不同的需求，需要不断地定制模型、定制数据来完成工作。

NLP中，还有一部分内容：知识图谱。知识图谱这个概念专门用来记录现实世界中的客观存在的事务的关联关系，对于 NLP任务也极为重要。更准确地讲，应当叫做领域知识图谱，几乎没有哪个机构可以做出一个通泛的图谱来供应用。

但知识谱图属于有多少人工，就有多少智能的最典型代表。知识图谱做一万年做不到GPT3的水平，就像蒸汽机做的再好也驱动不了登月火箭。

ChatGPT已经完全抹去了传统NLP业态中，需要分不同子任务、分不同领域数据场景的手工业模式，而是直接采用大模型，以对话形式，直接形成了大一统，进入了机器时代。类似于传统的手工纺织女工，完全由机器替代了。很多NLP子领域不再具备独立研究价值。

---

评价NLP模型的效果，应当从两方面入手：

一方面，是评价模型对自然语言本身的拟合，比如，前后语句连贯、通顺、符合正常人类的叙述习惯，能够理解反问、反讽、情绪、基本世界观的构建和逻辑推断。客观讲，ChatGPT已经做到了极致，它的语言表达水平，比很多人的作文能力都强非常多。

另一方面，是评价模型对知识、事实类信息的拟合，这块能力ChatGPT还无法很好胜任，比如模型会告诉我3+13=23，汪小菲的姐姐是大S，谷歌的CEO是库克，等等，大家也都明白，大量的博客、文章、段子，讨论过很多了，不再赘述。

目前，这些事实、知识，是由知识图谱来解决的。但以笨拙的实体、关系、属性等为基本概念所作的构建，显然是没有前途的。

---

参考：

https://zhuanlan.zhihu.com/p/158009816

开域聊天机器人-微软小冰的技术介绍（现实篇）

https://mp.weixin.qq.com/s/wBsh9dmMPks04X2pDB8Ang

沈向洋等重磅论文：公开微软小冰系统设计，迄今最详细！

https://www.zhihu.com/question/583134530

微软解散元宇宙团队投资近900亿搞ChatGPT，如何从商业角度解读此举？

https://zhuanlan.zhihu.com/p/605673596

ChatGPT这么强，会影响NLPer的就业环境吗

https://www.zhihu.com/question/575391861

ChatGPT印证了模型大一统的可行性，这在未来五年会对NLP从业者带来怎样的冲击？

## GPT-4

2023.3.15 OpenAI推出GPT-4。

GPT-4可以接受图像和文本输入，而GPT-3.5只接受文本。

官网：

https://openai.com/research/gpt-4

技术报告：

https://arxiv.org/abs/2303.08774

GPT-4 Technical Report

由于该技术报告更多的是效果报告，而非原理介绍。因此最有价值的反而是稍早之前的Kosmos-1模型的论文：

https://arxiv.org/abs/2302.14045

Language Is Not All You Need: Aligning Perception with Language Models

---

3月16日，百度推出“文心一言”，可惜效果惨不忍睹。。。股价已暴跌近10%。

2022年某统计显示互联网上开放信息中文占比仅1.3%，而英文占比63%，其中高质量部分优势更大，比如论文。

模型的基础能力强吊打一切，语种只是上层表达。

RLHF论文中的训练数据英文占比99%+，西、法、德语还占了剩下的大部分，中文估计就是0.0x%这个级别，效果大家都体验到了，中文和其他小语种能力的提升同样也非常显著，这很强的证明这种训练方法是让模型学到了跨越语种的隐含信息。

4月7日，阿里云在微信公众号官宣：自研大模型“通义千问”开始邀请用户测试体验。

---

写一段关于酒的文字，不能出现“酒”字。

只有GPT4做到了。GPT3.5提到了红酒杯，出现了一次酒。国内几个模型开篇就是“酒是xxxx”，拿酒当了主语。

http://www.199it.com/archives/1611000.html

新华社研究院：人工智能大模型体验报告

---

参考：

https://www.zhihu.com/question/589639535

OpenAI发布GPT-4，有哪些技术上的优化或突破？

https://mp.weixin.qq.com/s/iw0wESsyP8nkPuFkj_EkOg

OpenAI正式推出多模态GPT-4

https://www.zhihu.com/question/589937459

3月16日百度举办“文心一言”发布会，现场有哪些信息值得关注？

https://www.zhihu.com/question/589941496

百度正式推出“文心一言”，然而港股股价已暴跌近10%，客观来说其能力与ChatGPT相较如何？

# Large Language Model

![](/images/img5/huge_model.jpg)

OpenAI的GPT 3的规模为175B，Google的LaMDA规模为137B，PaLM的规模为540B，DeepMind的Gogher规模为280B。

国内也有中文巨型模型，比如清华&智谱GLM规模130B，华为“盘古”规模200B，百度“文心”规模260B，浪潮“源1.0”规模245B。

然而，OpenAI的中小模型（如 6B~13B）的性能已经远超一众超大模型（如130B的GLM和175B的OPT）。由于实力的不对等，OpenAI、Google、DeepMind等LLM头部玩家可能不再会公开最前沿的LLM研究进展（转为挤牙膏模式）。

悟道2.0是一个基于MoE的稀疏模型，总参数量超过万亿，存一个checkpoint 20T。模型是神威上训练的，那玩意没有GPU或者CUDA，是神威自己的一套底层架构，因此适配起来非常困难。这个项目更像是一个国产硬件上训练巨大模型的尝试，项目启动之初就知道不可能投入到产品。

其次，稀疏模型的参数量和普通模型没有太大的可比性。

175B级别的训练的成本非常高。例如2020年GPT-3的单次训练成本约460万美元，总训练成本达1200万美元，如果推算到2023，国内单次训练成本约662-1170万RMB。以2022年为例，OpenAI运行成本为5.44亿美元，其中约有2亿多美元是工资/劳务费。

国内这一堆利润导向的“AI研究”公司，阿里的达摩院甚至还闹出过自负盈亏这种笑话，让这些公司花费大量时间和金钱去做预训练语料库，估计比让恒大还清债务还难。

当初微软为了帮助OpenAI训练，在2020年给OpenAI搭建了一个有285000核CPU和10000个V100 GPU的超算环境。

Q：微软调度了1万张卡给OpenAI做训练，商汤的反馈是我们国内最多也就调动2000张卡，这个技术有难度吗？

A：我目前了解到这一块国内目前没有哪个达到1万张量级的，不可能的，现在没有这水平。现在的确从百度视角来看，现在几千张卡在测试就不错了。从技术积累来说，涉及到集群做调度还是有一些协同性的难度。我认为包括商汤、百度，离微软的差距还是短时间内无法追上的。

国内先搞一波小参数的大模型，PR一定要cover机器之心、新智元、量子位，然后宣称自己的130b模型超越了gpt4，并在自己的榜上发布测评结果，成功超越gpt4。最后一堆商业公司来买130b的模型，这就算是创业成功了，毕竟一套价格不菲，几千万。

国内的研究者总是发布达到chatgpt4 106%能力的工作，人家一更换测评数据集就泯然众人矣了。

参考：

https://www.zhihu.com/question/584132646

中国的大语言模型“悟道2.0”参数是GPT-3十倍，是否中国在大语言模型训练技术上已经远远超过美国？

https://zhuanlan.zhihu.com/p/463352552

稀疏性在机器学习中的发展趋势——Sparsity，稀疏激活，高效计算，MoE，稀疏注意力机制

https://zhuanlan.zhihu.com/p/254821426

乘风破浪的PTM：两年来预训练模型的技术进展（2020年之前的主流技术）

https://zhuanlan.zhihu.com/p/597586623

通向AGI之路：大型语言模型（LLM）技术精要

https://mp.weixin.qq.com/s/eV_9Mi2879w_gfoyiSm8Ug

LLM全景图（The Landscape of LLM）

https://www.zhihu.com/question/604592470

前两个月国产类ChatGPT大模型如雨后春笋，为何最近都没声音了?

https://mp.weixin.qq.com/s/oqhi58NcEVH1oVrW2p4xlQ

大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介

https://mp.weixin.qq.com/s/fUAUr9X3XLndfjga2QIHbA

大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning

https://mp.weixin.qq.com/s/f4l04f78F507JRrCawnV8w

大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2

https://mp.weixin.qq.com/s/nUAcCz6mcgGuUeuTfgqmOQ

大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体

https://mp.weixin.qq.com/s/N_N6RqKB9pjZ1tozfM5f5A

大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA

https://mp.weixin.qq.com/s/M2nds_FJBXooi08qDU-4yA

大模型参数高效微调技术原理综述（六）-MAM Adapter、UniPELT

https://mp.weixin.qq.com/s/P_AmTa4s8dOyc_0fZBgNPA

大模型参数高效微调技术原理综述（七）-最佳实践、总结

https://zhuanlan.zhihu.com/p/618695885

LLaMA, Alpaca, ColossalChat系列模型研究

https://zhuanlan.zhihu.com/p/642412124

LLM的推理优化技术纵览

# BERT进阶

## AR vs AE

自回归模型，是统计上一种处理时间序列的方法，用同一变数例如x的之前各期，亦即$$x_1$$至$$x_{t-1}$$来预测本期$$x_t$$的表现，并假设它们为一线性关系。因为这是从回归分析中的线性回归发展而来，只是不用x预测y，而是用x预测x自己，所以叫做自回归。

---

**AR**: Autoregressive Lanuage Modeling，又叫自回归语言模型。它指的是，依据前面(或后面)出现的tokens来预测当前时刻的token，代表模型有ELMO、GTP等。

$$\text{forward:}p(x)=\prod_{t=1}^Tp(x_t|x_{<t})$$

$$\text{backward:}p(x)=\prod_{t=T}^1p(x_t|x_{>t})$$

- 缺点：它只能利用单向语义而不能同时利用上下文信息。ELMO通过双向都做AR模型，然后进行拼接，但从结果来看，效果并不是太好。

- 优点：对自然语言生成任务(NLG)友好，天然符合生成式任务的生成过程。这也是为什么GPT能够编故事的原因。

**AE**:Autoencoding Language Modeling，又叫自编码语言模型。通过上下文信息来预测当前被mask的token，代表有BERT，Word2Vec(CBOW)。

$$p(x)=\prod_{x\in Mask}p(x_t|context)$$

- 缺点：由于训练中采用了MASK标记，导致预训练与微调阶段不一致的问题。此外对于生成式问题，AE模型也显得捉襟见肘，这也是目前为止，BERT为数不多没有实现大的突破的领域。

- 优点：能够很好的编码上下文语义信息，在自然语言理解(NLU)相关的下游任务上表现突出。
