---
layout: post
title:  Attention（七）——AR vs AE, Large Language Model
category: Attention 
---

* toc
{:toc}

# ChatGPT

## 微软小冰（续）

ChatGPT的出现打了两种人的脸：一种是对强人工智能保持悲观态度，认为强人工智能很长时间内都不可能出现的；一种是对超大模型持怀疑态度，认为通过超大模型来实现人工智能是错误道路的。

在2022年11月30日之前，市面上有大大小小的互联网或IT企业需要进行文本处理，相应地，也就需要雇佣大量的NLP工程师们来解决相关的问题。

绝大多数的NLP工程师们所做的工程项目，主要是针对某些特定任务提出一个具体的模型，进行有针对性的数据标注，然后再制作模型。简而言之，就是以NLP子任务独立进行研究开发。比如分词、实体识别、文本分类、相似度判别、机器翻译、文摘系统、事件抽取，等等，不一而足。

也就是说，NLP产业界实际上处于一种手工业模式，你干你的，我干我的，针对不同的企业、不同的需求，需要不断地定制模型、定制数据来完成工作。

NLP中，还有一部分内容：知识图谱。知识图谱这个概念专门用来记录现实世界中的客观存在的事务的关联关系，对于 NLP任务也极为重要。更准确地讲，应当叫做领域知识图谱，几乎没有哪个机构可以做出一个通泛的图谱来供应用。

但知识谱图属于有多少人工，就有多少智能的最典型代表。知识图谱做一万年做不到GPT3的水平，就像蒸汽机做的再好也驱动不了登月火箭。

ChatGPT已经完全抹去了传统NLP业态中，需要分不同子任务、分不同领域数据场景的手工业模式，而是直接采用大模型，以对话形式，直接形成了大一统，进入了机器时代。类似于传统的手工纺织女工，完全由机器替代了。很多NLP子领域不再具备独立研究价值。

---

评价NLP模型的效果，应当从两方面入手：

一方面，是评价模型对自然语言本身的拟合，比如，前后语句连贯、通顺、符合正常人类的叙述习惯，能够理解反问、反讽、情绪、基本世界观的构建和逻辑推断。客观讲，ChatGPT已经做到了极致，它的语言表达水平，比很多人的作文能力都强非常多。

另一方面，是评价模型对知识、事实类信息的拟合，这块能力ChatGPT还无法很好胜任，比如模型会告诉我3+13=23，汪小菲的姐姐是大S，谷歌的CEO是库克，等等，大家也都明白，大量的博客、文章、段子，讨论过很多了，不再赘述。

目前，这些事实、知识，是由知识图谱来解决的。但以笨拙的实体、关系、属性等为基本概念所作的构建，显然是没有前途的。

---

参考：

https://zhuanlan.zhihu.com/p/158009816

开域聊天机器人-微软小冰的技术介绍（现实篇）

https://mp.weixin.qq.com/s/wBsh9dmMPks04X2pDB8Ang

沈向洋等重磅论文：公开微软小冰系统设计，迄今最详细！

https://www.zhihu.com/question/583134530

微软解散元宇宙团队投资近900亿搞ChatGPT，如何从商业角度解读此举？

https://zhuanlan.zhihu.com/p/605673596

ChatGPT这么强，会影响NLPer的就业环境吗

https://www.zhihu.com/question/575391861

ChatGPT印证了模型大一统的可行性，这在未来五年会对NLP从业者带来怎样的冲击？

## GPT-4

2023.3.15 OpenAI推出GPT-4。

GPT-4可以接受图像和文本输入，而GPT-3.5只接受文本。

官网：

https://openai.com/research/gpt-4

技术报告：

https://arxiv.org/abs/2303.08774

GPT-4 Technical Report

由于该技术报告更多的是效果报告，而非原理介绍。因此最有价值的反而是稍早之前的Kosmos-1模型的论文：

https://arxiv.org/abs/2302.14045

Language Is Not All You Need: Aligning Perception with Language Models

---

2023.11.6 GPT-4 Turbo：支持128k上下文，相当于300页文档，输入价格大降2/3，速率限制翻倍。

---

3月16日，百度推出“文心一言”，可惜效果惨不忍睹。。。股价已暴跌近10%。

2022年某统计显示互联网上开放信息中文占比仅1.3%，而英文占比63%，其中高质量部分优势更大，比如论文。

模型的基础能力强吊打一切，语种只是上层表达。

RLHF论文中的训练数据英文占比99%+，西、法、德语还占了剩下的大部分，中文估计就是0.0x%这个级别，效果大家都体验到了，中文和其他小语种能力的提升同样也非常显著，这很强的证明这种训练方法是让模型学到了跨越语种的隐含信息。

4月7日，阿里云在微信公众号官宣：自研大模型“通义千问”开始邀请用户测试体验。

---

写一段关于酒的文字，不能出现“酒”字。

只有GPT4做到了。GPT3.5提到了红酒杯，出现了一次酒。国内几个模型开篇就是“酒是xxxx”，拿酒当了主语。

http://www.199it.com/archives/1611000.html

新华社研究院：人工智能大模型体验报告

---

如果严格来论，目前国内的自研大模型，不论是零一万物的Yi，还是百川智能的Baichuan，或者阿里旗下的通义千问，架构上和LLaMA都是一致的。

大家遥远的记忆中的Start-Copy Lee又回来了。

https://www.zhihu.com/question/630152920

如何看待李开复零一万物开源Yi大模型被指抄袭LLaMA?

---

参考：

https://www.zhihu.com/question/589639535

OpenAI发布GPT-4，有哪些技术上的优化或突破？

https://mp.weixin.qq.com/s/iw0wESsyP8nkPuFkj_EkOg

OpenAI正式推出多模态GPT-4

https://www.zhihu.com/question/589937459

3月16日百度举办“文心一言”发布会，现场有哪些信息值得关注？

https://www.zhihu.com/question/589941496

百度正式推出“文心一言”，然而港股股价已暴跌近10%，客观来说其能力与ChatGPT相较如何？

## Chinchilla Law

DeepMind研究了在给定算力预算下训练Transformer语言模型的最佳模型大小和tokens数量。

应该使用1,400B (1.4T) tokens来训练参数量大小为70B的LLM最佳。即：每个参数需要大约20个文本token。

https://zhuanlan.zhihu.com/p/627821763

训练LLM需要多少数据？

https://www.zhihu.com/question/628395521

如何看待微软论文声称ChatGPT是20B(200亿)参数量的模型？

# AR vs AE

自回归模型，是统计上一种处理时间序列的方法，用同一变数例如x的之前各期，亦即$$x_1$$至$$x_{t-1}$$来预测本期$$x_t$$的表现，并假设它们为一线性关系。因为这是从回归分析中的线性回归发展而来，只是不用x预测y，而是用x预测x自己，所以叫做自回归。

---

**AR**: Autoregressive Lanuage Modeling，又叫自回归语言模型。它指的是，依据前面(或后面)出现的tokens来预测当前时刻的token，代表模型有ELMO、GTP等。

$$\text{forward:}p(x)=\prod_{t=1}^Tp(x_t|x_{<t})$$

$$\text{backward:}p(x)=\prod_{t=T}^1p(x_t|x_{>t})$$

- 缺点：它只能利用单向语义而不能同时利用上下文信息。ELMO通过双向都做AR模型，然后进行拼接，但从结果来看，效果并不是太好。

- 优点：对自然语言生成任务(NLG)友好，天然符合生成式任务的生成过程。这也是为什么GPT能够编故事的原因。

**AE**:Autoencoding Language Modeling，又叫自编码语言模型。通过上下文信息来预测当前被mask的token，代表有BERT，Word2Vec(CBOW)。

$$p(x)=\prod_{x\in Mask}p(x_t|context)$$

- 缺点：由于训练中采用了MASK标记，导致预训练与微调阶段不一致的问题。此外对于生成式问题，AE模型也显得捉襟见肘，这也是目前为止，BERT为数不多没有实现大的突破的领域。

- 优点：能够很好的编码上下文语义信息，在自然语言理解(NLU)相关的下游任务上表现突出。

---

针对AR和AE的不同特点，又将前者称为Causal LM，后者称为Masked LM。

Causal：每个token只能看到在它之前的token信息，而看不到在它之后的token。

Masked：完形填空。

---

2023.3

ChatGPT的出现，为自然语言生成任务找到了商业化的路径。有鉴于此，Google也不得不在BERT上对AR模型，做了一些有损逼格的妥协。。。囧

![](/images/img5/T5.png)

deep encoder+shallow decoder

---

参考：

https://mp.weixin.qq.com/s/n6F6MTjrUCmvEoaLiVZpxA

更深的编码器+更浅的解码器=更快的自回归模型

https://mp.weixin.qq.com/s/pe2E69Gpw0nT9sSHvtBGSg

自回归与非自回归模型不可兼得？预训练模型BANG全都要！

https://www.zhihu.com/question/588325646

为什么现在的LLM都是Decoder only的架构？

https://www.zhihu.com/question/592545459

大模型都是基于Transformer堆叠，采用Encoder或者Decoder堆叠，有什么区别？

# Large Language Model

![](/images/img5/LLM_Tree.png)

![](/images/img5/LLM.png)

https://arthurchiao.art/blog/llm-practical-guide-zh/

大语言模型（LLM）综述与实用指南（Amazon，2023）

---

![](/images/img5/huge_model.jpg)

OpenAI的GPT 3的规模为175B，Google的LaMDA规模为137B，PaLM的规模为540B，DeepMind的Gogher规模为280B。

国内也有中文巨型模型，比如清华&智谱GLM规模130B，华为“盘古”规模200B，百度“文心”规模260B，浪潮“源1.0”规模245B。

然而，OpenAI的中小模型（如 6B~13B）的性能已经远超一众超大模型（如130B的GLM和175B的OPT）。由于实力的不对等，OpenAI、Google、DeepMind等LLM头部玩家可能不再会公开最前沿的LLM研究进展（转为挤牙膏模式）。

悟道2.0是一个基于MoE的稀疏模型，总参数量超过万亿，存一个checkpoint 20T。模型是神威上训练的，那玩意没有GPU或者CUDA，是神威自己的一套底层架构，因此适配起来非常困难。这个项目更像是一个国产硬件上训练巨大模型的尝试，项目启动之初就知道不可能投入到产品。

其次，稀疏模型的参数量和普通模型没有太大的可比性。

175B级别的训练的成本非常高。例如2020年GPT-3的单次训练成本约460万美元，总训练成本达1200万美元，如果推算到2023，国内单次训练成本约662-1170万RMB。以2022年为例，OpenAI运行成本为5.44亿美元，其中约有2亿多美元是工资/劳务费。
