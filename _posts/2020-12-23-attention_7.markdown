---
layout: post
title:  Attention（七）——Large Language Model
category: Attention 
---

* toc
{:toc}

# ChatGPT

## 微软小冰（续）

小冰的技术原理，走的是传统NLP原理那一套，已经过时了，没有使用深度学习，基于知识图谱回答，学习的知识非常有限。

ChatGPT的出现打了两种人的脸：一种是对强人工智能保持悲观态度，认为强人工智能很长时间内都不可能出现的；一种是对超大模型持怀疑态度，认为通过超大模型来实现人工智能是错误道路的。

在2022年11月30日之前，市面上有大大小小的互联网或IT企业需要进行文本处理，相应地，也就需要雇佣大量的NLP工程师们来解决相关的问题。

绝大多数的NLP工程师们所做的工程项目，主要是针对某些特定任务提出一个具体的模型，进行有针对性的数据标注，然后再制作模型。简而言之，就是以NLP子任务独立进行研究开发。比如分词、实体识别、文本分类、相似度判别、机器翻译、文摘系统、事件抽取，等等，不一而足。

也就是说，NLP产业界实际上处于一种手工业模式，你干你的，我干我的，针对不同的企业、不同的需求，需要不断地定制模型、定制数据来完成工作。

NLP中，还有一部分内容：知识图谱。知识图谱这个概念专门用来记录现实世界中的客观存在的事务的关联关系，对于 NLP任务也极为重要。更准确地讲，应当叫做领域知识图谱，几乎没有哪个机构可以做出一个通泛的图谱来供应用。

但知识谱图属于有多少人工，就有多少智能的最典型代表。知识图谱做一万年做不到GPT3的水平，就像蒸汽机做的再好也驱动不了登月火箭。

ChatGPT已经完全抹去了传统NLP业态中，需要分不同子任务、分不同领域数据场景的手工业模式，而是直接采用大模型，以对话形式，直接形成了大一统，进入了机器时代。类似于传统的手工纺织女工，完全由机器替代了。很多NLP子领域不再具备独立研究价值。

---

评价NLP模型的效果，应当从两方面入手：

一方面，是评价模型对自然语言本身的拟合，比如，前后语句连贯、通顺、符合正常人类的叙述习惯，能够理解反问、反讽、情绪、基本世界观的构建和逻辑推断。客观讲，ChatGPT已经做到了极致，它的语言表达水平，比很多人的作文能力都强非常多。

另一方面，是评价模型对知识、事实类信息的拟合，这块能力ChatGPT还无法很好胜任，比如模型会告诉我3+13=23，汪小菲的姐姐是大S，谷歌的CEO是库克，等等，大家也都明白，大量的博客、文章、段子，讨论过很多了，不再赘述。

目前，这些事实、知识，是由知识图谱来解决的。但以笨拙的实体、关系、属性等为基本概念所作的构建，显然是没有前途的。

---

参考：

https://zhuanlan.zhihu.com/p/158009816

开域聊天机器人-微软小冰的技术介绍（现实篇）

https://mp.weixin.qq.com/s/wBsh9dmMPks04X2pDB8Ang

沈向洋等重磅论文：公开微软小冰系统设计，迄今最详细！

https://www.zhihu.com/question/583134530

微软解散元宇宙团队投资近900亿搞ChatGPT，如何从商业角度解读此举？

https://zhuanlan.zhihu.com/p/605673596

ChatGPT这么强，会影响NLPer的就业环境吗

https://www.zhihu.com/question/575391861

ChatGPT印证了模型大一统的可行性，这在未来五年会对NLP从业者带来怎样的冲击？

## GPT-4

2023.3.15 OpenAI推出GPT-4。

GPT-4可以接受图像和文本输入，而GPT-3.5只接受文本。

官网：

https://openai.com/research/gpt-4

技术报告：

https://arxiv.org/abs/2303.08774

GPT-4 Technical Report

由于该技术报告更多的是效果报告，而非原理介绍。因此最有价值的反而是稍早之前的Kosmos-1模型的论文：

https://arxiv.org/abs/2302.14045

Language Is Not All You Need: Aligning Perception with Language Models

---

3月16日，百度推出“文心一言”，可惜效果惨不忍睹。。。股价已暴跌近10%。

2022年某统计显示互联网上开放信息中文占比仅1.3%，而英文占比63%，其中高质量部分优势更大，比如论文。

模型的基础能力强吊打一切，语种只是上层表达。

RLHF论文中的训练数据英文占比99%+，西、法、德语还占了剩下的大部分，中文估计就是0.0x%这个级别，效果大家都体验到了，中文和其他小语种能力的提升同样也非常显著，这很强的证明这种训练方法是让模型学到了跨越语种的隐含信息。

4月7日，阿里云在微信公众号官宣：自研大模型“通义千问”开始邀请用户测试体验。

---

写一段关于酒的文字，不能出现“酒”字。

只有GPT4做到了。GPT3.5提到了红酒杯，出现了一次酒。国内几个模型开篇就是“酒是xxxx”，拿酒当了主语。

http://www.199it.com/archives/1611000.html

新华社研究院：人工智能大模型体验报告

---

参考：

https://www.zhihu.com/question/589639535

OpenAI发布GPT-4，有哪些技术上的优化或突破？

https://mp.weixin.qq.com/s/iw0wESsyP8nkPuFkj_EkOg

OpenAI正式推出多模态GPT-4

https://www.zhihu.com/question/589937459

3月16日百度举办“文心一言”发布会，现场有哪些信息值得关注？

https://www.zhihu.com/question/589941496

百度正式推出“文心一言”，然而港股股价已暴跌近10%，客观来说其能力与ChatGPT相较如何？

## Chinchilla Law

DeepMind研究了在给定算力预算下训练Transformer语言模型的最佳模型大小和tokens数量。

应该使用1,400B (1.4T) tokens来训练参数量大小为70B的LLM最佳。即：每个参数需要大约20个文本token。

https://zhuanlan.zhihu.com/p/627821763

训练LLM需要多少数据？

https://www.zhihu.com/question/628395521

如何看待微软论文声称ChatGPT是20B(200亿)参数量的模型？

# Large Language Model

![](/images/img5/huge_model.jpg)

OpenAI的GPT 3的规模为175B，Google的LaMDA规模为137B，PaLM的规模为540B，DeepMind的Gogher规模为280B。

国内也有中文巨型模型，比如清华&智谱GLM规模130B，华为“盘古”规模200B，百度“文心”规模260B，浪潮“源1.0”规模245B。

然而，OpenAI的中小模型（如 6B~13B）的性能已经远超一众超大模型（如130B的GLM和175B的OPT）。由于实力的不对等，OpenAI、Google、DeepMind等LLM头部玩家可能不再会公开最前沿的LLM研究进展（转为挤牙膏模式）。

悟道2.0是一个基于MoE的稀疏模型，总参数量超过万亿，存一个checkpoint 20T。模型是神威上训练的，那玩意没有GPU或者CUDA，是神威自己的一套底层架构，因此适配起来非常困难。这个项目更像是一个国产硬件上训练巨大模型的尝试，项目启动之初就知道不可能投入到产品。

其次，稀疏模型的参数量和普通模型没有太大的可比性。

175B级别的训练的成本非常高。例如2020年GPT-3的单次训练成本约460万美元，总训练成本达1200万美元，如果推算到2023，国内单次训练成本约662-1170万RMB。以2022年为例，OpenAI运行成本为5.44亿美元，其中约有2亿多美元是工资/劳务费。

国内这一堆利润导向的“AI研究”公司，阿里的达摩院甚至还闹出过自负盈亏这种笑话，让这些公司花费大量时间和金钱去做预训练语料库，估计比让恒大还清债务还难。

当初微软为了帮助OpenAI训练，在2020年给OpenAI搭建了一个有285000核CPU和10000个V100 GPU的超算环境。

Q：微软调度了1万张卡给OpenAI做训练，商汤的反馈是我们国内最多也就调动2000张卡，这个技术有难度吗？

A：我目前了解到这一块国内目前没有哪个达到1万张量级的，不可能的，现在没有这水平。现在的确从百度视角来看，现在几千张卡在测试就不错了。从技术积累来说，涉及到集群做调度还是有一些协同性的难度。我认为包括商汤、百度，离微软的差距还是短时间内无法追上的。

国内先搞一波小参数的大模型，PR一定要cover机器之心、新智元、量子位，然后宣称自己的130b模型超越了gpt4，并在自己的榜上发布测评结果，成功超越gpt4。最后一堆商业公司来买130b的模型，这就算是创业成功了，毕竟一套价格不菲，几千万。

国内的研究者总是发布达到chatgpt4 106%能力的工作，人家一更换测评数据集就泯然众人矣了。

![](/images/img5/LLM.webp)

---

很多中间任务本身是为了服务下游高级任务，被拆解出来的，结果ChatGPT直接解决高级任务，中间任务自然也就不需要了。过去那一套，POS、句法树、依存都不用做了，现在都没看太多人提了，甚至乔姆斯基体系最近又被喷了。

中间层还不光是中间任务，还包括LLM中间训练部分的，比如模型结构、损失函数、优化器啊。

有段时间，各种魔改Transformer、优化器、损失函数。但到现在常用模型结构和Attention Is All Your Need中也没差太多，小改了LN、activation，而优化器则主要就修复了Adam实现的bug，成了AdamW. 结构方面出于推理效率考量的用一用MQA和GQA。

https://mp.weixin.qq.com/s/vfsB5t3r5dBACKQx6FshVw

选择你的道路：LLM 时代指南

---

参考：

https://www.zhihu.com/question/584132646

中国的大语言模型“悟道2.0”参数是GPT-3十倍，是否中国在大语言模型训练技术上已经远远超过美国？

https://zhuanlan.zhihu.com/p/463352552

稀疏性在机器学习中的发展趋势——Sparsity，稀疏激活，高效计算，MoE，稀疏注意力机制

https://zhuanlan.zhihu.com/p/254821426

乘风破浪的PTM：两年来预训练模型的技术进展（2020年之前的主流技术）

https://zhuanlan.zhihu.com/p/597586623

通向AGI之路：大型语言模型（LLM）技术精要

https://mp.weixin.qq.com/s/eV_9Mi2879w_gfoyiSm8Ug

LLM全景图（The Landscape of LLM）

https://www.zhihu.com/question/604592470

前两个月国产类ChatGPT大模型如雨后春笋，为何最近都没声音了?
