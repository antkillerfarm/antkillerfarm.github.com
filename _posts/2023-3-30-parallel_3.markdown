---
layout: post
title:  并行 & 框架 & 优化（四）——DeepSpeed, LoRA, FSDP, Megatron-LM, KV Cache
category: DL acceleration 
---

* toc
{:toc}

# DeepSpeed

显存内容可分为两类：

- 模型状态（model states）: 模型参数（fp16）、模型梯度（fp16）和Adam状态（fp32的模型参数备份，fp32的momentum和fp32的variance）。假设模型参数量为$$\Phi$$，则共需要$$2\Phi+2\Phi+(4\Phi+4\Phi+4\Phi)=16\Phi$$字节存储，可以看到，Adam状态占比75%。

![](/images/img5/ZeRO_offload.jpg)

- 剩余状态（residual states）: 除了模型状态之外的显存占用，包括激活值（activation）、各种临时缓冲区（buffer）以及无法使用的显存碎片（fragmentation）。

![](/images/img5/ZeRO.png)

ZeRO使用分片（partition）方法，即每张卡只存1/N的模型状态量。

![](/images/img5/ZeRO_2.png)

除了模型参数必须广播到各节点之外，模型梯度可以由各节点自己生成，然后再将之Reduce到保存数据的节点（即上图中的GPU 3）中。

只有模型梯度，由于来自不同的Data，需要各节点的同步，其他的Adam状态数据均与Data无关，由各节点自行计算更新即可。

代码：

https://github.com/microsoft/DeepSpeed

stage 1 & 2的代码在：

deepspeed/runtime/zero/stage_1_and_2.py

官网：

https://www.deepspeed.ai

DeepSpeed提供了PDSH、OpenMPI、MVAPICH三种方式在多机多卡模式下运行模型。

PDSH：Parallel Distributed Shell

参考：

https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters

ZeRO & DeepSpeed

https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/

DeepSpeed: Extreme-scale model training for everyone

https://mp.weixin.qq.com/s/Vb3AkoWHQY7WWBMZaVnf4g

微软发布DeepSpeed开源库，支持1000亿个参数模型的训练

https://zhuanlan.zhihu.com/p/621379646

人手一个ChatGPT！微软DeepSpeed Chat震撼发布，一键RLHF训练千亿级大模型

https://zhuanlan.zhihu.com/p/513571706

DeepSpeed之ZeRO系列：将显存优化进行到底

https://zhuanlan.zhihu.com/p/630734624

deepspeed入门教程

https://zhuanlan.zhihu.com/p/576673548

深度学习大模型训练--DeepSpeed源码解读

https://zhuanlan.zhihu.com/p/626420999

deepspeed chat代码解读

# LoRA

LoRA: Low-Rank Adaptation of Large Language Models是微软研究院引入的一项新技术，主要用于处理大模型微调的问题。

lora模型可以简单理解为在基础模型之上的一个补丁模型，用来训练特定风格、特定人物、特定动作等效果。因为基础模型提供了强大的通用能力，但对于指定人物、或者特定的一种风格掌握的并不精，所以需要lora模型来针对性学习下特定领域的效果。

![](/images/img5/LoRA.png)

$$h=xW_0'+xA'B'=W_0x+BAx$$

上图为LoRA的实现原理，其实现流程为：

- 在原始预训练语言模型旁边增加一个旁路，做降维再升维的操作来模拟内在秩；

- 用随机高斯分布初始化A，用零矩阵初始化B，训练时固定预训练模型的参数，只训练矩阵A与矩阵B；

- 推理的时候，只需要更新$$W=W_0+BA$$即可。

由于A和B的数据量极小，有的时候也会只发布A和B。用户将之下载之后，自行更新base model即可。这就相当于是model的patch。

由此还衍生出一种用法，一个base model + 若干针对不同领域/任务的patch，就可以服务于多个不同的领域/任务。

LoRA大幅降低了模型训练时的显存占用，因为并不优化主模型，所以主模型对应的优化器参数不需要存储。**但计算量没有明显变化**——虽然减少了主模型更新权重的计算，但却增加了旁路的正向和反向计算。

代码：

https://github.com/microsoft/LoRA

fine tune的示例：

examples/NLG/src/gpt2_ft.py

---

![](/images/img5/LLM_Adapter.png)

类似LORA这种，用较少参数量的fine-tuning，取代全参数量fine-tuning的方法，被称为Adapter，或者parameter-efficient fine-tuning (PEFT)。

论文：《LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models》

---

参考：

https://huggingface.co/datasets/HuggingFace-CN-community/translation/blob/main/lora_cn.md

使用LoRA进行Stable Diffusion的高效参数微调

https://zhuanlan.zhihu.com/p/631411685

微软LoRA：使用万分之一的参数微调你的GPT3模型

https://zhuanlan.zhihu.com/p/620327907

LoRA:大模型的低秩适配-最近大火的lora到底是什么东西？为啥stable diffusion和开源ChatGPT复现都在用？

https://zhuanlan.zhihu.com/p/639229126

深入浅出解析LoRA完整核心基础知识

https://mp.weixin.qq.com/s/nhIRulIRD6VrdPInDsaD8A

使用QLoRA对Llama 2进行微调的详细笔记

# FSDP

Fully Sharded Data Parallel是Facebook深度借鉴微软ZeRO之后提出的PyTorch DDP升级版本，可以认为是对标微软ZeRO，其本质是parameter sharding。Parameter sharding就是把模型参数等切分到各个GPU之上。

![](/images/img5/FSDP.png)

![](/images/img5/fsdp_sharding.png)

![](/images/img5/fsdp_workflow.png)

参考：

https://www.cnblogs.com/rossiXYZ/p/15815013.html

Facebook如何训练超大模型---(1)

https://www.cnblogs.com/rossiXYZ/p/15819817.html

Facebook如何训练超大模型---(2)

https://zhuanlan.zhihu.com/p/485208899

数据并行Deep-dive: 从DP到Fully Sharded Data Parallel（FSDP）完全分片数据并行

https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html

GETTING STARTED WITH FULLY SHARDED DATA PARALLEL(FSDP)

# Megatron-LM

Megatron是NVIDIA的研究小组。目前已经推出了三篇论文：

《Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism》

《Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM》

《Reducing Activation Recomputation in Large Transformer Models》

目前训练超大规模语言模型主要有两条技术路线：TPU + XLA + TensorFlow/JAX和GPU + PyTorch + Megatron-LM + DeepSpeed。前者由Google主导，后者背后则有NVIDIA、Meta、MS大厂加持。

![](/images/img5/Model_Parallel.jpg)

![](/images/img5/Model_Parallel.png)

代码：

https://github.com/NVIDIA/Megatron-LM

微软还有一个项目将DeepSpeed和Megatron-LM结合了起来：

https://github.com/microsoft/Megatron-DeepSpeed

参考：

https://zhuanlan.zhihu.com/p/522198082

Megatron-LM 第三篇Paper总结——Sequence Parallelism & Selective Checkpointing

https://zhuanlan.zhihu.com/p/366906920

Megatron论文和代码详细分析(1)

https://zhuanlan.zhihu.com/p/388830967

Megatron论文和代码详细分析(2)

https://blog.csdn.net/v_JULY_v/article/details/132462452

通俗理解Megatron-DeepSpeed之模型并行与数据并行

https://mp.weixin.qq.com/s/bvF50XRaA9cO2O4oB31kbg

大语言模型分布式训练的量化分析与最佳实践,以GPT-175B为例

# KV Cache

![](/images/img5/KV_Cache.png)

对于每个输入的prompt，在计算第一个token输出的时候，每个token的attention肯定是都要从头计算。但是在后续token的生成中，都需要计算self-attention，也就是输入prompt以及前面输出的token的attention。这是就需要用到前面每一个token的K和V，由于每一层的参数矩阵是不变的，此时只有刚生成的那个token的K和V需要从头计算，输入prompt和之前生成的token的K和V其实是跟上一轮一样的。

我们可以把每一层的K、V矩阵缓存起来，这就是所谓的KV Cache。

![](/images/img5/QKV_cache.png)

![](/images/img5/KV_Cache_2.png)

https://zhuanlan.zhihu.com/p/630832593

大模型推理性能优化之KV Cache解读

https://zhuanlan.zhihu.com/p/662498827

大模型推理加速：看图学KV Cache

# GSPMD

GSPMD是在GShard基础上发展而来的并行算法。

《GSPMD:General and Scalable Parallelization for ML Computation Graphs》

# Sequence Parallel

通用性较强的Ulysses，在小规模或长序列上表现更好。

《DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models》

FastSeq，它能将qkv projection的计算和all-gather通信重叠，只需多占用一点内存就可更进一步提升训练效率。

《FastSeq: Make Sequence Generation Faster》

# LLM Inference

https://zhuanlan.zhihu.com/p/653352979

LLM七种推理服务框架总结

https://zhuanlan.zhihu.com/p/671347964

大模型(LLM)推理框架汇总

## TensorRT-LLM

TensorRT-LLM是NVIDIA推出的基于TensorRT的LLM推理工具。

代码：

https://github.com/NVIDIA/TensorRT-LLM/

## vLLM

https://docs.vllm.ai/en/latest/

Easy, fast, and cheap LLM serving for everyone

# LLM实战经验

https://pytorch.org/blog/high-performance-llama-2/

https://huggingface.co/blog/zh/bloom-inference-optimization

https://huggingface.co/blog/zh/bloom-megatron-deepspeed

## LLAMA

代码：

https://github.com/facebookresearch/llama

量化的LLAMA模型：

https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ

参考：

https://github.com/jacoblee93/fully-local-pdf-chatbot

一个本地版本的阅读PDF的chatbot

## GEMMA

官网：

https://www.kaggle.com/models/google/gemma

# 工具

FairScale是由Facebook Research开发的PyTorch扩展库。FSDP就是首发于这个库。

---

https://zhuanlan.zhihu.com/p/412118353

Kokkos:一个异构并行计算通用平台

# 参考

https://mp.weixin.qq.com/s/F10UaaoxGPOE4pc59LBCRw

数据并行化对神经网络训练有何影响？谷歌大脑进行了实证研究

https://mp.weixin.qq.com/s/UF7DDenUQJ3bL83IHxOkIw

分布式优化算法及其在多智能体系统与机器学习中的应用

https://mp.weixin.qq.com/s/6h9MeBs89hTtWsYSZ4pZ5g

蚂蚁金服核心技术：百亿特征实时推荐算法揭秘

https://mp.weixin.qq.com/s/xV5cLbCPb7Nh6i4i7DxJIQ

没人告诉你的大规模部署AI高效流程！

https://mp.weixin.qq.com/s/8R7YhcZ_Dt0oFIF3bQovxw

为了提升DL模型性能，阿里工程师打造了流式编程框架

https://mp.weixin.qq.com/s/z6gXp-EeDID1ed8_DsUbOg

90秒训练AlexNet！商汤刷新纪录

https://mp.weixin.qq.com/s/HY2yPZ--Zm5_m3B70baWjQ

谷歌开源效率怪兽GPipe，速度提升25倍，CIFAR-10精度达到99%
