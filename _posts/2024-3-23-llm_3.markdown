---
layout: post
title:  Large Language Model（三）——LLAMA, Chinchilla Law
category: Attention 
---

* toc
{:toc}

# LLM大乱战（续）

https://mp.weixin.qq.com/s/oqhi58NcEVH1oVrW2p4xlQ

大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介

https://mp.weixin.qq.com/s/fUAUr9X3XLndfjga2QIHbA

大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning

https://mp.weixin.qq.com/s/f4l04f78F507JRrCawnV8w

大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2

https://mp.weixin.qq.com/s/nUAcCz6mcgGuUeuTfgqmOQ

大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体

https://mp.weixin.qq.com/s/N_N6RqKB9pjZ1tozfM5f5A

大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA

https://mp.weixin.qq.com/s/M2nds_FJBXooi08qDU-4yA

大模型参数高效微调技术原理综述（六）-MAM Adapter、UniPELT

https://mp.weixin.qq.com/s/P_AmTa4s8dOyc_0fZBgNPA

大模型参数高效微调技术原理综述（七）-最佳实践、总结

https://zhuanlan.zhihu.com/p/618695885

LLaMA, Alpaca, ColossalChat系列模型研究

https://zhuanlan.zhihu.com/p/638809556

大模型高效微调综述上：Adapter Tuning、AdaMix、PET、Prefix-Tuning、Prompt Tuning、P-tuning、P-tuning v2

https://zhuanlan.zhihu.com/p/651564985

主流大语言模型从预训练到微调的技术原理

# LLAMA

代码：

https://github.com/facebookresearch/llama

有用的LLAMA模型：

https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ

https://huggingface.co/Trelis/Llama-2-7b-chat-hf-sharded-bf16

Inference：

https://clay-atlas.com/blog/2023/09/21/huggingface-transformers-streaming/

使用HuggingFace Transformer中的TextStreamer和TextIteratorStreamer來實現串流式（stream）輸出生成token

各类开源LLM打榜：

https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

---

参考：

https://github.com/jacoblee93/fully-local-pdf-chatbot

一个本地版本的阅读PDF的chatbot

https://s-tm.cn/2023/06/10/SaveModelSharding/

使用HuggingFace transformers进行模型分片存储

## GEMMA

官网：

https://www.kaggle.com/models/google/gemma

## Mixtral

![](/images/img5/Mixtral.png)

https://zhuanlan.zhihu.com/p/676010571

欢迎Mixtral-当前Hugging Face上最先进的MoE模型

# Chinchilla Law

DeepMind研究了在给定算力预算下训练Transformer语言模型的最佳模型大小和tokens数量。

在LLaMA-1预训练时候，从各种开源数据集，凑够了1.4T的tokens。按照Chinchilla Law，应该使用1,400B (1.4T) tokens来训练参数量大小为70B的LLM最佳。即：每个参数需要大约20个文本token。

https://zhuanlan.zhihu.com/p/627821763

训练LLM需要多少数据？

https://www.zhihu.com/question/628395521

如何看待微软论文声称ChatGPT是20B(200亿)参数量的模型？

---

6B模型可以在在12/16/24G显存的消费级显卡部署和训练。如果一个公司的模型不打算在消费级显卡部署，通常不会训6B这个规模。而且通常还会有一个1.4b或者2.8b，这个是比较适合在手机、车载端量化部署的尺寸。

13B模型按照4k长度组织数据，数据并行=2，刚好占满一个8卡机，并且可以量化部署在A10甚至4090。

下一档也不是130B，目前更大模型有16B、34B、52B、56B、65B、70B、100B、130B、170B、220B这几个规模，基本都是刚好占满某种规格的算力，要么是训练要么是推理。如果需要加快训练速度，只需要倍增卡数即可。比如我们训7B模型以8卡为单位，8x8卡训，70B模型以80卡为单位，80x6卡训。

https://www.zhihu.com/question/627258986

现在LLM的大小为什都设计成6/7B、13B和130B几个档次？

# NVIDIA+

## cuLitho

光刻是芯片制造过程中最复杂、最昂贵、最关键的环节，其成本约占整个硅片加工成本的1/3甚至更多。计算光刻模拟了光通过光学元件并与光刻胶相互作用时的行为，应用逆物理算法来预测掩膜板上的图案，以便在晶圆上生成最终图案。

“计算光刻是芯片设计和制造领域中最大的计算工作负载，每年消耗数百亿CPU小时。”黄仁勋讲解道，“大型数据中心24x7全天候运行，以便创建用于光刻系统的掩膜板。这些数据中心是芯片制造商每年投资近2000亿美元的资本支出的一部分。”而cuLitho能够将计算光刻的速度提高到原来的40倍。

---

![](/images/img5/RL_EDA.png)

NV的EDA辅助设计

## profile

NVIDIA System Management Interface (NSMI): 即nvidia-smi。

NVIDIA Nsight Systems (NSYS)

NVIDIA Data Center GPU Manager (DCGM)

NVIDIA Management Library (NVML): NVML为GPU硬件数据提供了编程接口，开发者可以通过编程的方式访问GPU的各项数据，其中就包含GPU利用率，nvidia-smi和DCGM的背后就是NVML，推荐高级开发者使用。

https://fkong.tech/posts/2023-11-19-torch-gpu-util/

如何把PyTorch的GPU利用率提升到100%?

# 宋+

历史上的大理段氏，在五代后晋时期建立大理国，统治了云南地区318年（937年—1254年），随后投降蒙古军，成为蒙古汗国和元帝国的大理总管，又继续统治了今天云南中西部地区128年，于明初洪武十五年（1382年）被明军攻灭，作为我国云南地区统治者一共存在了446年。

https://www.zhihu.com/answer/524642062

为什么金庸先生在《天龙八部》里会把大理段氏写成一个江湖味十足的皇权，难道历史上这个政权真的很江湖么？

---

三百户为谋克，十谋克为猛安。

---

陆秀夫等至大元军中，求称侄纳币，不从；称侄孙，不从。

南宋一边求和，一边杀使者，蒙元一边派使者，一边以战促和，战着战着一不留神，把宋就给灭了。

南宋求和，是上层路线，上层很清楚打不过，所以需要求和，划江而治，但求和是条件不能太过分，如果太过分了，就不接受。

南宋杀使者，就是接受不了求和的条件，又需要振奋中下层的士气，表明抗战到底的决心。不然中层将领和底层战士听到上层全是怕死鬼，那他们还有战斗意志吗？所以在求和不成的情况下，得杀使者。

元朝派使者谈和统，走的是中下层路线，表明了元朝接收南宋全境后，不会搞屠杀报复，会把南宋臣民当元朝臣民对待（这一点在伯颜南下后，履行了承诺），从而瓦解南宋的抵抗意志。

---

公元1114年，出河店之战，金军1200，辽军7000（号称10万），结果辽军大败。

公元1115年，达鲁古之战，金军具体兵力未知，辽军3万（号称27万），结果辽军又是大败。

公元1115年，护步达冈之战，金军2万，辽军10万（号称70万），结果辽军再次大败。

北宋西军与西夏打打停停几十年，虽然战斗力不弱，但也没有超出西夏军的实力。而辽军和西夏军在面对金军时，多次惨败，这就注定北宋西军在面对金军时，打不赢是正常现象，打赢了反倒不正常。

种师道第一时间进京，并在判断敌我差距后，向钦宗建议，宋军应严防死守，不与金军正面冲突，待金军粮草吃完，往回撤时，再在黄河一线予以歼灭。但是钦宗想速战速决，主战派的李纲也想速战速决，他们都认为种师道太保守。最后钦宗就否决了种师道的意见，听从了李纲的建议，半夜派军偷袭，结果被擅长打野战的金军打了个惨败。

事后，丢人丢大了的宋钦宗转而议和，金军在得到许诺后，便撤军北上。种师道一直在等这个机会，他劝宋钦宗乘金兵渡黄河时袭击，但钦宗这个时候又不敢出战了，他在金军渡过黄河后，便罢免了种师道的职务。在他看来，金兵已走，种师道也没用了。

然而宋钦宗这边刚冷落种师道，金军那边又起了幺蛾子。由于金兵围攻太原不退（另一路金军），宋钦宗见战事又起，于是就转而主战，将种师道官复原职，令他挑起大梁。

种师道一直认为，打野战是不行的，金兵各个都像兰博一样，士气太盛，宋军必须要靠守取胜。因而他便建议，应集中兵马，先设好河北防线，以防金兵再次渡过黄河。

但是，钦宗和主战派又与先前一样，都是记吃不记打的货，主张穷追猛打，认为修防御是示弱表现，一定要跟金军打野战。因而种师道就再次被朝野批判，被骂作是怂包。

种师中战死后，种师道算是看清了主战派的那帮嘴脸。他根据实际情况，力劝宋钦宗马上西迁长安，依靠潼关天险躲避敌锋。但是这一次他又被大臣们群喷，被骂作是畏敌。

结果，后来金军的攻势果然无法抵挡，宋军输得稀里哗啦。宋钦宗看情况不对，就又罢了种师道的职务，同时还罢了李纲，派人向金国提出议和。种师道看到这种场景，再想到弟弟和种家军的惨死，悲愤交加，没多久就气死了。

曲端好比桂系或者具体点白崇禧吧，管军管民都有一手的，麾下的部曲战斗力在一众西军同僚衬托中鹤立鸡群。但是永远在打着自己的算盘，也永远别指望他算的对什么家国的大账，虽然抗击外敌不能说没有贡献，但站在他的友军角度觉得这人该死。

---

白沟之战，耶律大石对阵北宋第一名将种师道，耶律大石部起初只有两千人，种师道因杨可世新败，不敢渡河出击。次日，郭药师率常胜军赶到，辽军渡河发起攻击，种师道大军全面崩溃，被人一直追杀到雄城下。

https://www.zhihu.com/question/524124918

北宋拿回了几代人心心念念的幽云十六州，为什么那里的汉人反而日子过得糟糕了？

https://www.zhihu.com/question/313988961

北宋的种家军为何没有杨家将有名？

https://www.zhihu.com/question/522010669

金军为什么那么轻松不费吹灰之力就打到东京城下?
