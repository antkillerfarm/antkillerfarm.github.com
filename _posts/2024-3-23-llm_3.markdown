---
layout: post
title:  Large Language Model（三）——LLAMA, Chinchilla Law, MoE, LLM实战
category: Attention 
---

* toc
{:toc}

# LLM大乱战（续）

参考：

https://www.zhihu.com/question/584132646

中国的大语言模型“悟道2.0”参数是GPT-3十倍，是否中国在大语言模型训练技术上已经远远超过美国？

https://zhuanlan.zhihu.com/p/463352552

稀疏性在机器学习中的发展趋势——Sparsity，稀疏激活，高效计算，MoE，稀疏注意力机制

https://zhuanlan.zhihu.com/p/254821426

乘风破浪的PTM：两年来预训练模型的技术进展（2020年之前的主流技术）

https://zhuanlan.zhihu.com/p/597586623

通向AGI之路：大型语言模型（LLM）技术精要

https://mp.weixin.qq.com/s/eV_9Mi2879w_gfoyiSm8Ug

LLM全景图（The Landscape of LLM）

https://www.zhihu.com/question/604592470

前两个月国产类ChatGPT大模型如雨后春笋，为何最近都没声音了?

https://mp.weixin.qq.com/s/oqhi58NcEVH1oVrW2p4xlQ

大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介

https://mp.weixin.qq.com/s/fUAUr9X3XLndfjga2QIHbA

大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning

https://mp.weixin.qq.com/s/f4l04f78F507JRrCawnV8w

大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2

https://mp.weixin.qq.com/s/nUAcCz6mcgGuUeuTfgqmOQ

大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体

https://mp.weixin.qq.com/s/N_N6RqKB9pjZ1tozfM5f5A

大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA

https://mp.weixin.qq.com/s/M2nds_FJBXooi08qDU-4yA

大模型参数高效微调技术原理综述（六）-MAM Adapter、UniPELT

https://mp.weixin.qq.com/s/P_AmTa4s8dOyc_0fZBgNPA

大模型参数高效微调技术原理综述（七）-最佳实践、总结

https://zhuanlan.zhihu.com/p/618695885

LLaMA, Alpaca, ColossalChat系列模型研究

https://zhuanlan.zhihu.com/p/638809556

大模型高效微调综述上：Adapter Tuning、AdaMix、PET、Prefix-Tuning、Prompt Tuning、P-tuning、P-tuning v2

https://zhuanlan.zhihu.com/p/651564985

主流大语言模型从预训练到微调的技术原理

# LLAMA

![](/images/img6/transformer_vs_llama.svg)

![](/images/img6/llama.svg)

代码：

https://github.com/facebookresearch/llama

有用的LLAMA模型：

https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ

https://huggingface.co/Trelis/Llama-2-7b-chat-hf-sharded-bf16

Inference：

https://clay-atlas.com/blog/2023/09/21/huggingface-transformers-streaming/

使用HuggingFace Transformer中的TextStreamer和TextIteratorStreamer來實現串流式（stream）輸出生成token

各类开源LLM打榜：

https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

---

参考：

https://github.com/jacoblee93/fully-local-pdf-chatbot

一个本地版本的阅读PDF的chatbot

https://s-tm.cn/2023/06/10/SaveModelSharding/

使用HuggingFace transformers进行模型分片存储

https://www.zhihu.com/question/653373334

如何看待Meta发布Llama3，并将推出400B+版本？

https://mp.weixin.qq.com/s/VU_qUQEjqUNHd0UK6RLM9g

一文带你了解LLAMA（羊驼）系列

## GEMMA

官网：

https://www.kaggle.com/models/google/gemma

## Llama 3

llama 3是Meta于2024年发布的LLM。

论文：

《The Llama 3 Herd of Models》

本论文发布于2024.7，它实际上是Llama 3.1的技术报告。

![](/images/img5/LLAMA3.png)

https://blog.csdn.net/v_JULY_v/article/details/140659420

一文速览Llama 3.1——对其92页paper的全面细致解读：涵盖语言、视觉、语音的架构、原理

# LLM应用

![](/images/img6/LLM_IC.png)

大语言模型参与的芯片前端设计迭代流程

# Chinchilla Law

DeepMind研究了在给定算力预算下训练Transformer语言模型的最佳模型大小和tokens数量。

在LLaMA-1预训练时候，从各种开源数据集，凑够了1.4T的tokens。按照Chinchilla Law，应该使用1,400B (1.4T) tokens来训练参数量大小为70B的LLM最佳。即：每个参数需要大约20个文本token。

https://zhuanlan.zhihu.com/p/627821763

训练LLM需要多少数据？

https://www.zhihu.com/question/628395521

如何看待微软论文声称ChatGPT是20B(200亿)参数量的模型？

---

6B模型可以在在12/16/24G显存的消费级显卡部署和训练。如果一个公司的模型不打算在消费级显卡部署，通常不会训6B这个规模。而且通常还会有一个1.4b或者2.8b，这个是比较适合在手机、车载端量化部署的尺寸。

13B模型按照4k长度组织数据，数据并行=2，刚好占满一个8卡机，并且可以量化部署在A10甚至4090。

下一档也不是130B，目前更大模型有16B、34B、52B、56B、65B、70B、100B、130B、170B、220B这几个规模，基本都是刚好占满某种规格的算力，要么是训练要么是推理。如果需要加快训练速度，只需要倍增卡数即可。比如我们训7B模型以8卡为单位，8x8卡训，70B模型以80卡为单位，80x6卡训。

https://www.zhihu.com/question/627258986

现在LLM的大小为什都设计成6/7B、13B和130B几个档次？

---

Meta在LLaMA的观点是：给定模型的目标性能，并不需要用最优的计算效率在最快时间训练好模型，而应该在更大规模的数据上，训练一个相对更小模型，这样的模型在**推理阶段的成本更低**，尽管训练阶段的效率不是最优的（同样的算力其实能获得更优的模型，但是模型尺寸也会更大）。

https://zhuanlan.zhihu.com/p/667489780

解析大模型中的Scaling Law

---

当batch size较小时，更新方向（即对真实梯度的近似）会具有很高的方差，导致的梯度更新主要是噪声。

当batch size非常大时，我们从训练数据中抽样的任何两组数据都会非常相似（因为它们几乎完全匹配真实梯度）。

所以就有了Gradient Noise Scale Law。

https://zhuanlan.zhihu.com/p/666997679

大Batch训练LLM探索

# MoE

Mixture of Experts(MoE)

![](/images/img5/MoE.png)

![](/images/img6/switch_transformer.png)

---

![](/images/img6/MoE.png)

![](/images/img6/expert_parallel.png)

---

开源项目：

https://github.com/XueFuzhao/OpenMoE

https://github.com/laekov/fastmoe

---

https://mp.weixin.qq.com/s/XQSEg2_8_1lFqWdHVG6TVA

Switch Transformer: 高效稀疏的万亿参数Transformer

https://zhuanlan.zhihu.com/p/362525526

深入解读首个万亿级语言模型Switch Transformer

https://zhuanlan.zhihu.com/p/676980004

使用PyTorch实现混合专家(MoE)模型

https://huggingface.co/blog/zh/moe

混合专家模型(MoE)详解

https://zhuanlan.zhihu.com/p/662518387

MOE并行

https://zhuanlan.zhihu.com/p/1431483173

聊聊MoE实验性工作Upcycling Large Language Models into Mixture of Experts

https://zhuanlan.zhihu.com/p/672712751

大模型LLM之混合专家模型MoE（上-基础篇）

https://zhuanlan.zhihu.com/p/673048264

大模型LLM之混合专家模型MoE（下-实现篇）

## Mixtral

![](/images/img5/Mixtral.png)

https://zhuanlan.zhihu.com/p/676010571

欢迎Mixtral-当前Hugging Face上最先进的MoE模型

# VLM

https://zhuanlan.zhihu.com/p/702811733

Vision-Language Models (VLMs)多模态大模型一年多的进展与思考-2406

# LLM实战

https://pytorch.org/blog/high-performance-llama-2/

https://huggingface.co/blog/zh/bloom-inference-optimization

https://huggingface.co/blog/zh/bloom-megatron-deepspeed

---

![](/images/img5/Spike.webp)

训练过程中，损失偶尔会出现毛刺的情况。针对这种情况，Falcon作者会恢复到上一个最新的Checkpoint，并跳过1B Token数据继续训练。作者训练Falcon-180B时出现了9次毛刺。

Google训练PaLM模型遇到了同样的问题。针对此种情况，作者会重启训练，并从毛刺之前的100个step开始，跳过200-500个Batch的数据。作者也做了消融实验，发现并不是单个数据的问题，而可能是这连续的一系列Batch数据引起的。

https://mp.weixin.qq.com/s/rLJlaqI2RL7TGUEQyx-QaA

万卡GPU集群实战：探索LLM预训练的挑战

---

MFU（Model FLOPs Utilization）

$$\text{MFU} = \frac{\text{model FLOPs per iteration}}{\text{GPU单卡算力} \times \text{卡数} \times \text{一次迭代时间}}$$

Transformer模型的MFU计算公式：

$$\text{MFU} = \frac{72blsh^2 \left(1 + \frac{s}{6h} + \frac{V}{12hl}\right)}{F \times N \times T}$$

- b：批次大小（micro batch size）。
- l：Transformer 层数。
- s：序列长度。
- h：隐藏层维度。
- V：词表大小。
- F：GPU 单卡的峰值算力（FLOPS）。
- N：使用的 GPU 卡数量。
- T：一次迭代时间（秒）。

---

https://docs.swanlab.cn/zh/examples/pretrain_llm.html

从零预训练一个自己的大模型

本人的魔改版本，wiki_zh dataset + Qwen tokenizer + llama 2 model：

https://github.com/antkillerfarm/antkillerfarm_crazy/tree/master/python/ml/huggingface/llm_train.py

---

在大模型训练过程中，为确保训练的稳定性与有效性，需密切关注多项关键指标以评估训练状态，其中包括但不限于perplexity (PPL)、gradient norm (GNorm)、activation norm、内存占用情况以及Loss scale等参数。

梯度裁剪（Gradient Clipping）：作为一种常用的稳定训练手段，通常设定裁剪阈值为1.0，防止梯度过大引发训练不稳定。

Weight Decay（L2正则化）：设置合理的权重衰减率，如0.1，有助于防止过拟合，增强模型泛化能力。

特殊层的调整：GLM研究发现，embedding层往往存在较大的梯度异常情况，故需根据实际情况适度调整相关参数。

---

https://blog.csdn.net/v_JULY_v/article/details/132178447

七月论文审稿GPT第1版：通过3万多篇paper和10多万的review数据微调RWKV

https://blog.csdn.net/v_JULY_v/article/details/134183799

七月论文审稿GPT第2版：用一万多条paper-review数据微调LLaMA2 7B最终反超GPT4

https://blog.csdn.net/v_JULY_v/article/details/131552592

基于LangChain+LLM的本地知识库问答：从企业单文档问答到批量文档问答

https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5

这个网页收录了作者训练LLM时，各项指标的变化曲线。
