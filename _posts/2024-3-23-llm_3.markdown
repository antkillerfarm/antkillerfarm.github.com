---
layout: post
title:  Large Language Model（三）——LLAMA, Chinchilla Law
category: Attention 
---

* toc
{:toc}

# LLM大乱战（续）

参考：

https://www.zhihu.com/question/584132646

中国的大语言模型“悟道2.0”参数是GPT-3十倍，是否中国在大语言模型训练技术上已经远远超过美国？

https://zhuanlan.zhihu.com/p/463352552

稀疏性在机器学习中的发展趋势——Sparsity，稀疏激活，高效计算，MoE，稀疏注意力机制

https://zhuanlan.zhihu.com/p/254821426

乘风破浪的PTM：两年来预训练模型的技术进展（2020年之前的主流技术）

https://zhuanlan.zhihu.com/p/597586623

通向AGI之路：大型语言模型（LLM）技术精要

https://mp.weixin.qq.com/s/eV_9Mi2879w_gfoyiSm8Ug

LLM全景图（The Landscape of LLM）

https://www.zhihu.com/question/604592470

前两个月国产类ChatGPT大模型如雨后春笋，为何最近都没声音了?

https://mp.weixin.qq.com/s/oqhi58NcEVH1oVrW2p4xlQ

大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介

https://mp.weixin.qq.com/s/fUAUr9X3XLndfjga2QIHbA

大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning

https://mp.weixin.qq.com/s/f4l04f78F507JRrCawnV8w

大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2

https://mp.weixin.qq.com/s/nUAcCz6mcgGuUeuTfgqmOQ

大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体

https://mp.weixin.qq.com/s/N_N6RqKB9pjZ1tozfM5f5A

大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA

https://mp.weixin.qq.com/s/M2nds_FJBXooi08qDU-4yA

大模型参数高效微调技术原理综述（六）-MAM Adapter、UniPELT

https://mp.weixin.qq.com/s/P_AmTa4s8dOyc_0fZBgNPA

大模型参数高效微调技术原理综述（七）-最佳实践、总结

https://zhuanlan.zhihu.com/p/618695885

LLaMA, Alpaca, ColossalChat系列模型研究

https://zhuanlan.zhihu.com/p/638809556

大模型高效微调综述上：Adapter Tuning、AdaMix、PET、Prefix-Tuning、Prompt Tuning、P-tuning、P-tuning v2

https://zhuanlan.zhihu.com/p/651564985

主流大语言模型从预训练到微调的技术原理

# LLAMA

代码：

https://github.com/facebookresearch/llama

有用的LLAMA模型：

https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ

https://huggingface.co/Trelis/Llama-2-7b-chat-hf-sharded-bf16

Inference：

https://clay-atlas.com/blog/2023/09/21/huggingface-transformers-streaming/

使用HuggingFace Transformer中的TextStreamer和TextIteratorStreamer來實現串流式（stream）輸出生成token

各类开源LLM打榜：

https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

---

参考：

https://github.com/jacoblee93/fully-local-pdf-chatbot

一个本地版本的阅读PDF的chatbot

https://s-tm.cn/2023/06/10/SaveModelSharding/

使用HuggingFace transformers进行模型分片存储

https://www.zhihu.com/question/653373334

如何看待Meta发布Llama3，并将推出400B+版本？

https://mp.weixin.qq.com/s/VU_qUQEjqUNHd0UK6RLM9g

一文带你了解LLAMA（羊驼）系列

## GEMMA

官网：

https://www.kaggle.com/models/google/gemma

## Mixtral

![](/images/img5/Mixtral.png)

https://zhuanlan.zhihu.com/p/676010571

欢迎Mixtral-当前Hugging Face上最先进的MoE模型

## Llama 3

llama 3是Meta于2024年发布的LLM。

论文：

《The Llama 3 Herd of Models》

本论文发布于2024.7，它实际上是Llama 3.1的技术报告。

![](/images/img5/LLAMA3.png)

https://blog.csdn.net/v_JULY_v/article/details/140659420

一文速览Llama 3.1——对其92页paper的全面细致解读：涵盖语言、视觉、语音的架构、原理

# Chinchilla Law

DeepMind研究了在给定算力预算下训练Transformer语言模型的最佳模型大小和tokens数量。

在LLaMA-1预训练时候，从各种开源数据集，凑够了1.4T的tokens。按照Chinchilla Law，应该使用1,400B (1.4T) tokens来训练参数量大小为70B的LLM最佳。即：每个参数需要大约20个文本token。

https://zhuanlan.zhihu.com/p/627821763

训练LLM需要多少数据？

https://www.zhihu.com/question/628395521

如何看待微软论文声称ChatGPT是20B(200亿)参数量的模型？

---

6B模型可以在在12/16/24G显存的消费级显卡部署和训练。如果一个公司的模型不打算在消费级显卡部署，通常不会训6B这个规模。而且通常还会有一个1.4b或者2.8b，这个是比较适合在手机、车载端量化部署的尺寸。

13B模型按照4k长度组织数据，数据并行=2，刚好占满一个8卡机，并且可以量化部署在A10甚至4090。

下一档也不是130B，目前更大模型有16B、34B、52B、56B、65B、70B、100B、130B、170B、220B这几个规模，基本都是刚好占满某种规格的算力，要么是训练要么是推理。如果需要加快训练速度，只需要倍增卡数即可。比如我们训7B模型以8卡为单位，8x8卡训，70B模型以80卡为单位，80x6卡训。

https://www.zhihu.com/question/627258986

现在LLM的大小为什都设计成6/7B、13B和130B几个档次？

---

Meta在LLaMA的观点是：给定模型的目标性能，并不需要用最优的计算效率在最快时间训练好模型，而应该在更大规模的数据上，训练一个相对更小模型，这样的模型在**推理阶段的成本更低**，尽管训练阶段的效率不是最优的（同样的算力其实能获得更优的模型，但是模型尺寸也会更大）。

https://zhuanlan.zhihu.com/p/667489780

解析大模型中的Scaling Law

# VLM

https://zhuanlan.zhihu.com/p/702811733

Vision-Language Models (VLMs)多模态大模型一年多的进展与思考-2406

# Latex+

## 参考

https://zhuanlan.zhihu.com/p/508559139

Latex简明速查手册

https://www.zhihu.com/question/40763253

如何在Word和PowerPoint中优雅地插入Latex公式？

https://mp.weixin.qq.com/s/3RtKAT3WE8P2YIZRA2dZ3A

LaTeX的历史：图灵奖得主1977年开启的计划，引发学术圈重大变革

https://mp.weixin.qq.com/s/KMWhz5_EXWoxgmHUek6zBA

不会用latex写公式？看看这个python转latex的库（python2latex）

https://www.zhihu.com/question/65508676

怎么在LaTeX中排版Python代码？

https://mp.weixin.qq.com/s/LumDQL9VKTHWK3RkOhKFcg

1行代码搞定Latex公式编写，这个4.6M的Python小插件，堪称论文必备神器（handcalcs）

https://zhuanlan.zhihu.com/p/268332132

知乎公式编辑器的高级“玩法”

https://mp.weixin.qq.com/s/E4EECj0QQ90ElfPqOL80eg

Python编辑公式竟可以如此简单（latexify_py）

https://zhuanlan.zhihu.com/p/38178015

使用VSCode编写LaTeX

https://zhuanlan.zhihu.com/p/194831235

Knuth-Plass换行算法

https://mp.weixin.qq.com/s/kGVLsqstajyrZ_cfkFegrA

Latex数学排版简洁指南

# 东南亚+

1975年5月，红色高棉派兵入侵了越南的富国岛和土珠岛，向越南的安江省开炮，造成了大量的人员伤亡；

1977年4月，红色高棉派兵入侵了越南的安江、坚江等省份；9月，柬埔寨军队全面出击，进攻越南安江、西宁等边境省份，随后被越南人民军派兵驱离。

1977年12月，越南动员8个师约6万人的兵力入侵柬埔寨，一路攻抵金边城郊，将大量亲越民众带回越南，这是韩桑林、洪森组建部队的基础；

1978年4月，柬埔寨再次派兵入侵越南，屠杀越南边民数千人，制造了著名的巴祝事件。

1978年6月，中越关系彻底破裂，越南正式加入苏联阵营；1978年9月，越共决定彻底解决柬埔寨问题，开始动员军队；

1978年10月，越南开始公开集结军队，宋成飞抵北京求援，要求中国出兵参战；

1978年12月8日，北京下达战备命令，要求部队1979年1月10日前完成准备，进入战斗位置；

1978年12月25日，越南军队跨过边境，柬越战争爆发；

1978年12月31日，北京决定扩大作战规模，将出兵日期推迟到2月10日之后。

1979年1月6日，波尔布特等红色高棉高层逃离金边，次日，越南军队进入金边；至1979年1月12日，越南军队相继占领了磅同、暹粒、诗梳风、磅清扬、马德望、西哈努克港等主要城市，基本消灭了红色高棉政权；

1979年2月17日，对越自卫反击战爆发。

---

永历殉国后，曾追随他的南明将领四散奔逃，其中一名来自南京的武举人——杨高学的后代成为了果敢土司，统治该地直到1960年代。

奈温军政府于1962年政变上台后，采取武力手段将土司杨振材等杨家要人逮捕，并安了个贩毒罪名投入监狱。

原来作为杨家护卫队分队长的罗星汉投靠了缅甸军政府，被奈温委任为“果敢青年前进委员会”主席及自卫队队长。

同样在杨氏土司家族任自卫队分队长的彭家声以反对“大缅族主义”为名举起反政府大旗，成立“果敢人民革命军”武装反抗缅政府军和罗星汉部，并于1968年占领果敢。

2009年，缅甸军政府以搜查毒品和枪械厂为由发起对果敢同盟军的进攻，彭家声部溃败。同盟军副司令白所成和果敢县长明学昌率众投靠缅政府。

盘踞在缅北果敢的五大家族：白所成家族、魏超仁家族、刘国玺家族、明学昌家族和刘阿宝家族。

https://www.163.com/dy/article/IIU8UBSC0556135L.html

1020事件：四大家族造惨案，我国4名卧底亮身份被活埋，60人被杀

https://www.zhihu.com/question/630425466

缅北电诈重要头目明国平、明菊兰、明珍珍被成功抓获并移交中国警方，哪些信息值得关注？
