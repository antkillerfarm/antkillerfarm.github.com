---
layout: post
title:  浮点运算和代码优化, MPI
category: DL acceleration 
---

* toc
{:toc}

# 浮点运算和代码优化

## 浮点运算问题

浮点运算在工业中应用非常广泛，但嵌入式CPU通常没有对浮点运算提供直接的硬件支持。而采用标准库提供的软件计算方案，性能又很差。这时就需要使用浮点运算协处理器加速浮点运算。（486之前的PC，CPU和浮点运算协处理器FPU也是分开的，例如i486DX是有FPU的型号，而i486SX则是没有FPU的型号。）

硬件的支持离不开软件的使用。如果在添加了FPU的硬件上，使用浮点计算的软件方案的话，FPU也是不起作用的。因此必须用FPU驱动库函数替换标准库提供的软件方案的相应函数。

最直观的做法是将所有用到浮点计算的地方都替换成FPU函数。例如如下代码：

```c
float a,b,c;
a = b + c;
```

假设FPU加法函数的原型为：

`float Add(float a, float b);`

如果我们要使用FPU硬件加速的话，只需要将上述代码改为：

```c
float a,b,c;
a = Add(b,c);
```

就可以了。

上面的这种方法显然是直观而正确的，但是却不方便。需要将源代码中，所有涉及到浮点运算的地方都做相应的修改，而且以函数的方式取代C语言中的运算符，本身书写起来也很麻烦。

我们可以这样思考一下，C语言是如何将运算符转换成机器指令的呢？首先编译阶段肯定要做类型判断，整数加法和浮点数加法的指令显然不会相同。而链接阶段，只有符号表的概念，类型也好、运算符也好，都灰飞烟灭了。

因此，我们只要看一看浮点加法的汇编指令，就可以找到相关的符号表了。经查浮点加法对应的符号是__adddf3（gcc下）。因此它的原型就是：

`float64 __adddf3(float64 x, float64 y)`

将FPU函数写成这个样子，然后在链接阶段替换标准库函数就可以了。具体操作如下：

1.使用nm命令查看libgcc.a中的符号表，可以查到__adddf3在_addsub_df.o中。

2.使用`ar -d`从libgcc.a中去掉_addsub_df.o。

3.在链接时使用FPU函数库

## 立即数计算量的问题

请看以下代码：

```c
float64 a, b,c;
a = 2 * PI * (b - c) / 365.25;
```

机器执行这段代码会进行几次运算呢？

答案是3次。虽然有4个运算符，但2*PI是在编译阶段运算的。（这可以通过查看生成的汇编代码来验证。）基于同样的理由，我们还可以改进这段代码：

`a = 2*PI / 365.25 * (b - c);`

这样只需要2次运算了。

需要说明的是括号如果不改变运算的顺序的话，是不会改变计算次数的。因此

`a = (2*PI / 365.25) * (b - c);`

和

`a = 2*PI / 365.25 * (b - c);`

是等效的，都只需要2次运算。

## 3.冗余代码的问题

假设我们定义了a函数，但是在其他地方并未使用该函数，我们是否可以认为a函数的代码不会出现在最终的可执行文件中呢？

这个问题至少在gcc没有设置任何参数时，是否定的。不管a函数使用与否，它的代码都会包含在最终的可执行文件中。

对于PC来说，这不是个太大的问题，但对于嵌入式设备来说，任何空间的浪费都是不可接受的。

写到这里，有人会说，使用gcc的-o2选项优化代码，不就可以了吗？遗憾的是，这是不行的。

-o2有什么作用呢？还是上面的例子：

```c
void main()
{
  float64 a, b,c;
  a = 2 * PI * (b - c) / 365.25;
}
```

如果是-o2选项的话，机器会进行几次运算呢？

答案是0次。这种不涉及到输出结果的计算，直接被忽略掉了。

回到第一个问题。如何做才能在最终的可执行文件中不包含未使用的函数呢？步骤如下：

1.gcc添加-ffunction-sections选项。我们通常的做法是一个.c文件编译生成一个.o文件。而这个.c文件中的函数代码都会包含在.o文件的.text段（section）中。而-ffunction-sections选项会将每个函数放在单独的段中，例如a函数，会被放到.text.a段中。

2.ld添加--gc-sections选项。这个选项的作用是不链接未使用的段。

从上面的步骤可以看出，链接的最小单位既不是.o文件，也不是单个函数，而是段。

使用以上方法生成的程序，理论上没什么问题，但实际中，还是有不方便之处：为每一个函数生成一个段，可以想象可执行文件中会有多少段，而在某些平台上，代码在段之间的跳转是要比段内跳转消耗资源的。

我们可以在链接脚本中合并这些段，以下是一个简单的实例：

```bash
.text :
{
  . = ALIGN(4);
  text_start = .;
  *(.text.*)
  . = ALIGN(4);
  text_end = .;
}
```

## 参考

https://zhuanlan.zhihu.com/p/482979228

看完微软大神写的求平均值代码，我意识到自己还是too young了

# MPI

在《机器学习（三十三）》的Parameter Server一节，我们已经提到了Parameter Server这种参数中心化的分布式计算方案，但Parameter Server主要适用于稀疏矩阵的参数更新。对于多数的DL算法而言，采用**参数去中心化的集合通讯模式**是一种更有效率的做法。

---

集群可以很好解决单节点计算力不足的问题，但在集群中大规模的数据交换是很耗费时间的，因此需要一种在多节点的情况下能快速进行数据交流的标准，这就是Message passing interface(MPI)。

MPI的主流实现主要是：mpich，openmpi，Intel MPI，Microsoft MPI，其中Intel MPI和Microsoft MPI都是基于开源的mpich。

和MPI类似的技术，还有NVIDIA的NCCL，FaceBook的gloo等。

NVIDIA SHARP：Scalable Hierarchical Aggregation and Reduction Protocol

MPI官网：

http://www.mpi-forum.org/docs/

1994：MPI-1

1998：MPI-2

2012：MPI-3

2021：MPI-4

因为MPI的历史比较久远，深刻影响了后来的分布式并行程序，比如TF等。所以这里简单介绍一下几个关键的术语。

gloo官网：

https://github.com/facebookincubator/gloo

## Barrier

![](/images/img4/barrier.png)

这个方法会构建一个屏障，任何进程都没法跨越屏障，直到所有的进程都到达屏障。

实现屏障的方式有很多，最简单的是令牌环（token ring）。

第一个进程到达的屏障生成一个token，然后传递给下一个到达屏障的进程。所有的进程都到达屏障之后，token被传递回第一个进程。

`MPI_Barrier`在TF中被称为`AfterAll`。

## Bcast & Scatter

![](/images/img4/broadcastvsscatter.png)

`MPI_Bcast`给每个进程发送的是同样的数据，然而`MPI_Scatter`给每个进程发送的是一个数组的一部分数据。

![](/images/img4/broadcast_tree.png)

在上图的树形算法里，能够利用的网络连接每个阶段都会比前一阶段翻番，直到所有的进程接收到数据为止。

## Gather & Allgather

![](/images/img4/gather.png)

`MPI_Gather`跟`MPI_Scatter`是相反的。

![](/images/img4/allgather.png)

`MPI_Allgather`相当于一个`MPI_Gather`操作之后跟着一个`MPI_Bcast`操作。

## Reduce & Allreduce

![](/images/img4/mpi_reduce_1.png)

数据归约包括通过函数将一组数字归约为较小的一组数字。例如，假设我们有一个数字列表[1,2,3,4,5]。用sum函数归约此数字列表将产生`sum([1、2、3、4、5]) = 15`。

![](/images/img4/mpi_allreduce_1.png)

`MPI_Allreduce`等效于先执行`MPI_Reduce`，然后执行`MPI_Bcast`。

## Alltoall

![](/images/img4/all-to-all-collective.png)

`MPI_Alltoall`的工作方式是`MPI_Scatter`和`MPI_Gather`的组合。它的用途之一就是上图所示的数据的行列转置,但它比转置要灵活的多。

## groups

以上这些操作都涉及了多个计算节点。执行同一操作的多个节点组成一个group。

group里包含了相关的节点的id，如果group为null，则该操作会在整个计算集群上执行。两个group可以进行交集、并集之类的集合运算，生成新的group。

## 总结

```cpp
ScatterGather(SrcIDRange,SrcAddr,DstIDRange,DstAddr,UnitSize,TotalSize)
When there is only one source and UnitSize = TotalSize, it is a Broadcast
When there is only one source and UnitSize != TotalSize, it is a Scatter
When there is only one destination and UnitSize = TotalSize, it is a Gather
When UnitSize = TotalSize and there are multiple sources and destinations, it is a AllGather operation
When UnitSize != TotalSize and there are multiple sources and destinations, it is a All-to-All operation
```

从上面可以看出，除了Reduce一系的操作之外，其他的都可以总结为Scatter+Gather。

Scatter也被称为One-to-all，Gather也被称为All-to-one。

## AllReduce

AllReduce有多种具体的实现方式。

Ring AllReduce：

![](/images/img4/Ring_AllReduce.jpg)

Having-Doubling AllReduce：

![](/images/img4/Having-Doubling_AllReduce.jpg)

该算法每次选择节点距离倍增的节点相互通信，每次通信量倍减（或倍增）。

该算法的优点是通信步骤较少，只有$$2 * log_2N$$次（其中N表示参与通信的节点数）通信即可完成，所以其有更低的延迟。相比之下Ring算法的通信步骤是$$2 ∗ (N−1)$$次；缺点是每一个步骤相互通信的节点均不相同，链接来回切换会带来额外开销。

ring all-reduce具有理论上最优的传输带宽，而没有考虑每次传输都包含的延迟（latency）。当数据量V比较大时，延迟项可以忽略。当V特别小，或者设备数p特别大时，带宽就变得不重要了，反而是延迟比较关键。

这也是为什么英伟达NCCL里既实现了ring all-reduce，也实现了double-tree all-reduce算法。

https://www.zhihu.com/question/57799212

ring allreduce和tree allreduce的具体区别是什么？

https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/

Bringing HPC Techniques to Deep Learning

https://zhuanlan.zhihu.com/p/79030485

AllReduce算法的前世今生

https://mp.weixin.qq.com/s/4XMVYXnzpYZ4DrIabuTUig

Ring All-reduce: 分布式深度学习的巧妙同步

https://zhuanlan.zhihu.com/p/504957661

手把手推导Ring All-reduce的数学性质

https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/

Massively Scale Your Deep Learning Training with NCCL 2.4

https://zhuanlan.zhihu.com/p/611229620

NVIDIA的custom allreduce
