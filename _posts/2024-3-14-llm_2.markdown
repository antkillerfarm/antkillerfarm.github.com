---
layout: post
title:  Large Language Model（二）——AR vs AE, LLM大乱战
category: Attention 
---

* toc
{:toc}

# AR vs AE

自回归模型，是统计上一种处理时间序列的方法，用同一变数例如x的之前各期，亦即$$x_1$$至$$x_{t-1}$$来预测本期$$x_t$$的表现，并假设它们为一线性关系。因为这是从回归分析中的线性回归发展而来，只是不用x预测y，而是用x预测x自己，所以叫做自回归。

---

**AR**: Autoregressive Lanuage Modeling，又叫自回归语言模型。它指的是，依据前面(或后面)出现的tokens来预测当前时刻的token，代表模型有ELMO、GTP等。

$$\text{forward:}p(x)=\prod_{t=1}^Tp(x_t|x_{<t})$$

$$\text{backward:}p(x)=\prod_{t=T}^1p(x_t|x_{>t})$$

- 缺点：它只能利用单向语义而不能同时利用上下文信息。ELMO通过双向都做AR模型，然后进行拼接，但从结果来看，效果并不是太好。

- 优点：对自然语言生成任务(NLG)友好，天然符合生成式任务的生成过程。这也是为什么GPT能够编故事的原因。

**AE**:Autoencoding Language Modeling，又叫自编码语言模型。通过上下文信息来预测当前被mask的token，代表有BERT，Word2Vec(CBOW)。

$$p(x)=\prod_{x\in Mask}p(x_t|context)$$

- 缺点：由于训练中采用了MASK标记，导致预训练与微调阶段不一致的问题。此外对于生成式问题，AE模型也显得捉襟见肘，这也是目前为止，BERT为数不多没有实现大的突破的领域。

- 优点：能够很好的编码上下文语义信息，在自然语言理解(NLU)相关的下游任务上表现突出。

---

针对AR和AE的不同特点，又将前者称为Causal language models，后者称为Masked language models或者Prefix language models。

Causal：每个token只能看到在它之前的token信息，而看不到在它之后的token。

Masked：完形填空。

---

2023.3

ChatGPT的出现，为自然语言生成任务找到了商业化的路径。有鉴于此，Google也不得不在BERT上对AR模型，做了一些有损逼格的妥协。。。囧

![](/images/img5/T5.png)

deep encoder+shallow decoder

---

参考：

https://mp.weixin.qq.com/s/n6F6MTjrUCmvEoaLiVZpxA

更深的编码器+更浅的解码器=更快的自回归模型

https://mp.weixin.qq.com/s/pe2E69Gpw0nT9sSHvtBGSg

自回归与非自回归模型不可兼得？预训练模型BANG全都要！

https://www.zhihu.com/question/588325646

为什么现在的LLM都是Decoder only的架构？

https://www.zhihu.com/question/592545459

大模型都是基于Transformer堆叠，采用Encoder或者Decoder堆叠，有什么区别？

# LLM大乱战

![](/images/img5/LLM_Tree.png)

在LLM进化树中，BERT（Encoder-Only）、T5（Encoder-Decoder）、GPT（Decoder-Only）分别代表了不同的架构方向。那为什么在大模型时代，曾经风光无限的BERT家族和T5家族会逐渐没落了？

从纯算法模型结构上，Google的T5是比GPT更加优雅的神经网络模型结构， 但是由于T5的模型结构不是线性的，因为在Decoder和Encoder之间有复杂的连接关系（即对应的Cross Attention或者叫做 Cross Condition），导致T5在真正大规模堆叠的时候，实际上在工程领域，很难通过分布式并行高效的执行起来。因此，在目前已知的分布式并行优化上，T5很难通过规模化扩展模型的规模，Scale到千亿参数以上。这就是AI系统反过来影响算法发展，对算子作出的一种选择作用。

目前预训练大模型成本居高（ GPT-4 训练一次的成本超过 5000W 美金），再往上翻倍探索下一个FLOPs数量级也变得十分困难。因此百亿级别和千亿级别的 MoE 架构开始慢慢成为了大模型时代考虑的下一个主流方向.

---

![](/images/img5/LLM.png)

https://arthurchiao.art/blog/llm-practical-guide-zh/

大语言模型（LLM）综述与实用指南（Amazon，2023）

---

![](/images/img5/huge_model.jpg)

OpenAI的GPT 3的规模为175B，Google的LaMDA规模为137B，PaLM的规模为540B，DeepMind的Gogher规模为280B。

国内也有中文巨型模型，比如清华&智谱GLM规模130B，华为“盘古”规模200B，百度“文心”规模260B，浪潮“源1.0”规模245B。

然而，OpenAI的中小模型（如 6B~13B）的性能已经远超一众超大模型（如130B的GLM和175B的OPT）。由于实力的不对等，OpenAI、Google、DeepMind等LLM头部玩家可能不再会公开最前沿的LLM研究进展（转为挤牙膏模式）。

悟道2.0是一个基于MoE的稀疏模型，总参数量超过万亿，存一个checkpoint 20T。模型是神威上训练的，那玩意没有GPU或者CUDA，是神威自己的一套底层架构，因此适配起来非常困难。这个项目更像是一个国产硬件上训练巨大模型的尝试，项目启动之初就知道不可能投入到产品。

其次，稀疏模型的参数量和普通模型没有太大的可比性。

175B级别的训练的成本非常高。例如2020年GPT-3的单次训练成本约460万美元，总训练成本达1200万美元，如果推算到2023，国内单次训练成本约662-1170万RMB。以2022年为例，OpenAI运行成本为5.44亿美元，其中约有2亿多美元是工资/劳务费。

---

国内这一堆利润导向的“AI研究”公司，阿里的达摩院甚至还闹出过自负盈亏这种笑话，让这些公司花费大量时间和金钱去做预训练语料库，估计比让恒大还清债务还难。

当初微软为了帮助OpenAI训练，在2020年给OpenAI搭建了一个有285000核CPU和10000个V100 GPU的超算环境。

Q：微软调度了1万张卡给OpenAI做训练，商汤的反馈是我们国内最多也就调动2000张卡，这个技术有难度吗？

A：我目前了解到这一块国内目前没有哪个达到1万张量级的，不可能的，现在没有这水平。现在的确从百度视角来看，现在几千张卡在测试就不错了。从技术积累来说，涉及到集群做调度还是有一些协同性的难度。我认为包括商汤、百度，离微软的差距还是短时间内无法追上的。

国内先搞一波小参数的大模型，PR一定要cover机器之心、新智元、量子位，然后宣称自己的130b模型超越了gpt4，并在自己的榜上发布测评结果，成功超越gpt4。最后一堆商业公司来买130b的模型，这就算是创业成功了，毕竟一套价格不菲，几千万。

国内的研究者总是发布达到chatgpt4 106%能力的工作，人家一更换测评数据集就泯然众人矣了。

4月份业界就有传闻说字节想花上百万美金从OpenAI挖人，结果面试官被OpenAI反挖走了。

百度的数据团队也非常强，数据采集、数据清洗都是相当专业的。单是数据增强，一个月就花几千万的OpenAI API调用费用。

在初创公司已经发布的大模型中，只有Moonshot的模型水平超过了GPT-3.5。并没有直接照抄LLaMA的架构，而是做了很多工程上的优化。

![](/images/img5/LLM.webp)

https://www.zhihu.com/question/608763410

国内AI大模型已近80个，哪个最有前途？

---

很多中间任务本身是为了服务下游高级任务，被拆解出来的，结果ChatGPT直接解决高级任务，中间任务自然也就不需要了。过去那一套，POS、句法树、依存都不用做了，现在都没看太多人提了，甚至乔姆斯基体系最近又被喷了。

中间层还不光是中间任务，还包括LLM中间训练部分的，比如模型结构、损失函数、优化器啊。

有段时间，各种魔改Transformer、优化器、损失函数。但到现在常用模型结构和Attention Is All Your Need中也没差太多，小改了LN、activation，而优化器则主要就修复了Adam实现的bug，成了AdamW. 结构方面出于推理效率考量的用一用MQA和GQA。

https://mp.weixin.qq.com/s/vfsB5t3r5dBACKQx6FshVw

选择你的道路：LLM时代指南

---

![](/images/img5/LLM_pipeline.jpg)

---

数据泄露（data contamination）在大模型时代可以说是极难避免的。随着大模型训练数据规模的扩大，保证训练数据与常用benchmark之间没有重叠变得几乎不可能。

https://zhuanlan.zhihu.com/p/665426752

大模型是如何在评测中“作弊”的？

---

当年华为自己没有靠谱的团队，外包给别人训练了盘古。

现在当年训练盘古的人出来做自己的模型，这个模型就是KIMI。

去查下循环智能就知道了，盘古算是华为委托他们做的，ChatGPT火了之后，杨植麟把循环智能的人拉出来成立的moonshot。

此前就一直传闻张予彤在月之暗面的融资过程中“帮了不少忙”，收了不少融资费用，更有甚者直接点破，说kimi就是一个夫妻店。

中国模型4小龙：deepseek，qwen，豆包，kimi。

"大模型六小虎"（月之暗面、百川智能、智谱AI、零一万物、MiniMax、阶跃星辰）

“杭州六小龙”，指的是“游戏科学、深度求索、宇树科技、云深处科技、强脑科技和群核科技”这六家近期崛起于杭州，处于技术前沿、深具行业影响力的科企。

https://www.zhihu.com/question/654072307

如何看待月之暗面创始人杨植麟减股套现千万美金？

https://zhuanlan.zhihu.com/p/19272031682

DeepSeek创始人梁文锋

---

DeepSeek估值时，对标的是OpenAI，其估值区间被估算在10亿美元至1500亿美元之间。

根据“路边消息社”报道，现在想约见梁文锋，需要先报省委办公厅。一般人见不到梁文锋。网上还传出了“大年三十特警陪同回家，村口警察巡逻”的消息。
