---
layout: post
title:  Large Language Model（二）——AR vs AE, LLM大乱战
category: Attention 
---

* toc
{:toc}

# AR vs AE

自回归模型，是统计上一种处理时间序列的方法，用同一变数例如x的之前各期，亦即$$x_1$$至$$x_{t-1}$$来预测本期$$x_t$$的表现，并假设它们为一线性关系。因为这是从回归分析中的线性回归发展而来，只是不用x预测y，而是用x预测x自己，所以叫做自回归。

---

**AR**: Autoregressive Lanuage Modeling，又叫自回归语言模型。它指的是，依据前面(或后面)出现的tokens来预测当前时刻的token，代表模型有ELMO、GTP等。

$$\text{forward:}p(x)=\prod_{t=1}^Tp(x_t|x_{<t})$$

$$\text{backward:}p(x)=\prod_{t=T}^1p(x_t|x_{>t})$$

- 缺点：它只能利用单向语义而不能同时利用上下文信息。ELMO通过双向都做AR模型，然后进行拼接，但从结果来看，效果并不是太好。

- 优点：对自然语言生成任务(NLG)友好，天然符合生成式任务的生成过程。这也是为什么GPT能够编故事的原因。

**AE**:Autoencoding Language Modeling，又叫自编码语言模型。通过上下文信息来预测当前被mask的token，代表有BERT，Word2Vec(CBOW)。

$$p(x)=\prod_{x\in Mask}p(x_t|context)$$

- 缺点：由于训练中采用了MASK标记，导致预训练与微调阶段不一致的问题。此外对于生成式问题，AE模型也显得捉襟见肘，这也是目前为止，BERT为数不多没有实现大的突破的领域。

- 优点：能够很好的编码上下文语义信息，在自然语言理解(NLU)相关的下游任务上表现突出。

---

针对AR和AE的不同特点，又将前者称为Causal language models，后者称为Masked language models或者Prefix language models。

Causal：每个token只能看到在它之前的token信息，而看不到在它之后的token。

Masked：完形填空。

---

2023.3

ChatGPT的出现，为自然语言生成任务找到了商业化的路径。有鉴于此，Google也不得不在BERT上对AR模型，做了一些有损逼格的妥协。。。囧

![](/images/img5/T5.png)

deep encoder+shallow decoder

---

参考：

https://mp.weixin.qq.com/s/n6F6MTjrUCmvEoaLiVZpxA

更深的编码器+更浅的解码器=更快的自回归模型

https://mp.weixin.qq.com/s/pe2E69Gpw0nT9sSHvtBGSg

自回归与非自回归模型不可兼得？预训练模型BANG全都要！

https://www.zhihu.com/question/588325646

为什么现在的LLM都是Decoder only的架构？

https://www.zhihu.com/question/592545459

大模型都是基于Transformer堆叠，采用Encoder或者Decoder堆叠，有什么区别？

# LLM大乱战

![](/images/img5/LLM_Tree.png)

在LLM进化树中，BERT（Encoder-Only）、T5（Encoder-Decoder）、GPT（Decoder-Only）分别代表了不同的架构方向。那为什么在大模型时代，曾经风光无限的BERT家族和T5家族会逐渐没落了？

从纯算法模型结构上，Google的T5是比GPT更加优雅的神经网络模型结构， 但是由于T5的模型结构不是线性的，因为在Decoder和Encoder之间有复杂的连接关系（即对应的Cross Attention或者叫做 Cross Condition），导致T5在真正大规模堆叠的时候，实际上在工程领域，很难通过分布式并行高效的执行起来。因此，在目前已知的分布式并行优化上，T5很难通过规模化扩展模型的规模，Scale到千亿参数以上。这就是AI系统反过来影响算法发展，对算子作出的一种选择作用。

目前预训练大模型成本居高（ GPT-4 训练一次的成本超过 5000W 美金），再往上翻倍探索下一个FLOPs数量级也变得十分困难。因此百亿级别和千亿级别的 MoE 架构开始慢慢成为了大模型时代考虑的下一个主流方向.

---

![](/images/img5/LLM.png)

https://arthurchiao.art/blog/llm-practical-guide-zh/

大语言模型（LLM）综述与实用指南（Amazon，2023）

---

![](/images/img5/huge_model.jpg)

OpenAI的GPT 3的规模为175B，Google的LaMDA规模为137B，PaLM的规模为540B，DeepMind的Gogher规模为280B。

国内也有中文巨型模型，比如清华&智谱GLM规模130B，华为“盘古”规模200B，百度“文心”规模260B，浪潮“源1.0”规模245B。

然而，OpenAI的中小模型（如 6B~13B）的性能已经远超一众超大模型（如130B的GLM和175B的OPT）。由于实力的不对等，OpenAI、Google、DeepMind等LLM头部玩家可能不再会公开最前沿的LLM研究进展（转为挤牙膏模式）。

悟道2.0是一个基于MoE的稀疏模型，总参数量超过万亿，存一个checkpoint 20T。模型是神威上训练的，那玩意没有GPU或者CUDA，是神威自己的一套底层架构，因此适配起来非常困难。这个项目更像是一个国产硬件上训练巨大模型的尝试，项目启动之初就知道不可能投入到产品。

其次，稀疏模型的参数量和普通模型没有太大的可比性。

175B级别的训练的成本非常高。例如2020年GPT-3的单次训练成本约460万美元，总训练成本达1200万美元，如果推算到2023，国内单次训练成本约662-1170万RMB。以2022年为例，OpenAI运行成本为5.44亿美元，其中约有2亿多美元是工资/劳务费。

---

国内这一堆利润导向的“AI研究”公司，阿里的达摩院甚至还闹出过自负盈亏这种笑话，让这些公司花费大量时间和金钱去做预训练语料库，估计比让恒大还清债务还难。

当初微软为了帮助OpenAI训练，在2020年给OpenAI搭建了一个有285000核CPU和10000个V100 GPU的超算环境。

Q：微软调度了1万张卡给OpenAI做训练，商汤的反馈是我们国内最多也就调动2000张卡，这个技术有难度吗？

A：我目前了解到这一块国内目前没有哪个达到1万张量级的，不可能的，现在没有这水平。现在的确从百度视角来看，现在几千张卡在测试就不错了。从技术积累来说，涉及到集群做调度还是有一些协同性的难度。我认为包括商汤、百度，离微软的差距还是短时间内无法追上的。

国内先搞一波小参数的大模型，PR一定要cover机器之心、新智元、量子位，然后宣称自己的130b模型超越了gpt4，并在自己的榜上发布测评结果，成功超越gpt4。最后一堆商业公司来买130b的模型，这就算是创业成功了，毕竟一套价格不菲，几千万。

国内的研究者总是发布达到chatgpt4 106%能力的工作，人家一更换测评数据集就泯然众人矣了。

4月份业界就有传闻说字节想花上百万美金从OpenAI挖人，结果面试官被OpenAI反挖走了。

百度的数据团队也非常强，数据采集、数据清洗都是相当专业的。单是数据增强，一个月就花几千万的OpenAI API调用费用。

在初创公司已经发布的大模型中，只有Moonshot的模型水平超过了GPT-3.5。并没有直接照抄LLaMA的架构，而是做了很多工程上的优化。

![](/images/img5/LLM.webp)

https://www.zhihu.com/question/608763410

国内AI大模型已近80个，哪个最有前途？

---

很多中间任务本身是为了服务下游高级任务，被拆解出来的，结果ChatGPT直接解决高级任务，中间任务自然也就不需要了。过去那一套，POS、句法树、依存都不用做了，现在都没看太多人提了，甚至乔姆斯基体系最近又被喷了。

中间层还不光是中间任务，还包括LLM中间训练部分的，比如模型结构、损失函数、优化器啊。

有段时间，各种魔改Transformer、优化器、损失函数。但到现在常用模型结构和Attention Is All Your Need中也没差太多，小改了LN、activation，而优化器则主要就修复了Adam实现的bug，成了AdamW. 结构方面出于推理效率考量的用一用MQA和GQA。

https://mp.weixin.qq.com/s/vfsB5t3r5dBACKQx6FshVw

选择你的道路：LLM时代指南

---

![](/images/img5/LLM_pipeline.jpg)

---

最早的盘古应该是原语音语义实验室蒋欣老师和首席科学家刘群老师等带团队开发的，好像是mindspore太烂了，没训好。

后来唐睿明（原推荐搜索实验室主任）为了升职接姚老师，就去抢了蒋老师的位置，本来好像是应该尚立峰（现主任）来管。但是好像被唐打压了，一直不给机会参与大模型，甚至开会都不叫。唐不懂大模型，搞得更烂了，就那个底子上来就要训230b，据说ppt还有2.6T大模型。不过他跟上面关系好，还一直特别听话，只会逼著大家加班，每天晚上拉著汇报，工作日不准坐飞机。现在唐离职了，尚回来管，听那边的朋友说好了一些，但是还是天天攻关出方案训新的模型。

为了掩盖技术的无能，唐睿明选择了一种更原始、更残酷的方式：让团队成员通宵达旦地“人肉盯防”，一旦损失函数（loss）出现异常，立刻手动干预。就这样，靠着工程师的血肉之躯，硬是制造出了“0 loss震荡”的完美PPT，再次呈报给领导，作为其“技术领导力”的又一铁证。

当上大模型负责人后，唐睿明的野心进一步膨胀，将语音语义、决策推理等多个团队的成员尽数收入其“四纵”麾下，其实际负责人尚利峰等人被彻底架空，沦为有名无实的光杆司令。

王云鹤这人怎么说呢，年轻气盛，刚上来，领导支持就很强势，对大模型略懂，毕竟号称搞了什么小的大模型，内部都嘲笑这个属于华为专有名词。据说135b两波团队打得很凶，高压情况下，都看中了那块业务吧，套壳也是很有可能的，毕竟天天在心声上当演员。

王云鹤2018年博士毕业，经历不到7年时间，于2025年2月中旬，从原本的小模型实验室主任，正式顶替姚骏，被任命为诺亚方舟实验室主任（这一位置的前任包括大家耳熟能详的杨强、李航等，王云鹤可谓德不配位）。

王云鹤还主导引进了Fisher Yu这一劣迹斑斑、PUA女学生致死的所谓计算机科学家来诺亚当吉祥物。

华为在一次公开的大模型演示中，工作人员不小心按到了ctrl+c退出了python，当时程序刚好跑在`time.sleep(6)`这行代码下，直接报出错误，连带着`time.sleep(6)`这行代码一起返回来了。被人质疑是提前准备好了答案，`time.sleep(6)`用来混时间，假装模型在输出。

https://www.zhihu.com/question/1925157415541801408

如何看待“盘古之殇”一文爆料华为盘古大模型涉嫌“套壳、续训、洗水印”？

https://gitcode.com/ascend-tribe/ascend-cluster-infra/tree/main/HighAvailability

HW大规模集群技术报告

https://github.com/knemik97/Manifesto-against-the-Plagiarist-Yunhe-Wang

讨贼王云鹤檄文
