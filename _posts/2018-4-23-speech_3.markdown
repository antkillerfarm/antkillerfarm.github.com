---
layout: post
title:  语音识别（三）——声学模型, 解码器技术
category: speech 
---

# 声源定位（续）

## 波束形成

声源定位的方法包括**波束形成，超分辨谱估计和TDOA**，分别将声源和阵列之间的关系转变为**空间波束，空间谱和到达时间差**，并通过相应的信息进行定位。

波束形成是通用的信号处理方法，这里是指将一定几何结构排列的麦克风阵列的各麦克风输出信号经过处理（例如加权、时延、求和等）形成空间指向性的方法。波束形成主要是抑制主瓣以外的声音干扰，这里也包括人声，比如几个人围绕Echo谈话的时候，Echo只会识别其中一个人的声音。

波束形成可分为常规的波束形成CBF（Conventional Beam Forming）、CBF+Adaptive Filter和自适应波束形成ABF（Adaptive Beam Forming）。

## 超分辨谱估计

如MUSIC，ESPRIT等，对其协方差矩阵（相关矩阵）进行特征分解，构造空间谱，关于方向的频谱，谱峰对应的方向即为声源方向。适合多个声源的情况，且声源的分辨率与阵列尺寸无关，突破了物理限制，因此成为超分辨谱方案。这类方法可以拓展到宽带处理，但是对误差十分敏感，如麦克风单体误差，通道误差，适合远场模型，矩阵运算量巨大。

## TDOA

TDOA（time difference of arrival）是先后估计声源到达不同麦克风的时延差，通过时延来计算距离差，再利用距离差和麦克风阵列的空间几何位置来确定声源的位置。分为TDOA估计和TDOA定位两步：

### TDOA估计

常用的有广义互相关GCC（Generalized Cross Correlation）和LMS自适应滤波。

### TDOA定位

TDOA估值进行声源定位，三颗麦克风阵列可以确定空间声源位置，增加麦克风会增高数据精度。定位的方法有MLE最大似然估计，最小方差，球形差值和线性相交等。

TDOA相对来讲应用广泛，定位精度高，且计算量最小，实时性好，可用于实时跟踪，在目前大部分的智能定位产品中均采用TDOA技术做为定位技术。

## 参考

https://wenku.baidu.com/view/903f907f31b765ce05081431.html

基于传声器阵列的声源定位

https://zhuanlan.zhihu.com/p/35590325

MIT提出像素级声源定位系统PixelPlayer：无监督地分离视频中的目标声源

https://zhuanlan.zhihu.com/p/27921878

揭秘武林绝学——“听声辨位”

# 其他前端问题

## 语音增强

语音增强是指当语音信号被各种各样的噪声(包括语音)干扰甚至淹没后，从含噪声的语音信号中提取出纯净语音的过程。

![](/images/img2/Speech_Enhancement.jpg)

## 去混响（Dereverberation）

一般我们听音乐时，希望有混响的效果，这是听觉上的一种享受。合适的混响会使得声音圆润动听、富有感染力。混响（Reverberation）现象指的是声波在室内传播时，要被墙壁、天花板、地板等障碍物形成反射声，并和直达声形成叠加，这种现象称为混响。

但是，混响现象对于识别就没有什么好处了。由于混响则会使得不同步的语音相互叠加，带来了音素的交叠掩蔽效应（Phoneme Overlap Effect），从而严重影响语音识别效果。

影响语音识别的部分一般是晚期混响部分，所以去混响的主要工作重点是放在如何去除晚期混响上面，多年来，去混响技术抑制是业界研究的热点和难点。利用麦克风阵列去混响的主要方法有以下几种：

(1)基于盲语音增强的方法（Blind signal enhancement approach），即将混响信号作为普通的加性噪声信号，在这个上面应用语音增强算法。

(2)基于波束形成的方法（Beamforming based approach），通过将多麦克风对收集的信号进行加权相加，在目标信号的方向形成一个拾音波束，同时衰减来自其他方向的反射声。

(3)基于逆滤波的方法（An inverse filtering approach），通过麦克风阵列估计房间的房间冲击响应（Room Impulse Response, RIR），设计重构滤波器来补偿来消除混响。

## 声源信号提取

家里人说话太多，DingDong听谁的呢。这个时候就需要DingDong聪明的辨别出哪个声音才是指令。而麦克风阵列可以实现声源信号提取，声源信号的提取就是从多个声音信号中提取出目标信号，声源信号分离技术则是将需要将多个混合声音全部提取出来。

利用麦克风阵列做信号的提取和分离主要有以下几种方式：

(1)基于波束形成的方法，即通过向不同方向的声源分别形成拾音波束，并且抑制其他方向的声音，来进行语音提取或分离；

(2)基于传统的盲源信号分离（Blind Source Separation）的方法进行，主要包括主成分分析（Principal Component Analysis，PCA）和基于独立成分分析（Independent Component Analysis，ICA）的方法。

## 回声抵消

严格来说，这里不应该叫回声，应该叫“自噪声”。回声是混响的延伸概念，这两者的区别就是回声的时延更长。一般来说，超过100毫秒时延的混响，人类能够明显区分出，似乎一个声音同时出现了两次，我们就叫做回声，比如天坛著名的回声壁。

实际上，这里所指的是语音交互设备自己发出的声音，比如Echo音箱，当播放歌曲的时候若叫Alexa，这时候麦克风阵列实际上采集了正在播放的音乐和用户所叫的Alexa声音，显然语音识别无法识别这两类声音。回声抵消就是要去掉其中的音乐信息而只保留用户的人声，之所以叫回声抵消，只是延续大家的习惯而已，其实是不恰当的。

## 参考

https://zhuanlan.zhihu.com/p/27977550

极限元：智能语音前端处理中的几个关键问题

https://zhuanlan.zhihu.com/p/24139910

远场语音交互中的麦克风阵列技术解读

https://zhuanlan.zhihu.com/p/22512377

自然的语音交互——麦克风阵列

# 语言模型

语言模型是针对某种语言建立的概率模型，目的是建立一个能够描述给定词序列在语言中的出现的概率的分布。

给定下边两句话：

定义机器人时代的大脑引擎，让生活更便捷、更有趣、更安全。

代时人机器定义引擎的大脑，生活让更便捷，有趣更，安更全。

语言模型会告诉你，第一句话的概率更高，更像一句”人话”。

语言模型技术广泛应用于语音识别、OCR、机器翻译、输入法等产品上。语言模型建模过程中，包括词典、语料、模型选择，对产品的性能有至关重要的影响。Ngram模型是最常用的建模技术，采用了马尔科夫假设，目前广泛地应用于工业界。

>语言模型属于NLP的范畴，这里不再赘述。

参考：

https://zhuanlan.zhihu.com/p/23504402

语言模型技术

# 声学模型

声学模型主要有两个问题，分别是特征向量序列的可变长和音频信号的丰富变化性。

**可变长特征向量序列**问题在学术上通常有动态时间规划（Dynamic Time Warping, DTW）和隐马尔科夫模型（Hidden Markov Model, HMM）方法来解决。

**音频信号的丰富变化性**是由说话人的各种复杂特性或者说话风格与语速、环境噪声、信道干扰、方言差异等因素引起的。声学模型需要足够的鲁棒性来处理以上的情况。

在过去，主流的语音识别系统通常使用梅尔倒谱系数（Mel-Frequency Cepstral Coefficient, MFCC）或者线性感知预测（Perceptual Linear Prediction, PLP）作为特征，使用混合高斯模型-隐马尔科夫模型（GMM-HMM）作为声学模型。

在近些年，区分性模型，比如深度神经网络（Deep Neural Network, DNN）在对声学特征建模上表现出更好的效果。基于深度神经网络的声学模型，比如上下文相关的深度神经网络-隐马尔科夫模型（CD-DNN-HMM）在语音识别领域已经大幅度超越了过去的GMM-HMM模型。

参考：

https://zhuanlan.zhihu.com/p/23567981

声学模型

# 解码器技术

解码器模块主要完成的工作包括：给定输入特征序列$$x_1^T$$的情况下，在由声学模型、声学上下文、发音词典和语言模型等四种知识源组成的搜索空间（Search Space）中，通过维特比（Viterbi）搜索，寻找最佳词串$$[w_1^N]^{opt}=[w_1,\dots,w_N]_{opt}$$，使得满足：

$$[w_1^N]^{opt}=\mathop{\arg\max}_{w_1^N,N}p(w_1^N\mid x_1^T)$$

在解码过程中，各种解码器的具体实现可以是不同的。按搜索空间的构成方式来分，有动态编译和静态编译两种方式。

**静态编译**，是把所有知识源统一编译在一个状态网络中，在解码过程中，根据节点间的转移权重获得概率信息。由AT&T提出的Weighted Finite State Transducer（WFST）方法是一种有效编译搜索空间并消除冗余信息的方法。

**动态编译**，预先将发音词典编译成状态网络构成搜索空间，其他知识源在解码过程中根据活跃路径上携带的历史信息动态集成。

参考：

https://zhuanlan.zhihu.com/p/23648888

语音识别之解码器技术简介

# 人类声音

成年男性：80-140 Hz

成年女性：130-220 Hz

儿童：180-320 Hz

从信号处理的角度，人类声音的处理方式和普通的雷达信号处理并无本质差异，主要的区别在于：**雷达信号经过了载波调制，而人类声音则没有这个步骤。**

参考：

https://wenku.baidu.com/view/6123ba2f0066f5335a8121fe.html

人声频率范围及各频段音色效果

# 建模单元

建模单元是指声音建模的最小单元。从细到粗，一般有**state、phoneme、character**三级。

描述一种语言的基本单位被称为音素phoneme，例如BRYAN这个词就可以看做是由B, R, AY, AX, N五个音素构成的。这种模式也叫做单音素monophone模式。

然而语音没有图像识别那么简单，因为我们再说话的时候很多发音都是连在一起的，很难区分，所以一般用左中右三个HMM state来描述一个音素，也就是说BRYAN这个词中的R音素就变成了用B-R, R, R-AY三个HMM state来表示。这种模式又被称作三音素triphone模式。

character显然是个最粗的划分，尽管英语是表音文字，然而一个字母有多个发音，仍然是个普遍现象。

在GMM-HMM时代，人们倾向于细粒度建模，因为模型越细，效果越好。但DL时代，人们更倾向于粗粒度建模，因为这样做，可以加快语音识别的解码速度，从而可以使用更深、更复杂的神经网络建模声学模型。

