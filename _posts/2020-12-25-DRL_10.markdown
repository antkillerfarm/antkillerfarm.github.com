---
layout: post
title:  深度强化学习（十）——DRL参考资源（1）
category: DRL 
---

* toc
{:toc}

# RLHF（续）

## DPO

Direct Preference Optimization是一类不训练reward model而直接更新LM的方法。

![](/images/img5/DPO.png)

对于同一个propmt，给定一个好的回答和一个不好的回答，通过降低不好回答被采样的概率，提升好回答的概率，从而进行模型训练。

---

Token-level Direct Preference Optimization（TDPO）

Monolithic Preference Optimization without Reference Model（ORPO）

---

https://blog.csdn.net/v_JULY_v/article/details/134242910

RLHF的替代之DPO原理解析：从RLHF、Claude的RAILF到DPO、Zephyr

## Chain-of-Thought

思维链就是一系列中间的推理步骤(a series of intermediate reasoning steps)。通过让大模型逐步参与将一个复杂问题分解为一步一步的子问题，并依次进行求解的过程可以显著提升大模型的性能。

![](/images/img6/COT.png)

OpenAI o1 = COT + RL

![](/images/img6/PSRL.png)

![](/images/img6/R3.png)

每一轮推导，都把推导结果作为输入塞给下一轮作为输入，如此反复，相当于用一张巨大的草稿纸记录做题过程，每次演算都把推导过程写在草稿纸上，这样下次演算可以参考之前的演算过程，这样重复演算多次，每次演算的角度可以不一样，最终结果就非常综合而且全面，草稿纸越大，能写的演算过程越多，应对一些刁钻复杂问题，也就更容易得出准确答案。

https://blog.csdn.net/Julialove102123/article/details/135499567

一文读懂Chain of Thought，CoT思维链

https://zhuanlan.zhihu.com/p/721952915

Reverse-o1:OpenAI o1原理逆向工程图解

https://blog.csdn.net/v_JULY_v/article/details/142865592

一文通透OpenAI o1：从CoT、Quiet-STaR、Self-Correct、Self-play RL、MCTS等技术细节到工程复现

https://zhuanlan.zhihu.com/p/19969128139

MCTS树搜索会是复刻OpenAI O1/O3的有效方法吗

https://blog.csdn.net/v_JULY_v/article/details/145289228

一文速览火爆全球的推理模型DeepSeek R1：如何通过纯RL训练以比肩甚至超越OpenAI o1(含Kimi K1.5的解读)

https://zhuanlan.zhihu.com/p/20538667476

三张图速通DeepSeek-R1论文和技术原理

## Constitutional AI

奖励在RL中起着至关重要的作用，指导着优化过程。在可以通过外部工具进行验证的领域，如某些编码或数学场景中，RL显示出了出色的效果。然而，在更一般的场景中，通过硬编码构建反馈机制是不切实际的。

Constitutional AI是一种由Anthropic公司提出的理念和技术方法。简单来说就是，不是所有打标都要人搞，比如有害性标注就可以交给模型负责，然后我们可以在降低有害性，提高有用性两个维度上，做帕累托改进。

## TRL

TRL是huggingface的库，用于微调和调整大型语言模型，包括Transformer语言和扩散模型。这个库支持多种方法，如监督微调（Supervised Fine-tuning, SFT）、奖励建模（Reward Modeling, RM）、邻近策略优化（Proximal Policy Optimization, PPO）以及直接偏好优化（Direct Preference Optimization, DPO）。

代码：

https://github.com/huggingface/trl

参考：

https://zhuanlan.zhihu.com/p/693304721

RLHF：TRL - Transformers Reinforcement Learning使用教程

# DRL和Robotics

虽然，DRL似乎生来就是为了Robotics，然而现实中的无人系统，目前基本还是使用传统的控制方法。

例如：

https://www.zhihu.com/question/50050401

如何看待百度无人车， 三千多个场景，一万多个if？

不止国内，就连业界标杆Boston Dynamics也是这样：

https://www.zhihu.com/question/29871410

波士顿动力Boston Dynamics的大狗Big Dog用的了哪些控制、估计等算法？

直到最近才终于有了改观：

https://mp.weixin.qq.com/s/xSODAGf3QcJ3A9oq6xP11A

真的超越了波士顿动力！深度强化学习打造的ANYmal登上Science子刊

此外，还有一些论文：

《Learning to Drive in a Day》

---

强化学习和模仿学习，本质都是在任务层面的（Task-level Control），而传统的机器人控制都是在动作级（Action-level）和伺服级（Servo-level）。

https://www.zhihu.com/question/504142198

现有的控制器已经能完成对于机械臂的控制了，将深度强化学习应用到机械臂的控制上还有什么实际意义吗？

---

参考：

https://zhuanlan.zhihu.com/p/59197798

那些个端到端的自动驾驶研发系统

https://mp.weixin.qq.com/s/yU-6r-7l50y5msw8gUWnUQ

美团技术部解析：无人车端到端驾驶模型概述

# Model Predictive Control

MPC（Model Predictive Control）+WBC (Whole-Body Control)是典型的做腿足类机器人用的算法。

机械臂+灵巧手+移动小车，一般被称为AMR（Autonomous Mobile Robot）。

教程：

http://cse.lab.imtlucca.it/~bemporad/mpc_course.html

Model Predictive Control

https://www.syscop.de/teaching/ss2021/model-predictive-control-and-reinforcement-learning

Model Predictive Control and Reinforcement Learning

http://web.mit.edu/dimitrib/www/LessonsfromAlphazero.pdf

Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive Control

参考：

https://mp.weixin.qq.com/s/pMnbVWDDVgIdFpzkfSUd5Q

时延MPC自主车辆协同控制算法与仿真

https://mp.weixin.qq.com/s/UAPbNq_KNWCFd7p8U8HnYQ

模型预测控制（MPC）

https://zhuanlan.zhihu.com/p/99409532

模型预测控制简介（model predictive control）

https://zhuanlan.zhihu.com/p/507623074

Model Based + MPC + Planning + RL相关

https://mp.weixin.qq.com/s/BFNaEBa8KwvRBK_mSunXjw

MPC与LQR比较

https://www.zhihu.com/question/424186342

模型预测控制（MPC）和基于模型的强化学习（Model-based RL）之间的联系是什么？

# DRL参考资源

https://mp.weixin.qq.com/s/ruAAQEjPIDPWPiuPepKU6Q

Deep Reinforcement Learning: An Overview

https://mp.weixin.qq.com/s/FjmlXqlpaRVLdAbjqspOJA

这是一份你必须学习的强化学习算法清单

https://mp.weixin.qq.com/s/GISY-FvV1Vml3CNLInjgYg

Tensorflow2.0实现29种深度强化学习算法大汇总

https://mp.weixin.qq.com/s/lfXQqllfFPtuNIrQZiD-NQ

深度强化学习十大原则

https://mp.weixin.qq.com/s/I8IwPCY6-zocJKFXMr6rUg

深度强化学习的18个关键问题

https://mp.weixin.qq.com/s/_lmz0l1vP_CQ6p6DdFnHWA

谷歌大脑工程师的深度强化学习劝退文

https://zhuanlan.zhihu.com/p/39999667

强化学习路在何方？

https://zhuanlan.zhihu.com/p/25239682

深度强化学习（Deep Reinforcement Learning）入门

https://mp.weixin.qq.com/s/Ns5AUGfDoTeTwnBD-pYdLA

详解强化学习当前进展及未来方向

https://mp.weixin.qq.com/s/VIMX63zBaZ5ji70UmHHajg

强化学习入门知识超全梳理

https://mp.weixin.qq.com/s/e6QOz-MQSn7n53EPtpw64w

DeepMind强化学习综述：她可以很快，但快从慢中来

https://zhuanlan.zhihu.com/p/72642285

基于模型的强化学习论文合集

https://zhuanlan.zhihu.com/p/77976582

强化学习并行训练论文合集

https://mp.weixin.qq.com/s/sHP03DAeZvdBxwVPuSNRPg

如何丝滑地入门神经网络？写个AI赛车游戏，只训练4代就能安全驾驶

https://mp.weixin.qq.com/s/1SZKhG1ZbD1pl-3YQD_umg

6行代码搞定基本的RL算法，速度围观Reddit高赞帖

https://mp.weixin.qq.com/s/6n5HawyR4AgH8Dq0gJMw2g

强化学习的基本概念与代码实现

https://mp.weixin.qq.com/s/AI3i3ZLZ-fynavbeNAMKgA

强化学习应用介绍，41页报告带你快速了解RL的最新应用价值

https://mp.weixin.qq.com/s/-Cd7rQY3c3CINjtB99UZWw

自动驾驶领域中的强化学习，附18页论文下载

https://mp.weixin.qq.com/s/UBWN4_Sem8W1WcjsLAnjjQ

强化学习，路在何方？

https://book.yunzhan365.com/iths/nahn/mobile/index.html

强化学习和最优控制的《十个关键点》81页PPT汇总

https://zhuanlan.zhihu.com/p/107143378

Learning SR for Text Classification via RL

https://mp.weixin.qq.com/s/6c8imZFRAoxgs18rk_tlEw

炼丹感悟：On the Generalization of RL

https://mp.weixin.qq.com/s/PpBMu-5ozc9UOBbFhjsf3A

漫谈强化学习中的引导搜索策略（GPS方法）

https://mp.weixin.qq.com/s/TUk1PWT9CfPGEW77UKxpjw

三招武林绝学带你玩转“强化学习”

https://mp.weixin.qq.com/s/Uin1gOmJEa6cvkiFJm6cHw

你当年没玩好的《愤怒的小鸟》，AI现在也犯难了

https://mp.weixin.qq.com/s/YY1FIMjDIMABdwRC5x9w4g

17种深度强化学习算法用Pytorch实现

https://mp.weixin.qq.com/s/lxnyoo46tS7-MXAB7HNouA

人工智能在电网管理中的应用

https://mp.weixin.qq.com/s/ZYIfzk1kZ4azpmxWagkQuw

强化学习如何用于模型量化？

https://mp.weixin.qq.com/s/w4m4fG-a30AgulNPhBVC5w

基于强化学习的自动交易系统研究与发展综述

https://mp.weixin.qq.com/s/oyWyb58l4ztqIEyrk0g01A

强化学习在美团“猜你喜欢”的实践

https://mp.weixin.qq.com/s/oA98YyLqn1B22QZ5b_iDVA

IJCAI 2018：聚焦强化学习的学习效率

https://mp.weixin.qq.com/s/FtHJCXniVne2TGKfgCeS9w

Pieter Abbeel：深度强化学习加速方法

https://mp.weixin.qq.com/s/xlnwB9e1ks-4M4djnyIAyQ

解读72篇DeepMind深度强化学习论文

https://mp.weixin.qq.com/s/VYIyWRykREjOyLu4YDhLeA

61篇NIPS2019深度强化学习论文及部分解读

https://zhuanlan.zhihu.com/p/48186354

robot浅谈

https://mp.weixin.qq.com/s/f-rmdWq3kJUJGhuPwU8JtQ

深度策略梯度算法是真正的策略梯度算法吗？

https://mp.weixin.qq.com/s/SiVUmjHyw8ztiaSWNSZ-qA

多任务深度强化学习综述

https://zhuanlan.zhihu.com/p/51202503

强化学习在视觉上的应用（RL for computer Vision）

https://mp.weixin.qq.com/s/Dq4HsRg05bVMjvrsrxfOOQ

从这篇YouTube论文，剖析强化学习在工业级场景推荐系统中的应用

https://mp.weixin.qq.com/s/cyaQt-SO7sgG49kuUyPJbQ

旷视开源的深度强化学习绘画智能体论文解读

https://mp.weixin.qq.com/s/curzH8WrFyl5WWT4lDfpwQ

Chrome暗藏的恐龙跳一跳，已经被AI轻松掌握了

https://www.cnblogs.com/massquantity/p/13842139.html

Flink+强化学习搭建实时推荐系统

https://mp.weixin.qq.com/s/zfVrmErz3ZaGz7Ha7qGoIw

要提升微信看一看推荐混排的长期收益？试试深度强化学习

https://mp.weixin.qq.com/s/RLEuaiRdq6AbTSUcYQ5O3A

深度强化学习在NLP怎么用？看清华黄民烈老师这一份120页《自然语言处理和搜索中的深度强化学习应用》讲义
