---
layout: post
title:  深度强化学习（九）——王者荣耀, RLHF
category: DRL 
---

* toc
{:toc}

# 王者荣耀

腾讯绝悟也是DRL在MOBA游戏上的尝试。

https://mp.weixin.qq.com/s/ptgHNil1gqrH2p-x3UZoFw

紧急更新下降难度，《王者荣耀》绝悟AI难倒一片玩家

https://mp.weixin.qq.com/s/kAuzmc70NYAr9HmyioD5xA

腾讯绝悟AI完全体研究登上国际顶会与顶刊

https://mp.weixin.qq.com/s/8jkJy8ZarvpodmBccAc74A

中国AI足球队勇夺世界冠军，腾讯出品

https://mp.weixin.qq.com/s/ma2SgFuNYu50oexnfBYYLw

用AI打王者荣耀？认真谈谈强化学习的价值

https://mp.weixin.qq.com/s/IFGUXI3YXVsjIVuJ717uaw

用人工智能打王者荣耀：匹茨堡大学&腾讯AI Lab为游戏AI引入MCTS方法

https://mp.weixin.qq.com/s/Q3WWqEnUaIvnLwoFHY5lfQ

腾讯王者荣耀AI论文首次曝光：五AI王者局开黑与人类战队打成平手

https://mp.weixin.qq.com/s/Mdq9fL10Zcjs_z3PeqtN9Q

腾讯AI单挑碾压王者荣耀职业玩家：人类15场只能赢1局，坚持不到8分钟

https://mp.weixin.qq.com/s/_Ze8C8UQV9UZXnSX6qq2GA

强化学习玩王者荣耀

https://mp.weixin.qq.com/s/mj8PG-wRqSbEIR38fongHA

用自己训练的AI玩王者荣耀是什么体验？

# RLHF

## reward modeling

![](/images/img2/reward_modeling.jpg)

训练一个奖励模型，其中包含来自用户的反馈，从而捕捉他们的意图。与此同时，我们通过强化学习训练一个策略，使奖励模型的奖励最大化。换句话说，我们把学习做什么(奖励模型)和学习怎么做(策略)区分开来。

参考：

https://mp.weixin.qq.com/s/4yGQtHtMqWlaB7MAsr8T_g

DeepMind重磅论文：通过奖励模型，让AI按照人类意图行事

https://mp.weixin.qq.com/s/TIWnnCmVZnFQNH9Fig5aTw

DeepMind发布新奖励机制：让智能体不再“碰瓷”

https://zhuanlan.zhihu.com/p/337759337

BeBold：一种新的强化学习探索准则

## RLHF in ChatGPT

TAMER（Training an Agent Manually via Evaluative Reinforcement，评估式强化人工训练代理）框架将人类标记者引入到Agents的学习循环中，可以通过人类向Agents提供奖励反馈（即指导Agents进行训练），从而快速达到训练任务目标。

![](/images/img4/TAMER.jpg)

这算得上是一种有监督学习+RL了。

![](/images/img5/RLHF.png)

利用强化学习在大模型中注入人类的经验，所谓的Reinforcement Learning from Human Feedback(RLHF)，Policy Network输出的多样性及Reward的学习是ChatGPT成功的关键。

## RLHF

RLHF于2017年6月由OpenAI联合Google DeepMind推出。

论文：

《DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales》

![](/images/img5/RLHF_3.png)

- Step 1

将所有语料进行无监督训练，得到Pretrained Model，然后使用标注数据（如QA pair）进行Supervised Fine-Tuning，得到SFT Model。

- Step 2

编写QA pair毕竟比较费力，打分得到好/坏信息相对简单一些，同样也对Pretrained Model进行微调，得到Reward Model。

- Step 3

Actor/Critic Model在RLHF阶段是需要训练的；而Reward/Reference Model是参数冻结的。

Reference Model一般用SFT阶段得到的SFT模型做初始化，在训练过程中，它的参数是冻结的。Reference模型的主要作用是防止Actor”训歪”。

Critic Model一般用Reward Model初始化，Reward模型由于已经包含了人类提供的事实数据，因此它在这里是无需训练的。

![](/images/img5/RLHF_2.png)

![](/images/img5/rlhf.jpg)

---

ReMax认为，我们可以丢掉Critic（教练），Actor不再需要受到Critic的指导，而是直接去对齐Reference Model。

Group Relative Policy Optimization(GRPO)

这类方法效果一般，通常用于没有计算资源的时候Fine-Tuning之用。

![](/images/img6/GRPO.png)

https://blog.csdn.net/v_JULY_v/article/details/141535986

一文通透DeepSeek-V2(改造Transformer的中文模型)：详解MoE、GRPO、MLA

---

参考：

https://zhuanlan.zhihu.com/p/592671478

ChatGPT背后的算法——RLHF

https://zhuanlan.zhihu.com/p/612572103

RLHF的其他优化方向

https://zhuanlan.zhihu.com/p/608176805

如何看懂ChatGPT里的RLHF公式以及相关实现

https://zhuanlan.zhihu.com/p/644680366

LLaMA2 RLHF技术细节

https://zhuanlan.zhihu.com/p/645068532

大模型RLHF的trick

https://zhuanlan.zhihu.com/p/635569455

RLHF实践

https://zhuanlan.zhihu.com/p/667152180

一些RLHF的平替汇总（2023.11）

https://zhuanlan.zhihu.com/p/621456865

DeepSpeed-Chat开源了

https://brightliao.com/2023/05/25/chatgpt-rlhf/

ChatGPT的自动优化

https://www.zhihu.com/question/651021172

为什么需要RLHF？SFT不够吗？

https://blog.csdn.net/v_JULY_v/article/details/132939877

从零实现带RLHF的类ChatGPT：逐行解析微软DeepSpeed Chat的源码

https://blog.csdn.net/sinat_37574187/article/details/138200789

图解大模型RLHF系列之：人人都能看懂的PPO原理与源码解读

https://blog.csdn.net/v_JULY_v/article/details/128579457

ChatGPT技术原理解析：从RL之PPO算法、RLHF到GPT4、instructGPT

https://blog.csdn.net/v_JULY_v/article/details/129996493

详解带RLHF的类ChatGPT：从TRL、ChatLLaMA到ColossalChat、DSC

## DPO

Direct Preference Optimization是一类不训练reward model而直接更新LM的方法。

![](/images/img5/DPO.png)

对于同一个propmt，给定一个好的回答和一个不好的回答，通过降低不好回答被采样的概率，提升好回答的概率，从而进行模型训练。

---

Token-level Direct Preference Optimization（TDPO）

Monolithic Preference Optimization without Reference Model（ORPO）

---

https://blog.csdn.net/v_JULY_v/article/details/134242910

RLHF的替代之DPO原理解析：从RLHF、Claude的RAILF到DPO、Zephyr

# 常见问题建模方法

![](/images/img4/DRL.jpg)

https://zhuanlan.zhihu.com/p/356166466

强化学习落地：计算机系统

# DRL和Robotics

虽然，DRL似乎生来就是为了Robotics，然而现实中的无人系统，目前基本还是使用传统的控制方法。

例如：

https://www.zhihu.com/question/50050401

如何看待百度无人车， 三千多个场景，一万多个if？

不止国内，就连业界标杆Boston Dynamics也是这样：

https://www.zhihu.com/question/29871410

波士顿动力Boston Dynamics的大狗Big Dog用的了哪些控制、估计等算法？

直到最近才终于有了改观：

https://mp.weixin.qq.com/s/xSODAGf3QcJ3A9oq6xP11A

真的超越了波士顿动力！深度强化学习打造的ANYmal登上Science子刊

此外，还有一些论文：

《Learning to Drive in a Day》

---

强化学习和模仿学习，本质都是在任务层面的（Task-level Control），而传统的机器人控制都是在动作级（Action-level）和伺服级（Servo-level）。

https://www.zhihu.com/question/504142198

现有的控制器已经能完成对于机械臂的控制了，将深度强化学习应用到机械臂的控制上还有什么实际意义吗？

---

参考：

https://zhuanlan.zhihu.com/p/59197798

那些个端到端的自动驾驶研发系统

https://mp.weixin.qq.com/s/yU-6r-7l50y5msw8gUWnUQ

美团技术部解析：无人车端到端驾驶模型概述

# Model Predictive Control

MPC（Model Predictive Control）+WBC (Whole-Body Control)是典型的做腿足类机器人用的算法。

机械臂+灵巧手+移动小车，一般被称为AMR（Autonomous Mobile Robot）。

教程：

http://cse.lab.imtlucca.it/~bemporad/mpc_course.html

Model Predictive Control

https://www.syscop.de/teaching/ss2021/model-predictive-control-and-reinforcement-learning

Model Predictive Control and Reinforcement Learning

http://web.mit.edu/dimitrib/www/LessonsfromAlphazero.pdf

Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive Control

参考：

https://mp.weixin.qq.com/s/pMnbVWDDVgIdFpzkfSUd5Q

时延MPC自主车辆协同控制算法与仿真

https://mp.weixin.qq.com/s/UAPbNq_KNWCFd7p8U8HnYQ

模型预测控制（MPC）

https://zhuanlan.zhihu.com/p/99409532

模型预测控制简介（model predictive control）

https://zhuanlan.zhihu.com/p/507623074

Model Based + MPC + Planning + RL相关

https://mp.weixin.qq.com/s/BFNaEBa8KwvRBK_mSunXjw

MPC与LQR比较

https://www.zhihu.com/question/424186342

模型预测控制（MPC）和基于模型的强化学习（Model-based RL）之间的联系是什么？

# DRL参考资源

https://mp.weixin.qq.com/s/ruAAQEjPIDPWPiuPepKU6Q

Deep Reinforcement Learning: An Overview

https://mp.weixin.qq.com/s/FjmlXqlpaRVLdAbjqspOJA

这是一份你必须学习的强化学习算法清单

https://mp.weixin.qq.com/s/GISY-FvV1Vml3CNLInjgYg

Tensorflow2.0实现29种深度强化学习算法大汇总

https://mp.weixin.qq.com/s/lfXQqllfFPtuNIrQZiD-NQ

深度强化学习十大原则

https://mp.weixin.qq.com/s/I8IwPCY6-zocJKFXMr6rUg

深度强化学习的18个关键问题

https://mp.weixin.qq.com/s/_lmz0l1vP_CQ6p6DdFnHWA

谷歌大脑工程师的深度强化学习劝退文

https://zhuanlan.zhihu.com/p/39999667

强化学习路在何方？

https://zhuanlan.zhihu.com/p/25239682

深度强化学习（Deep Reinforcement Learning）入门

https://mp.weixin.qq.com/s/Ns5AUGfDoTeTwnBD-pYdLA

详解强化学习当前进展及未来方向

https://mp.weixin.qq.com/s/VIMX63zBaZ5ji70UmHHajg

强化学习入门知识超全梳理

https://mp.weixin.qq.com/s/e6QOz-MQSn7n53EPtpw64w

DeepMind强化学习综述：她可以很快，但快从慢中来

https://zhuanlan.zhihu.com/p/72642285

基于模型的强化学习论文合集

https://zhuanlan.zhihu.com/p/77976582

强化学习并行训练论文合集

https://mp.weixin.qq.com/s/sHP03DAeZvdBxwVPuSNRPg

如何丝滑地入门神经网络？写个AI赛车游戏，只训练4代就能安全驾驶

https://mp.weixin.qq.com/s/1SZKhG1ZbD1pl-3YQD_umg

6行代码搞定基本的RL算法，速度围观Reddit高赞帖

https://mp.weixin.qq.com/s/6n5HawyR4AgH8Dq0gJMw2g

强化学习的基本概念与代码实现

https://mp.weixin.qq.com/s/S2eGPTON3XmfN830m4vaaA

腾讯AI Lab：可自适应于不同环境和任务的强化学习方法

https://mp.weixin.qq.com/s/dlOFM7LuOF2npDP_EaITvg

效率提高50倍！谷歌提出从图像中学习世界的强化学习新方法（PlaNet）

https://mp.weixin.qq.com/s/3qWN6DjuGUZgCpxH7XF4fg

谷歌重磅开源RL智能体Dreamer，仅靠图像学习从机器人到Atari的控制策略，样本效率暴增20倍

https://mp.weixin.qq.com/s/Maoy2feVWj5hpn4Ysh_47A

你追踪，我逃跑：一种用于主动视觉跟踪的对抗博弈机制

https://mp.weixin.qq.com/s/4w3wIyy4H0UGeqOGhNcIbA

强化学习落地！京东等发布综述《深度强化学习在搜索，推荐和在线广告中的应用》

https://mp.weixin.qq.com/s/F_GsfAJRFJ_o8PETqUL35g

谷歌大脑AI飞速解锁雅达利，训练不用两小时：预测能力“前所未有”
