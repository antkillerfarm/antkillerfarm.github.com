---
layout: post
title:  语音识别（五）——Mel-Frequency Analysis, FBank, 语音识别的评价指标, 声学模型进阶
category: speech 
---

# Cepstrum Analysis（续）

这里，我们对Fourier transform做一个简单的回顾。

设h(t)是一个时域函数，而H(f)是一个频域函数，则Fourier transform为：

$$H(f)=\int_{-\infty}^\infty h(t)e^{2\pi i ft}\mathrm{d}t$$

inverse Fourier transformation为：

$$h(t)=\int_{-\infty}^\infty H(f)e^{-2\pi i ft}\mathrm{d}f$$

因此，对频谱做FT，也被叫做inverse FT，简称IFT。

>从上式还可以看出，FT和IFT的公式非常类似，因此从编程角度，一个FT函数既可以做FT，也可以稍作修改后，做IFT运算。因此在不强调目的性的情况下，IFT也可以直接称为FT。比如，MFCC特征最后的IDFT变换，实际上是DCT变换。

传统的IFFT的结果是一个时域函数，然而这里是对log frequency domain做IFFT，因此，它的值域只能被称作pseudo-frequency domain。

从上图可以看出，**Spectral Envelope主要是低频成分，而Spectral details主要是高频成分。**

![](/images/img2/Cepstral_5.png)

显然，如果把Spectral Envelope和Spectral details叠加起来就是原来的频谱信号了。

换句话说，我们知道了$$\log X[k]$$，就可以求出$$x[k]$$，经过低通滤波就可以得到$$h[k]$$。

这里的$$x[k]$$被称作倒谱Cepstrum（这个是一个新造出来的词，把spectrum的前面四个字母顺序倒过来就是倒谱的单词了）。

而我们所关心的$$h[k]$$就是倒谱的低频部分，它在语音识别中被广泛用于描述特征。

# Mel-Frequency Analysis

## Mel scale

Mel scale是Stevens、Volkmann和Newman于1937年发明的一种主观音阶标准。

>Stanley Smith Stevens，1906～1973，Harvard University心理学教授。

>John E. Volkmann，1905～1980，Radio Corporation of America研究员。

>Edwin B. Newman，1908~1989，Harvard University心理学教授。

声音作为一种波动，一般以Hz作为频率差异的客观标准，然而相同频率差的两组声音，在人耳听来，其频率差（也就是所谓的音阶）实际上是不同的。因此，Stevens等人采取实验的方法，确定了人耳的主观音阶标准。

该标准以Mel作为单位，规定1000Hz的声音所对应的音阶为1000Mel。

Mel scale从严格的定义上并没有一个简单的公式来表示。但一般采用如下公式进行转换：

$$m = 2595 \log_{10}\left(1 + \frac{f}{700}\right)$$

从中可以看出，**人耳对于高频声音的分辨率实际上是不如低频声音的**。

![](/images/img2/Mel_scale.png)

>Mel是melody的别称，有的blog上说Mel是个人，他发明了MFCC，这纯粹是胡说八道。

## MFCC

Mel-frequency Cepstral Coefficients是由Paul Mermelstein提出的一种音频特征。

>Paul G. Mermelstein，明尼苏达大学神经科学教授。

![](/images/img2/Mel_Filters.png)

由之前对Mel scale的介绍可知：人耳对于高频声音的分辨率实际上是不如低频声音的。

因此，我们可以使用一组Triangular window对声音进行滤波（如上图所示）。这里的Triangular window不是均匀分布的，而是低频部分更密集一些。

这些Triangular window被称作**Mel-Filters**。被Mel-Filters过滤之后的Spectrum，被称作**Mel-Spectrum**。

对Mel-Spectrum执行Cepstrum Analysis，就得到了Mel-Frequency Cepstral Coefficients，也就是MFCC。

![](/images/img2/MFCC.png)

上图是MFCC的计算流程。

除了MFCC之外，delta MFCC和double-delta MFCC也是常用的特征。他们的计算过程如下所示：

![](/images/img2/MFCC_2.jpg)

可见，delta MFCC和double-delta MFCC，实际上就是MFCC的一阶差分和二阶差分。

在实际中使用的语音特征，往往是各种特征的组合。比如，常用的**39维MFCC特征**，其组成如下：

12 MFCC feature

1 energy feature

12 delta MFCC features

12 double-delta MFCC features

1 delta energy feature

1 double-delta energy feature

## 计算能量谱

energy的计算比较简单，无论是如上图的时域能量统计，还是在DFT之后进行频域能量统计都是可以的。参见《数学狂想曲（一）》。

需要注意的是，频域能量包含了实部能量+虚部能量。

## Discrete Cosine Transform

离散傅立叶变换需要进行复数运算，尽管有FFT可以提高运算速度，但在图像编码、特别是在实时处理中非常不便。离散傅立叶变换在实际的图像通信系统中很少使用，但它具有理论的指导意义。

根据离散傅立叶变换的性质，实偶函数的傅立叶变换只含实的余弦项，因此构造了一种实数域的变换——离散余弦变换(DCT)。

通过研究发现，DCT除了具有一般的正交变换性质外，其变换阵的基向量很近似于Toeplitz矩阵的特征向量，后者体现了人类的语言、图像信号的相关特性。因此，在对语音、图像信号变换的确定的变换矩阵正交变换中，DCT变换被认为是一种准最佳变换。

相对应的还有IDCT。

DCT还有一个特点是，对于一般的语音信号，这一步的结果的前几个系数特别大，后面的系数比较小，可以忽略。比如Mel-Filters一般取40个三角形，所以DCT的结果也是40个点；实际中，一般仅保留前12~20个，这就进一步压缩了数据。

类似的，还有Discrete Sine Transform，它和DCT的区别在于：DST用于实奇对称数据，而DCT用于实偶对称数据。这里的对称指的是采样对称，而非物理数值上的对称。

除此之外，针对人耳的听觉特性，还有Constant-Q transform。它与STFT的公式基本相同，差别在于后者的filter的中心频点间隔均匀，而前者的间隔越往高频越稀疏：

$$\delta f_k = 2^{1/n} \cdot \delta f_{k-1} = \left( 2^{1/n} \right)^k \cdot \delta f_\text{min}$$

上式中的$$f_k$$即为filter的中心频点。

## 参考

http://blog.csdn.net/zouxy09/article/details/9156785

梅尔频率倒谱系数（MFCC）

https://my.oschina.net/jamesju/blog/193343

语音特征参数MFCC提取过程详解

https://liuyanfeier.github.io/2017/10/26/2017-10-27-Kaldi%E4%B9%8Bfbank%E5%92%8Cmfcc%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/

kaldi之fbank和mfcc特征提取

https://zhuanlan.zhihu.com/p/26680599

语音信号预处理及特征参数提取

# FBank

Filter bank和MFCC的计算步骤基本一致，只是没有做IDFT而已。

FBank与MFCC对比：

1.计算量：MFCC是在FBank的基础上进行的，所以MFCC的计算量更大

2.特征区分度：FBank特征相关性较高（相邻滤波器组有重叠），MFCC具有更好的判别度，这也是在大多数语音识别论文中用的是MFCC，而不是FBank的原因

3.使用对角协方差矩阵的GMM由于忽略了不同特征维度的相关性，MFCC更适合用来做特征。

4.DNN/CNN可以更好的利用这些相关性，使用fbank特征可以更多地降低WER。

参考：

http://blog.csdn.net/wxb1553725576/article/details/78048546

Kaldi特征提取之-FBank

# Pitch Detection

http://blog.csdn.net/zouxy09/article/details/9141875

基音周期估计（Pitch Detection）

# Vector Quantization

http://blog.csdn.net/zouxy09/article/details/9153255

矢量量化（Vector Quantization）

# fMLLR

https://blog.csdn.net/xmdxcsj/article/details/78512645

声学特征变换fMLLR

# SGMM

https://blog.csdn.net/quhediegooo/article/details/68946100

子空间高斯混合模型-SGMM

# PLP

Perceptual Linear Prediction

《Perceptual Time Varying Linear Prediction Model for Speech Applications》

https://www.isip.piconepress.com/courses/msstate/ece_8463/lectures/current/lecture_17/index.html

SPECTRAL TRANSFORMATIONS

# VTLN

https://blog.csdn.net/jiangyangbo/article/details/6535928

VTLN(Vocal Tract Length Normalisation)

# HMM与语音识别

HMM的基本概念参见《机器学习（二十二）》，这里谈一下HMM在语音识别领域的应用。

从概率的角度来说，语音识别的目标是寻找最可能的$$P(W\mid O)$$。这里的W表示word，O表示observation。

直接找显然没这么容易，所以要用到Bayes公式：

$$\frac{P(W)P(O\mid W)}{P(O)}$$

这里只有$$P(O)$$已知，剩下的两个参数都需要额外提供。其中HMM提供$$P(O\mid W)$$，LM提供$$P(W)$$。

由于HMM的path上的概率是各个transition probability的乘积，而这些概率都小于1，因此他们的乘积必然是更小的数。这时可以考虑使用对数，不仅可将乘法变为加法，同时数值的范围也得到了改善。

# 语音识别的评价指标

语音识别的评价指标主要是Word Error Rate（WER）。

错误的情况包括三种：

1.Substitutions：错词。

2.Deletions：漏词。

3.Insertions：多词。

$$WER=100\%\times \frac{Subs+Dels+Ins}{\text{word in correct sentence}}$$

类似的还有CER/PER：Character/Phoneme Error Rate。

需要注意的是，评价WER时，需要在ASR output和Label之间进行对齐操作，而不是简单的从左往右匹配，否则将无法正确处理Deletions和Insertions的情形。

还有根据公式可知，WER是可以大于100%的。

参考：

https://blog.csdn.net/quhediegooo/article/details/56834417

语音识别评估标准-WER

# 声学模型进阶

## 语音质量

更高的采样率可以降低WER。一般来说，16KHz相比8KHz的WER要小10%左右。

## Voice Detection

长时间的silence会增加WER，因此我们需要判断当前是否在说话。

Voice Detection包括两个方面：

1.Beginning-Point Detection。也叫做Voice Activity Detection（VAD）。有些类似于唤醒检测，但并不局限于设备的开机时刻。

2.End-Point Detection。

参考：

https://zhuanlan.zhihu.com/p/24432663

Voice Active Detection(VAD)的过去时与现在时

https://blog.csdn.net/wxb1553725576/article/details/78069089

Kaldi特征提取之-VAD

https://blog.csdn.net/shichaog/article/details/78257068

VAD综述

## Feature normalization

有时候需要对Feature进行normalization。例如，对MFCC特征减去均值，可以有效提升在噪声环境下的识别率。

## Tri-phone Models

英语一般包含43个音素，因此Tri-phone共有$$43^3\approx 80K$$种不同组合。

但是这些组合的概率是众寡悬殊的，有些组合很常见，而有些组合很罕见。因此我们需要合并相似的发音组合。这通常采用CART决策树来进行聚类。这样做还可以减少状态数量，提高计算效率。

## 发音词典

Pronunciation Dict用于将文本转换成对应的发音。比较常用的有CMU的发音词典，用于美国英语，包含了100K的单词。用法参见《LSTM Speech Recognition实战》。

然而，无论多大的词典都会有遇到Unknown Words的情况。一般可根据现有发音构建统计模型，来预测发音。这也是符合人们的认知规律的：人遇到一个陌生的新词，也会根据过往的经验，来预测词的发音。通常这样做，会有70%～85%的准确率。

