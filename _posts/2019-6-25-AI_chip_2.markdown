---
layout: post
title:  AI Chip（二）
category: Chip 
---

* toc
{:toc}

# AI Chip

## TPU

​Tensor Processing Unit(TPU)是Google推出的AI芯片系列。目前已经有3个版本：

TPU v1, deployed 2015, 92 teraops, inference only.

TPU v2, cloud TPU 2017, pod 2018, 180 teraflops, 64 GB HBM, training and inference, generally available. 11.5 petaflops in a pod.

TPU v3, cloud beta 2018, 420 teraflops, 128 GB HBM, training and inference, beta. >100 petaflops in a pod.

论文：

《In-Datacenter Performance Analysis of a Tensor Processing Unit​》

《TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings》

![](/images/img4/TPU.png)

---

![](/images/img5/TPU.png)

https://www.zhihu.com/answer/2716117809

David Patterson在UCB的演讲，分享了Google TPU近十年的发展历程以及心得体会

## Coral Dev Board

Coral Dev Board是Google于2019年3月推出的一款搭载TPU的嵌入式开发板。

参考：

http://linuxgizmos.com/google-launches-i-mx8m-dev-board-with-edge-tpu-ai-chip/

Google launches i.MX8M dev board with Edge TPU AI chip

## 脉动阵列

**AI=矩阵乘+非线性**

由于非线性只有可怜的O(N)的计算量，矩阵乘的O(N^3)的计算量才是AI中的绝对计算瓶颈。

---

Systolic array是孔祥重和他的博士生Charles Leiserson于1978年发明的。

论文：

《Why systolic architectures?》

![](/images/img4/Systolic_array.gif)

>孔祥重（H. T. Kung/Kung, Hsiang-Tsung），1945年生。台湾国立清华大学本科（1968）+CMU博士（1974）。CMU、Harvard教授。台湾中央研究院院士、美国工程院院士。除了Systolic array之外，数据库领域的Optimistic concurrency control（乐观并发控制）也是他的贡献。   
>除了Charles Eric Leiserson之外，他的博士生还有Robert Tappan Morris，也就是著名的Morris Worm的作者。

参考：

http://web.cecs.pdx.edu/~mperkows/temp/May22/0020.Matrix-multiplication-systolic.pdf

矩阵乘法器原理

http://www.eecs.harvard.edu/~htk/publication/1980-introduction-to-vlsi-systems-kung-leiserson.pdf

Algorithms for VLSI Processor Arrays

https://zhuanlan.zhihu.com/p/26522315

脉动阵列-因Google TPU获得新生

http://www.sohu.com/a/142237570_505803

我们应该拥抱“脉动阵列”吗？

https://zhuanlan.zhihu.com/p/26882794

Google深度揭秘TPU

---

参考：

https://mp.weixin.qq.com/s/1X9xiZkmVPI-j-aipr-ocg

AlphaGo Master最新架构和算法，谷歌云与TPU拆解

https://mp.weixin.qq.com/s/Yo0uKd1Mzy4mmS4r0mxfVw

有图有真相：深度拆解谷歌TPU3.0，新一代AI协同处理器

https://mp.weixin.qq.com/s/kPrZ0PuevXEJjVB7RXs70g

谷歌TPU率队，颠覆3350亿美元的半导体行业

https://mp.weixin.qq.com/s/wunqEHC6c-yUVXTl4yTG4w

仅需1/5成本：TPU是如何超越GPU，成为深度学习首选处理器的

https://mp.weixin.qq.com/s/vncPcczTyqglndeZAgjWfw

一文读懂：谷歌千元级Edge TPU为何如此之快？

https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/

Tearing Apart Google’s TPU 3.0 AI Coprocessor

https://mp.weixin.qq.com/s/3DubSQ1D59Z-PCovjHiidQ

TPU(灵魂三问 WHAT? WHY? HOW?)

https://mp.weixin.qq.com/s/j053qtpdz21IdgS22uGwAw

深入剖析卷积乘累加的硬件加速技术

https://mp.weixin.qq.com/s/XXO4hkjJkcZ5sTVVKWghEw

TPU的起源，Jeff Dean综述后摩尔定律时代的ML硬件与算法

https://mp.weixin.qq.com/s/e8zOrmiIidy8IoV1yMfdKg

谷歌长文总结四代TPU打造经验：里程碑式的TPUv4是怎样炼成的？

https://mp.weixin.qq.com/s/uEAlKdpzutOpnPGmg5dOjw

谷歌TPU超算，大模型性能超英伟达，已部署数十台：图灵奖得主新作

## Groq 

Groq是Google TPU团队的部分成员创立的AI芯片公司。

官网：

https://groq.com/

参考：

https://zhuanlan.zhihu.com/p/102357412

Groq，“软件定义硬件”概念的背后

https://www.zhihu.com/question/510527941

DSA AI芯片，相对GPGPU，能效上有多大优势？是从哪些方面提升了能效？

## Graphcore

Graphcore是一家英国芯片设计企业，成立于2012年。它的产品被称为IPU，主打数据中心的AI训练和推理。

官网：

https://www.graphcore.ai/

文档：

https://docs.graphcore.ai

这里必须表扬一下Graphcore的文档，写的非常好。

---

![](/images/img5/Tensorflow_Poplar.png)

Graphcore的Tensorflow支持，使用了XLA接口。

![](/images/img5/pytorch-software-stack.png)

而Pytorch支持，则采用了自研的PopTorch和PopART库。

PopTorch作为pytorch的平替，在建模型的同时，将模型导出为onnx格式。

PopART负责导入onnx格式的模型，并调度硬件执行。

---

参考：

https://zhuanlan.zhihu.com/p/103963276

深度剖析AI芯片初创公司Graphcore的IPU

https://mp.weixin.qq.com/s/WZQDmyjGgkGMpLjVP5jKlw

Graphcore

https://zhuanlan.zhihu.com/p/31782874

Graphcore AI芯片：更多分析

https://mp.weixin.qq.com/s/7vxJTh4IHeqUsc7IsLFLSA

解密哈萨比斯投资的IPU，他们要分英伟达一杯羹

https://www.cnblogs.com/zuyunfei/p/16349835.html

AI芯片：编程模型和硬件抽象（Nvidia CUDA vs Graphcore Poplar）

## Expedera

官网：

https://www.expedera.com

![](/images/img5/Expedera.png)

其中的partial summation (PSUM)也是近年来针对大模型的一种常见的优化方法。原理有点类似MapReduce，PSUM相当于Reducer，对数据进行初步的收集，再最终汇总到一起。这样效率上要比全局只有一个单元进行summation要高的多。

参考：

https://www.expedera.com/wp-content/uploads/2023/02/Architectural-Considerations-for-Compute-in-Memory-in-AI-Inference-final.pdf

https://www.expedera.com/wp-content/uploads/2021/04/Expedera-Linley-White-Paper.pdf

## 算力 vs 带宽

![](/images/img5/bandwidth_compute_bound.png)

在过去十年的竞争中，核心竞争指标是算力，现在变成了内存带宽和容量。

https://blog.csdn.net/tugouxp/article/details/126479276

利用roofline模型分析异构系统算力VS带宽

https://mp.weixin.qq.com/s/y9dVg9YtfWxu6NcW-fxi6Q

内存带宽与计算能力，谁才是决定深度学习执行性能的关键？

## HBM

HBM：High Bandwidth Memory是一款新型的CPU/GPU内存芯片（即“RAM”），其实就是将很多个DDR芯片堆叠在一起后和GPU封装在一起，实现大容量，高位宽的DDR组合阵列。

![](/images/img2/HBM.jpg)

这是HBM的结构图。

![](/images/img2/HBM_2.jpg)

这是HBM和GDDR5的对比图。

从上面两图可以看出，HBM的集成程度在die一级，介于PCB和chip之间，这也就是近来比较火的Chiplet技术。

HBM带宽是传统内存（DRAM）的4.5倍，因此更适合处理AI应用程序所需的大量数据。这种性能的提升是如此之大，以至于许多客户更愿意支付专用内存所需的更高价格(每GB大约25美元，标准内存大约8美元)。

参考：

https://zhuanlan.zhihu.com/p/33990592

HBM火了，它到底是什么？

https://zhuanlan.zhihu.com/p/34164501

HBM技术之显卡应用

https://mp.weixin.qq.com/s/5JrsQnwL6u1fZNIpKq_hQA

DRAM的架构历史和未来

https://mp.weixin.qq.com/s/PzD1lCe5h3VkMrhFb7CfQg

后SoC时代或将迎来Chiplet拐点

https://mp.weixin.qq.com/s/oMA9RSaq0nuSNgKDRwHavQ

Chiplet最强科普

https://zhuanlan.zhihu.com/p/356505099

Chiplet与异构集成技术研究

https://www.zhihu.com/question/565166250

AMD的chiplets芯片出了这么多年了，为什么Intel和NVIDIA还不跟进？

https://www.zhihu.com/question/318057718

hbm2显存和gddr6显存,如果单从相同显存比较性能哪个好？

https://www.zhihu.com/question/422438453

NVIDIA为什么在游戏卡上死活不用HBM2显存？

## FPGA

CGRA：Coarse Grained Reconfigurable Array

FPGA：Field Programmable Gate Array

FPGA主要是由以LUT和MUX为基础构成的门级可编程逻辑块构成的。

CGRA是coarse-grained的粗粒度，它是编程重构ALU阵列而非逻辑门阵列，因此很多的运算由硬核实现，不是LUT搭真值表。

https://www.zhihu.com/question/407417134

CGRA和FPGA有什么不同？

---

https://mp.weixin.qq.com/s/Cy_vb0PpcvGTDmlMt1VkSw

从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局

https://mp.weixin.qq.com/s/DnurlGgd5q4Fwjxy3YnIrQ

当数据库遇见FPGA：X-DB异构计算如何实现百万级TPS？

https://mp.weixin.qq.com/s/H_KnHQ6AzCNFqmW8uDj1rA

基于FPGA的深度学习加速器综述：挑战与机遇

https://mp.weixin.qq.com/s/acAbNP_ERnKlL3_9v_mwow

FPGA：AI ASIC的必经之路？

https://mp.weixin.qq.com/s/K_dohaZbCISZlxe1Utu50w

如何用FPGA加速卷积神经网络(CNN)？

https://zhuanlan.zhihu.com/p/152167194

两大FPGA公司的“AI技术路线”

## 行业信息

https://mp.weixin.qq.com/s/vGoWsyaal-gAzsrhPguvFg

深度解读：华为麒麟芯片是如何炼成的！

https://mp.weixin.qq.com/s/8RDHTn6P63otKXUdrHhbjw

一文看懂AI芯片产业生态及竞争格局

https://mp.weixin.qq.com/s/jINnom16KWiEKiug3N-f8g

一文看懂AI芯片：三大门派四大场景146亿美元大蛋糕

https://mp.weixin.qq.com/s/-FwuhibwwG6CFUcZXNBTFA

投资者梳理AI芯片产业，一文秒懂AI芯片生态！

https://zhuanlan.zhihu.com/p/28325678

零基础看懂全球AI芯片：详解“xPU”

https://mp.weixin.qq.com/s/Zng0NTR9P78lnR_vniiM8g

Chris Rowen: 分析全球334家真正的深度学习创业公司，盘点25家AI芯片创业公司

https://zhuanlan.zhihu.com/p/33462550

传统IP Vendor的AI加速器一览

https://mp.weixin.qq.com/s/IaCWZXQI8mYLJQXwDoNQcQ

自动驾驶芯片：GPU的现在和ASIC的未来

https://mp.weixin.qq.com/s/KjQ5BTGd92Y0Mqzk1A5JYg

老兵戴辉讲述海思视频监控芯片从0到1的血泪史！如何一步步成为行业霸主的

https://mp.weixin.qq.com/s/MwZ9j1MIwRBrJK4iWKzRqQ

芯故事：和AMD有渊源的那些AI创业公司

## 参考

![](/images/img3/Wallace_Tree.png)

https://mp.weixin.qq.com/s/_n1FA7H5q4AwXqeBg9tekA

硬件实现快速累加

>Christopher Stewart "Chris" Wallace，1933～2004，澳大利亚计算机科学家和物理学家。University of Sydney博士（1959）。Monash University教授。ACM fellow。在早期计算机的软件/硬件方面皆有重大贡献。

---

几乎每一个概述的AI加速解决方案都是从一个已经有几十年历史的学术思想开始的：脉动阵列起源于1978；VLIW架构起源于1983；数据流编程的概念可以追溯到1975；早期的内存内处理（processing-in-memory）出现在20世纪70年代。

https://www.thepaper.cn/newsDetail_forward_16268882

详解AI加速器（一）：2012年的AlexNet到底做对了什么？

https://www.thepaper.cn/newsDetail_forward_16641034

详解AI加速器（二）：为什么说现在是AI加速器的黄金时代？
