---
layout: post
title:  AI Chip（二）
category: Chip 
---

* toc
{:toc}

# AI Chip

## TPU

​Tensor Processing Unit(TPU)是Google推出的AI芯片系列。目前已经有3个版本：

TPU v1, deployed 2015, 92 teraops, **inference only**.

TPU v2, cloud TPU 2017, pod 2018, 180 teraflops, 64 GB HBM, **training and inference**, generally available. 11.5 petaflops in a **pod**.

TPU v3, cloud beta 2018, 420 teraflops, 128 GB HBM, training and inference, beta. >100 petaflops in a pod.

论文：

《In-Datacenter Performance Analysis of a Tensor Processing Unit​》

《TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings》

![](/images/img4/TPU.png)

---

![](/images/img5/TPU.png)

TPUv2的改进：

- TPU v2将上述两个互相独立的缓冲区调整位置后合并为向量存储区（Vector Memory），从而提高可编程性。

- 改进针对的是激活函数管道（Activation Pipeline），TPU v1的管道内包含一组负责非线性激活函数运算的固定功能单元。TPU v2则将其改为可编程性更高的向量单元（Vector Unit），使其对编译器和编程人员而言更易用。

- 将矩阵乘法单元直接与向量存储区连接，如此一来，矩阵乘法单元就成为向量单元的协处理器。这种结构对编译器和编程人员而言更友好。

- TPU v1使用DDR3内存，因为它针对的是推理，只需使用已有的权重，不需要生成权重。针对训练的TPU v2则不一样，训练时既要读取权重，也要写入权重，所以在v2中，原本的DDR3改为与向量存储区相连，这样就既能向其读取数据，又能向其写入数据。

- 将DDR3改为HBM。因为从DDR3读取参数速度太慢，影响性能，而HBM的读写速度快20倍。

- 在HBM和向量存储区之间增加互连（Interconnect），用于TPU之间的连接，组成Pod超级计算机。

TPUv3在体系结构方面改进不大，主要是堆料：加计算单元，加内存，加主频等等。

---

TPUv4是TPU系列时隔4年（2021）之后的大升级。它的改进主要是：

- TPU核心间的网络拓扑由2D Tours改为3D Tours。

- 加入Sparse Core专门处理Transformer模型中的Embedding操作。

- 引入Optical Circuit Switching，OCS。光互连是物理距离较远的芯片的首选。隔壁的PCIE系列也类似，PCIE 6将是最后一代使用电线的标准，未来的PCIE 7+也将转向光通信。

---

TPU v5e是2023年发布的。

https://zhuanlan.zhihu.com/p/653993836

面向大模型的最强DSA——TPU v5e架构分析

https://zhuanlan.zhihu.com/p/626412573

谈谈Google TPUv4处理器的硬件结构、计算范式与SuperPod互连拓扑－－部分细节对比Nvidia

---

https://www.zhihu.com/answer/2716117809

David Patterson在UCB的演讲，分享了Google TPU近十年的发展历程以及心得体会

## Coral Dev Board

Coral Dev Board是Google于2019年3月推出的一款搭载TPU的嵌入式开发板。

参考：

http://linuxgizmos.com/google-launches-i-mx8m-dev-board-with-edge-tpu-ai-chip/

Google launches i.MX8M dev board with Edge TPU AI chip

## 脉动阵列

**AI=矩阵乘+非线性**

由于非线性只有可怜的O(N)的计算量，矩阵乘的O(N^3)的计算量才是AI中的绝对计算瓶颈。

---

Systolic array是孔祥重和他的博士生Charles Leiserson于1978年发明的。

论文：

《Why systolic architectures?》

![](/images/img4/Systolic_array.gif)

>孔祥重（H. T. Kung/Kung, Hsiang-Tsung），1945年生。台湾国立清华大学本科（1968）+CMU博士（1974）。CMU、Harvard教授。台湾中央研究院院士、美国工程院院士。除了Systolic array之外，数据库领域的Optimistic concurrency control（乐观并发控制）也是他的贡献。   
>除了Charles Eric Leiserson之外，他的博士生还有Robert Tappan Morris，也就是著名的Morris Worm的作者。

参考：

http://web.cecs.pdx.edu/~mperkows/temp/May22/0020.Matrix-multiplication-systolic.pdf

矩阵乘法器原理

http://www.eecs.harvard.edu/~htk/publication/1980-introduction-to-vlsi-systems-kung-leiserson.pdf

Algorithms for VLSI Processor Arrays

https://zhuanlan.zhihu.com/p/26522315

脉动阵列-因Google TPU获得新生

http://www.sohu.com/a/142237570_505803

我们应该拥抱“脉动阵列”吗？

https://zhuanlan.zhihu.com/p/26882794

Google深度揭秘TPU

---

参考：

https://mp.weixin.qq.com/s/1X9xiZkmVPI-j-aipr-ocg

AlphaGo Master最新架构和算法，谷歌云与TPU拆解

https://mp.weixin.qq.com/s/Yo0uKd1Mzy4mmS4r0mxfVw

有图有真相：深度拆解谷歌TPU3.0，新一代AI协同处理器

https://mp.weixin.qq.com/s/kPrZ0PuevXEJjVB7RXs70g

谷歌TPU率队，颠覆3350亿美元的半导体行业

https://mp.weixin.qq.com/s/wunqEHC6c-yUVXTl4yTG4w

仅需1/5成本：TPU是如何超越GPU，成为深度学习首选处理器的

https://mp.weixin.qq.com/s/vncPcczTyqglndeZAgjWfw

一文读懂：谷歌千元级Edge TPU为何如此之快？

https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/

Tearing Apart Google’s TPU 3.0 AI Coprocessor

https://mp.weixin.qq.com/s/3DubSQ1D59Z-PCovjHiidQ

TPU(灵魂三问 WHAT? WHY? HOW?)

https://mp.weixin.qq.com/s/j053qtpdz21IdgS22uGwAw

深入剖析卷积乘累加的硬件加速技术

https://mp.weixin.qq.com/s/XXO4hkjJkcZ5sTVVKWghEw

TPU的起源，Jeff Dean综述后摩尔定律时代的ML硬件与算法

https://mp.weixin.qq.com/s/e8zOrmiIidy8IoV1yMfdKg

谷歌长文总结四代TPU打造经验：里程碑式的TPUv4是怎样炼成的？

https://mp.weixin.qq.com/s/uEAlKdpzutOpnPGmg5dOjw

谷歌TPU超算，大模型性能超英伟达，已部署数十台：图灵奖得主新作

https://zhuanlan.zhihu.com/p/656363598

谷歌TPU2机器学习集群的幕后

## Groq 

Groq是Google TPU团队的部分成员创立的AI芯片公司。

官网：

https://groq.com/

参考：

https://zhuanlan.zhihu.com/p/102357412

Groq，“软件定义硬件”概念的背后

https://www.zhihu.com/question/510527941

DSA AI芯片，相对GPGPU，能效上有多大优势？是从哪些方面提升了能效？

## Graphcore

Graphcore是一家英国芯片设计企业，成立于2012年。它的产品被称为IPU，主打数据中心的AI训练和推理。

官网：

https://www.graphcore.ai/

文档：

https://docs.graphcore.ai

这里必须表扬一下Graphcore的文档，写的非常好。

---

![](/images/img5/Tensorflow_Poplar.png)

Graphcore的Tensorflow支持，使用了XLA接口。

![](/images/img5/pytorch-software-stack.png)

而Pytorch支持，则采用了自研的PopTorch和PopART库。

PopTorch作为pytorch的平替，在建模型的同时，将模型导出为onnx格式。

PopART负责导入onnx格式的模型，并调度硬件执行。

---

参考：

https://zhuanlan.zhihu.com/p/103963276

深度剖析AI芯片初创公司Graphcore的IPU

https://mp.weixin.qq.com/s/WZQDmyjGgkGMpLjVP5jKlw

Graphcore

https://zhuanlan.zhihu.com/p/31782874

Graphcore AI芯片：更多分析

https://mp.weixin.qq.com/s/7vxJTh4IHeqUsc7IsLFLSA

解密哈萨比斯投资的IPU，他们要分英伟达一杯羹

https://www.cnblogs.com/zuyunfei/p/16349835.html

AI芯片：编程模型和硬件抽象（Nvidia CUDA vs Graphcore Poplar）

## Expedera

官网：

https://www.expedera.com

![](/images/img5/Expedera.png)

其中的partial summation (PSUM)也是近年来针对大模型的一种常见的优化方法。原理有点类似MapReduce，PSUM相当于Reducer，对数据进行初步的收集，再最终汇总到一起。这样效率上要比全局只有一个单元进行summation要高的多。

参考：

https://www.expedera.com/wp-content/uploads/2023/02/Architectural-Considerations-for-Compute-in-Memory-in-AI-Inference-final.pdf

https://www.expedera.com/wp-content/uploads/2021/04/Expedera-Linley-White-Paper.pdf

## HBM

HBM：High Bandwidth Memory是一款新型的CPU/GPU内存芯片（即“RAM”），其实就是将很多个DDR芯片堆叠在一起后和GPU封装在一起，实现大容量，高位宽的DDR组合阵列。

![](/images/img2/HBM.jpg)

这是HBM的结构图。

![](/images/img2/HBM_2.jpg)

这是HBM和GDDR5的对比图。

从上面两图可以看出，HBM的集成程度在die一级，介于PCB和chip之间，这也就是近来比较火的Chiplet技术。

HBM带宽是传统内存（DRAM）的4.5倍，因此更适合处理AI应用程序所需的大量数据。这种性能的提升是如此之大，以至于许多客户更愿意支付专用内存所需的更高价格(每GB大约25美元，标准内存大约8美元)。

参考：

https://zhuanlan.zhihu.com/p/33990592

HBM火了，它到底是什么？

https://zhuanlan.zhihu.com/p/34164501

HBM技术之显卡应用

https://mp.weixin.qq.com/s/5JrsQnwL6u1fZNIpKq_hQA

DRAM的架构历史和未来

https://mp.weixin.qq.com/s/PzD1lCe5h3VkMrhFb7CfQg

后SoC时代或将迎来Chiplet拐点

https://mp.weixin.qq.com/s/oMA9RSaq0nuSNgKDRwHavQ

Chiplet最强科普

https://zhuanlan.zhihu.com/p/356505099

Chiplet与异构集成技术研究

https://www.zhihu.com/question/565166250

AMD的chiplets芯片出了这么多年了，为什么Intel和NVIDIA还不跟进？

https://www.zhihu.com/question/318057718

hbm2显存和gddr6显存,如果单从相同显存比较性能哪个好？

https://www.zhihu.com/question/422438453

NVIDIA为什么在游戏卡上死活不用HBM2显存？

## FPGA

CGRA：Coarse Grained Reconfigurable Array

FPGA：Field Programmable Gate Array

FPGA主要是由以LUT和MUX为基础构成的门级可编程逻辑块构成的。

CGRA是coarse-grained的粗粒度，它是编程重构ALU阵列而非逻辑门阵列，因此很多的运算由硬核实现，不是LUT搭真值表。

https://www.zhihu.com/question/407417134

CGRA和FPGA有什么不同？

---

https://mp.weixin.qq.com/s/Cy_vb0PpcvGTDmlMt1VkSw

从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局

https://mp.weixin.qq.com/s/DnurlGgd5q4Fwjxy3YnIrQ

当数据库遇见FPGA：X-DB异构计算如何实现百万级TPS？

https://mp.weixin.qq.com/s/H_KnHQ6AzCNFqmW8uDj1rA

基于FPGA的深度学习加速器综述：挑战与机遇

https://mp.weixin.qq.com/s/acAbNP_ERnKlL3_9v_mwow

FPGA：AI ASIC的必经之路？

https://mp.weixin.qq.com/s/K_dohaZbCISZlxe1Utu50w

如何用FPGA加速卷积神经网络(CNN)？

https://zhuanlan.zhihu.com/p/152167194

两大FPGA公司的“AI技术路线”
