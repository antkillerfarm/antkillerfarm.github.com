---
layout: post
title:  深度强化学习（八）——StarCraft, AlphaStar, Dota 2
category: DRL 
---

* toc
{:toc}

# MARL（续）

MARL的目标是找到每一个状态的纳什均衡策略，然后将这些策略联合起来。

<div style="text-align: center;">
<table>
  <tr>
    <td colspan="2" rowspan="2"></td>
    <td colspan="3">Game setting</td>
  </tr>
  <tr>
    <td>Stateless Games</td>
    <td>Team Markov Games</td>
    <td>General Markov Games</td>
  </tr>
  <tr>
    <td rowspan="2">Information<br/>Requirement</td>
    <td>Independent Learners</td>
    <td>Stateless Q-learning<br/>Learning Automata<br/>IGA<br/>FMQ<br/>Commitment Sequences<br/>Lenient Q-learners</td>
    <td>Policy Search<br/>Policy Gradient</td>
    <td>MG-ILA<br/>(WoLF-)PG<br/>Learning of Coordination<br/>Independent RL<br/>CQ-learning</td>
  </tr>
  <tr>
    <td>Joint Action Learners</td>
    <td></td>
    <td>Distributed- Q<br/>Sparse Tabular Q<br/>Utile Coordination</td>
    <td>Nash-Q<br/>Friend-or-Foe Q<br/>Asymmetric Q<br/>Joint Action Learning<br/>Correlated-Q</td>
  </tr>
</table>
</div>

参考：

https://zhuanlan.zhihu.com/c_1061939147282915328

一个MARL方面的专栏

https://www.zhihu.com/people/yao-xing-hu/posts

一个MARL方面的专栏

https://github.com/LantaoYu/MARL-Papers

Paper list of multi-agent reinforcement learning (MARL)

https://mp.weixin.qq.com/s/5I8-_4Xsj8wzJwmDOQpkWQ

多智能体学习231页PPT总结

https://blog.csdn.net/qq_38163755/article/details/98057751

Game Theory and Multi-agent Reinforcement Learning笔记1

https://blog.csdn.net/qq_38163755/article/details/98223987

Game Theory and Multi-agent Reinforcement Learning笔记2

https://mp.weixin.qq.com/s/Yx0pfAJwOSQD2ABJd3gJSA

多智能体强化学习（MARL）近年研究概览

https://mp.weixin.qq.com/s/bQx-ydEAfijgK1Ii5ilPFQ

UCL汪军团队新方法提高群体智能，解决大规模AI合作竞争

https://mp.weixin.qq.com/s/4lj_G2z0Lbfk5mOgvxLC6w

对抗式协作：一个框架解决多个无监督学习视觉问题

https://mp.weixin.qq.com/s/0v57oHMEDcJuUivs8D5pnQ

善于单挑却难以协作，构建多智能体AI系统为何如此之难？

https://mp.weixin.qq.com/s/tME5GsKEXveVcgWb-Zbx_A

乔明达ICML 2018论文提出协作学习的鲁棒性方法

https://mp.weixin.qq.com/s/0Xuxr7CXqsxrssW2I6NCBQ

通用智能化：BAIR简述人类-机器人协作新技术

https://mp.weixin.qq.com/s/GNbCu1lbOmwJDCU3vgMbtQ

OpenAI发布多智能体深度强化学习新算法LOLA

https://mp.weixin.qq.com/s/5Go20MyBxdVI1r5SkwA6lw

面向星际争霸：DeepMind提出多智能体强化学习新方法

https://mp.weixin.qq.com/s/TlcVxjhHXXGSpIvptC4H8g

使用Gym和CNN构建多智能体自动驾驶马里奥赛车

https://mp.weixin.qq.com/s/zRcq1LMtK72Cl4T3877tgQ

牛津教授吐槽DeepMind心智神经网络，还推荐了这些多智能体学习论文

https://mp.weixin.qq.com/s/qU6XD45RGeMI-T7GI0dg2Q

OpenAI竞争性自我对抗训练：简单环境下获得复杂的智能体

https://mp.weixin.qq.com/s/cUFSaHWGKhyZhEvRdX09hw

谷歌大脑QT-Opt算法，机器人探囊取物成功率96%

https://mp.weixin.qq.com/s/xxB1lsiQxSfaHPKwZUI9bw

拔掉机器人的一条腿，它还能学走路？——三次元里优化的DRL策略

https://mp.weixin.qq.com/s/e4hriDvTRLkg-mIifVWayw

如何解决深度学习中的多体问题

https://mp.weixin.qq.com/s/Js8lvoqtl7vcwdw3i7UOHg

多智能体(MARL)强化学习与博弈论

https://zhuanlan.zhihu.com/p/61060358

Paper survey: Multi-Agent Reinforcement Learning

https://zhang-yi-chi.github.io/2019/08/15/cooperative-multi-agent-rl/

Cooperative Multi-Agent Reinforcement Learning

https://mp.weixin.qq.com/s/pLN-FHNSDYPcKmJCa4vOYA

机器人在线“偷懒”怎么办？阿里研究出了这两套算法

https://mp.weixin.qq.com/s/dkJr29LqO2PdSR6ds7X2qg

协作多智能体强化学习中的回报函数设计

https://mp.weixin.qq.com/s/m18xiMRryXl_BoCxKVUbTA

多Agent强化学习综述

https://mp.weixin.qq.com/s/R12bVEDWpHydNair-P5TzA

多智能体深度强化学习中的Q值路径分解

https://mp.weixin.qq.com/s/1JJo0FgMVlhIy6R9CxI_sQ

多智能体强化学习算法理论研究

https://mp.weixin.qq.com/s/qnAXhfGb74ivRwlGcdApOQ

一文详解多智能体强化学习的基础和应用

https://mp.weixin.qq.com/s/4AfoQ6IBJ2rKbmKvYD1Dow

多Agent深度强化学习综述(中文版)，21页pdf

https://mp.weixin.qq.com/s/KPS6RWtmeawR_vPAY4Bh1A

听说你的多智能体强化学习算法不work？那你用对MAPPO了吗?

https://mp.weixin.qq.com/s/qof0GWI3Jfsy3Tpc41wa4A

通过奖励随机化发现多智能体游戏中多样性策略行为，清华、UC伯克利等研究者提出全新算法RPG

https://mp.weixin.qq.com/s/aVDcwkcDkbU23MN4Wcf0mg

多智能体深度强化学习：综述

# StarCraft

AIIDE（Artificial Intelligence and Interactive Digital Entertainment）

曾经有人设计出一种 “悍马2000（Automation 2000）”的脚本，极限APM达到15000（顶尖职业选手APM大约为200+），实现了一系列诸如“100只狗拆掉20辆坦克”、“机枪兵甩毒爆”、“无双运输机甩牛”等眼花缭乱的壮举。

https://mp.weixin.qq.com/s/zbki0baZw-NjAgZwKcfgFg

《星际争霸》与人工智能

https://mp.weixin.qq.com/s/Jdd7S6JFKRdsVe2w-W01IA

Facebook田渊栋开源游戏平台ELF，简化版《星际争霸》完美测试人工智能

https://mp.weixin.qq.com/s/QD6BdAB332xHoSH3dIfM5Q

学习顶级玩家Replay，人工智能学会了星际争霸的“大局观”

https://mp.weixin.qq.com/s/0Q-Kg6pNVRl3tqv8-wH-bg

Facebook开源史上最大星际争霸AI研究数据集

https://mp.weixin.qq.com/s/KbFvs8EcEfm29zhLCPt0Iw

DeepMind进军星际争霸2，谷歌Facebook打响通用AI战争

https://mp.weixin.qq.com/s/WTRD3wRQ_wSXjjlt7jiwSg

CommandCenter：基于暴雪官方API的星际争霸2 AI Bot

https://mp.weixin.qq.com/s/CQmZcwI4bgrEiWHP0gBLfQ

DeepMind星际争霸2开源机器学习平台入门

https://zhuanlan.zhihu.com/p/28434323

迈向通用人工智能：星际争霸2人工智能研究环境SC2LE完全入门指南

https://zhuanlan.zhihu.com/p/29246185

重现DeepMind星际争霸强化学习算法

https://mp.weixin.qq.com/s/vA9cHv73iHgTy9Q_bk4KCw

DeforGAN：用GAN实现星际争霸开全图外挂！

https://mp.weixin.qq.com/s/x0G-LNCano7qjGdpj-fpCQ

2017年度星际争霸AI竞赛结果出炉

https://mp.weixin.qq.com/s/AndV-htPeTXlpAm1GONNtA

中科院开源星际争霸2宏观运营研究数据集MSC

https://mp.weixin.qq.com/s/I9gfoCxEYW5lSIy5QGyt6w

2018星际AI大赛（2018.11）冠军诞生！一个个机器学习算法，都输给不会学习的韩国bot

https://github.com/TeamSAIDA/SAIDA

2018.11冠军SAIDA的github

https://github.com/TorchCraft/TorchCraft

2018.11亚军CherryPi的github

https://github.com/bmnielsen/Locutus/

2018.8冠军Locutus的github

https://mp.weixin.qq.com/s/yAYF2i-Pr84RliKrJyPaUA

伯克利星际争霸II AI“撞车”腾讯，作者：我们不一样

https://mp.weixin.qq.com/s/uC8gi4kN_muRTwRL0RCZjQ

要玩转这个星际争霸II开源AI，你只需要i5+GTX1050

https://mp.weixin.qq.com/s/wyn-V4-DcjjOKziith3pXw

开源星际争霸2多智能体挑战smac

https://mp.weixin.qq.com/s/e4iv6FJAMZACEqWFmxeSFg

让AI掌握星际争霸微操：中科院提出强化学习+课程迁移学习方法

https://mp.weixin.qq.com/s/JbCIBFDRvg5qcpZ11g58dw

DeepMind提出关系性深度强化学习：在星际争霸2任务中获得最优水平

https://mp.weixin.qq.com/s/h0be7yRQMjLFi8EteTzuMQ

腾讯AI在星际2完整对战中击败“作弊级”内建Bot

https://zhuanlan.zhihu.com/p/29222384

sc2-101: 第一个rule-base的星际二agent

https://mp.weixin.qq.com/s/pggQL5uLasJ7icnlrMs9OQ

自动化所开源星际争霸2基准AI

# AlphaStar

AlphaStar是DeepMind在解决了围棋问题之后，在RTS游戏领域的尝试。

里程碑事件：

2018.12 5:0击败TLO，5:0击败MaNa。

2019.1 表演赛不敌MaNa。

论文：

《Grandmaster level in StarCraft II using multi-agent reinforcement learning》

参考：

https://mp.weixin.qq.com/s/_Y0bCjTu9UrHfnen15htqQ

AlphaStar称霸星际争霸2！AI史诗级胜利，DeepMind再度碾压人类

https://mp.weixin.qq.com/s/axr5VFbHQmYo0shW9ilBaQ

DeepMind回应一切：AlphaStar两百年相当于人类多长时间？

https://www.zhihu.com/question/310011363

如何评价DeepMind在北京时间19年1月25日2点的《星际争霸 2》项目演示？

https://mp.weixin.qq.com/s/k0l2uoik-Z9aA9zax7AoZg

中科院自动化所深度解析：Deepmind AlphaStar如何战胜人类职业玩家

https://zhuanlan.zhihu.com/p/55781614

AlphaStar背后的机器学习原理

https://mp.weixin.qq.com/s/XljE82cJZfFOgf2KrXWSKA

DeepMind首个战胜星际2职业玩家的AI为何无敌？新视角揭秘AI里程碑

https://mp.weixin.qq.com/s/_t2vLFfIG6VYAdWDalpIXQ

说人话教AI打游戏，Facebook开源迷你版星际争霸，成果登上NeurIPS 2019

https://mp.weixin.qq.com/s/X7QQn6W2dB3y_Ljii-sJ3w

Alphastar再登Nature！星际争霸任一种族，战网狂虐99.8%人类玩家

https://zhuanlan.zhihu.com/p/102749648

基于多智能体强化学习主宰星际争霸游戏

https://zhuanlan.zhihu.com/p/92543229

AlphaStar

https://zhuanlan.zhihu.com/p/97720096

浅谈AlphaStar

https://mp.weixin.qq.com/s/R4RXxLan7H2sCbBrKqyH6w

基于多智能体强化学习的《星际争霸II》中大师级水平的技术研究

## 启元世界

启元世界是国内的一家AI公司，碰巧也搞星际争霸2。研究员Flood Sung后来也加盟了该团队。

2020.6.21 启元AI 2：0 人类选手《星际争霸I/II》全国冠军黄慧明（TooDming）和中国星际最强人族选手、黄金总决赛三连冠选手李培楠（TIME）。

参考：

https://mp.weixin.qq.com/s/ev6F_prsP5nAtFKze3-zCQ

启元AI两局2：0战胜中国星际争霸冠军，仅用顶级科技巨头1%算力

https://www.zhihu.com/question/402678197

如何看待启元世界AI星际指挥官打败人族选手TIME？

# Dota 2

Dota 2的AI项目主要是OpenAI来搞，算是DRL在MOBA游戏上的应用。

里程碑事件：

2017.8 1vs1击败Dendi。2：0。稍后，OpenAI将该版本放在线上，被人找出破绽，惨遭调戏。

2018.6 OpenAI Five击败半职业玩家。

2018.7 OpenAI Five在Ti8表演赛被paiN Gaming和中国退役大神队击败。

2019.4 OpenAI Five击败Ti8冠军OG。

2019.12 OpenAI开发了更强的Rerun。Rerun对OpenAI Five的胜率为98%。

论文:

《Dota 2 with Large Scale Deep Reinforcement Learning》

参考：

https://mp.weixin.qq.com/s/rPsxvzOLuoo_IRtgH94Ecw

OpenAI自学习多智能体5v5团队战击败人类玩家

https://mp.weixin.qq.com/s/VPVzXzBYD_nYO1SjTMNGVA

Dota2团战AI击败人类最全解析：能团又能gank，AI一日人间180年

https://mp.weixin.qq.com/s/5Vg9RFvyNv6T7QkIfPm1aQ

DOTA2中打败Dendi的AI如何炼出？

https://mp.weixin.qq.com/s/e4lK3UoQq-8j1YIWKALBLg

嵌入技术在Dota2人工智能战队OpenAI Five中的应用

https://mp.weixin.qq.com/s/xsoS3tvOh78rkbI0UUIkPQ

2:0！Dota2世界冠军OG被OpenAI碾压，全程人类只推掉两座外塔

https://mp.weixin.qq.com/s/X73jL5T_uZ87F3iE-uTnpw

Dota2冠军OG如何被AI碾压？OpenAI累积三年的完整论文终于放出
