---
layout: post
title:  AI Chip（一）——TPU, HBM
category: Chip 
---

* toc
{:toc}

# AI Chip

![](/images/img5/hw_ai.jpg)

![](/images/img2/AI_Chips.png)

https://basicmi.github.io/AI-Chip/

A list of ICs and IPs for AI, Machine Learning and Deep Learning.

![](/images/img4/AI_chip.png)

## NN计算的硬件设计

NN计算问题的瓶颈主要包括两类：

1.数学运算的速度。NN运算主要以乘加为主，实现这类加速功能的硬件单元一般被称为NN Processor。这也是第一代AI芯片主要解决的问题。

再细分的话，又有矩阵派和卷积派两种。

矩阵派的通用性好，且FC运算速度快于卷积派。

而卷积派由于针对Conv的Kernel数据不变这一特点进行优化，Conv速度极快。

2.IO问题，也称带宽问题。早期的NN由于算子有限，基本只有FC、Conv、Pooling、Activation等少数几种算子。但现在的NN模型算子可就多了，且有相当部分算子属于非计算类的搬运数据算子，比如permute等。针对这类运算，一般采用被称作Tensor Processor的硬件单元进行加速。

## TPU

​Tensor Processing Unit(TPU)是Google推出的AI芯片系列。目前已经有3个版本：

TPU v1, deployed 2015, 92 teraops, **inference only**.

TPU v2, cloud TPU 2017, pod 2018, 180 teraflops, 64 GB HBM, **training and inference**, generally available. 11.5 petaflops in a **pod**.

TPU v3, cloud beta 2018, 420 teraflops, 128 GB HBM, training and inference, beta. >100 petaflops in a pod.

论文：

《In-Datacenter Performance Analysis of a Tensor Processing Unit​》

《TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings》

![](/images/img4/TPU.png)

---

![](/images/img5/TPU.png)

TPUv2的改进：

- TPU v2将上述两个互相独立的缓冲区调整位置后合并为向量存储区（Vector Memory），从而提高可编程性。

- 改进针对的是激活函数管道（Activation Pipeline），TPU v1的管道内包含一组负责非线性激活函数运算的固定功能单元。TPU v2则将其改为可编程性更高的向量单元（Vector Unit），使其对编译器和编程人员而言更易用。

- 将矩阵乘法单元直接与向量存储区连接，如此一来，矩阵乘法单元就成为向量单元的协处理器。这种结构对编译器和编程人员而言更友好。

- TPU v1使用DDR3内存，因为它针对的是推理，只需使用已有的权重，不需要生成权重。针对训练的TPU v2则不一样，训练时既要读取权重，也要写入权重，所以在v2中，原本的DDR3改为与向量存储区相连，这样就既能向其读取数据，又能向其写入数据。

- 将DDR3改为HBM。因为从DDR3读取参数速度太慢，影响性能，而HBM的读写速度快20倍。

- 在HBM和向量存储区之间增加互连（Interconnect），用于TPU之间的连接，组成Pod超级计算机。

TPUv3在体系结构方面改进不大，主要是堆料：加计算单元，加内存，加主频等等。

---

TPUv4是TPU系列时隔4年（2021）之后的大升级。它的改进主要是：

- TPU核心间的网络拓扑由2D Tours改为3D Tours。

- 加入Sparse Core专门处理Transformer模型中的Embedding操作。

- 引入Optical Circuit Switching，OCS。

---

TPU v5e是2023年发布的。

从TPU v5开始，e系列负责训推一体，Pod规模不会很大,部署更加灵活，而p系列则负责更大规模的Foundation Model训练任务。

https://zhuanlan.zhihu.com/p/653993836

面向大模型的最强DSA——TPU v5e架构分析

https://zhuanlan.zhihu.com/p/626412573

谈谈Google TPUv4处理器的硬件结构、计算范式与SuperPod互连拓扑－－部分细节对比Nvidia

https://zhuanlan.zhihu.com/p/3326989452

简单谈谈Google TPUv6

---

https://www.zhihu.com/answer/2716117809

David Patterson在UCB的演讲，分享了Google TPU近十年的发展历程以及心得体会

## Coral Dev Board

Coral Dev Board是Google于2019年3月推出的一款搭载TPU的嵌入式开发板。

参考：

http://linuxgizmos.com/google-launches-i-mx8m-dev-board-with-edge-tpu-ai-chip/

Google launches i.MX8M dev board with Edge TPU AI chip

## 脉动阵列

**AI=矩阵乘+非线性**

由于非线性只有可怜的O(N)的计算量，矩阵乘的O(N^3)的计算量才是AI中的绝对计算瓶颈。

---

Systolic array是孔祥重和他的博士生Charles Leiserson于1978年发明的。

论文：

《Why systolic architectures?》

![](/images/img4/Systolic_array.gif)

>孔祥重（H. T. Kung/Kung, Hsiang-Tsung），1945年生。台湾国立清华大学本科（1968）+CMU博士（1974）。CMU、Harvard教授。台湾中央研究院院士、美国工程院院士。除了Systolic array之外，数据库领域的Optimistic concurrency control（乐观并发控制）也是他的贡献。   
>除了Charles Eric Leiserson之外，他的博士生还有Robert Tappan Morris，也就是著名的Morris Worm的作者。

上图中，矩阵A和B都是脉动式的进入计算单元，这种情况，一般被称为全脉动阵列。

如果A和B，其中之一保持不变，只有另一个脉动式的进入计算单元，则为半脉动阵列。

参考：

http://web.cecs.pdx.edu/~mperkows/temp/May22/0020.Matrix-multiplication-systolic.pdf

矩阵乘法器原理

http://www.eecs.harvard.edu/~htk/publication/1980-introduction-to-vlsi-systems-kung-leiserson.pdf

Algorithms for VLSI Processor Arrays

https://zhuanlan.zhihu.com/p/26522315

脉动阵列-因Google TPU获得新生

http://www.sohu.com/a/142237570_505803

我们应该拥抱“脉动阵列”吗？

https://zhuanlan.zhihu.com/p/26882794

Google深度揭秘TPU

---

参考：

https://mp.weixin.qq.com/s/1X9xiZkmVPI-j-aipr-ocg

AlphaGo Master最新架构和算法，谷歌云与TPU拆解

https://mp.weixin.qq.com/s/Yo0uKd1Mzy4mmS4r0mxfVw

有图有真相：深度拆解谷歌TPU3.0，新一代AI协同处理器

https://mp.weixin.qq.com/s/kPrZ0PuevXEJjVB7RXs70g

谷歌TPU率队，颠覆3350亿美元的半导体行业

https://mp.weixin.qq.com/s/wunqEHC6c-yUVXTl4yTG4w

仅需1/5成本：TPU是如何超越GPU，成为深度学习首选处理器的

https://mp.weixin.qq.com/s/vncPcczTyqglndeZAgjWfw

一文读懂：谷歌千元级Edge TPU为何如此之快？

https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/

Tearing Apart Google’s TPU 3.0 AI Coprocessor

https://mp.weixin.qq.com/s/3DubSQ1D59Z-PCovjHiidQ

TPU(灵魂三问 WHAT? WHY? HOW?)

https://mp.weixin.qq.com/s/j053qtpdz21IdgS22uGwAw

深入剖析卷积乘累加的硬件加速技术

https://mp.weixin.qq.com/s/XXO4hkjJkcZ5sTVVKWghEw

TPU的起源，Jeff Dean综述后摩尔定律时代的ML硬件与算法

https://mp.weixin.qq.com/s/e8zOrmiIidy8IoV1yMfdKg

谷歌长文总结四代TPU打造经验：里程碑式的TPUv4是怎样炼成的？

https://mp.weixin.qq.com/s/uEAlKdpzutOpnPGmg5dOjw

谷歌TPU超算，大模型性能超英伟达，已部署数十台：图灵奖得主新作

https://zhuanlan.zhihu.com/p/656363598

谷歌TPU2机器学习集群的幕后

## HBM

HBM：High Bandwidth Memory是一款新型的CPU/GPU内存芯片（即“RAM”），其实就是将很多个DDR芯片堆叠在一起后和GPU封装在一起，实现大容量，高位宽的DDR组合阵列。

![](/images/img2/HBM.jpg)

这是HBM的结构图。

![](/images/img2/HBM_2.jpg)

这是HBM和GDDR5的对比图。

从上面两图可以看出，HBM的集成程度在chip一级，介于PCB和die之间，这也就是近来比较火的Chiplet技术。

HBM带宽是传统内存（DRAM）的4.5倍，因此更适合处理AI应用程序所需的大量数据。这种性能的提升是如此之大，以至于许多客户更愿意支付专用内存所需的更高价格(每GB大约25美元，标准内存大约8美元)。

LPDDR和HBM卷成本，如果考虑的是$/GB，那LPDDR确实有优势，但如果考虑的是$/GBps，HBM还是最具性价比的选择。而LLM虽然对内存容量有比较大的需求，但对于内存带宽同样有巨大的需求。

NVIDIA是GDDR协议几乎实际的掌控者，每一代GDDR出来，Nvidia都会来个GDDR-X，这个-X，实际上是下一代GDDR的beta特性，即其他人用GDDR6的时候，Nvidia有GDDR6-X定制（并且独享供货），而这玩意儿实际上是GDDR7-beta。

---

HBM是将DRAM透过先进封装技术堆栈而成，利用硅穿孔（TSV）技术，再填充导电物质连接金属球以达通电效果。

HBM堆栈层数愈多，DRAM就必须做得更薄，因此必须导入更先进的DRAM制程。

普通DRAM颗粒，透过TSV垂直方向钻孔、TCB热压键合的封装技术，可以堆栈出规格不算高的HBM装置救急。

HBM的生产卡点主要在前段Dram，目前由于国内无法取得西方先进设备，工艺被限制在18nm也就是1y水平的Dram，但同样是利用DUVi + MP以及结构的优化去推进到接近1a的水平，进而生产HBM所需要的DDR5以上颗粒。

---

参考：

https://zhuanlan.zhihu.com/p/33990592

HBM火了，它到底是什么？

https://zhuanlan.zhihu.com/p/34164501

HBM技术之显卡应用

https://mp.weixin.qq.com/s/5JrsQnwL6u1fZNIpKq_hQA

DRAM的架构历史和未来

https://mp.weixin.qq.com/s/PzD1lCe5h3VkMrhFb7CfQg

后SoC时代或将迎来Chiplet拐点

https://mp.weixin.qq.com/s/oMA9RSaq0nuSNgKDRwHavQ

Chiplet最强科普

https://zhuanlan.zhihu.com/p/356505099

Chiplet与异构集成技术研究

https://www.zhihu.com/question/565166250

AMD的chiplets芯片出了这么多年了，为什么Intel和NVIDIA还不跟进？

https://www.zhihu.com/question/318057718

hbm2显存和gddr6显存,如果单从相同显存比较性能哪个好？

https://www.zhihu.com/question/422438453

NVIDIA为什么在游戏卡上死活不用HBM2显存？

https://zhuanlan.zhihu.com/p/681519240

HBM发展概述

## 存算一体

![](/images/img4/bottleneck.png)

传统架构存在上图所示的“存储墙”问题。

目前存算技术主要有几个方向：

查存计算（Processing With Memory）：GPU中对于复杂函数就采用了这种计算方法，是早已落地多年的技术。存储芯片内部的存储单元完成查表计算操作，存储单元和计算单元完全融合，没有一个独立的计算单元。

近存计算（Computing Near Memory）：典型代表包括AMD的Zen系列CPU。计算操作由位于存储区域外部的独立计算芯片/模块完成。这种架构设计的代际设计成本较低，适合传统架构芯片转入。

存内计算（Computing In Memory）：典型代表是Mythic、闪忆、知存、九天睿芯。计算操作由位于存储芯片/区域内部的独立计算单元完成，存储和计算可以是模拟的也可以是数字的。这种路线适合算法固定的场景算法计算，目前主要用于语音等轻算力场景。
